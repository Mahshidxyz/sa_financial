# Joint Models for Longitudinal and Time-to-Event Data

In this Chapter we will see a joint modelling approach in order to analyze **two types of outcomes** produced usually in longitudinal studies, particularly, a set of **longitudinal response measurements** and the **time to an event of interest**, such as default, death, etc.

These two outcomes are usually analyzed separately, using a **mixed effects model** [@linearmixed] for the longitudinal response and a **survival model** for the time-to-event. Here, we are going to see how we can analyze them jointly.


#### Why should I use these type of models? {-}

As we mentioned in Chapter \@ref(cox), the Cox PH hazard model can be extended in order to incorporate time-dependent variables. However, when we focus **our interest in the time-to-event** and we wish to take into account the effect of the longitudinal variable as a time-dependent covariate, **traditional approaches** for analyzing time-to-event data (such as the partial likelihood for the Cox proportional hazards models) **are not applicable in all situations**. 

In particular, **standard time-to-event models require that time-dependent covariates are external**; that is, the value of this covariate at time point $t$ is not affected by the occurrence of an event at time point $u$, with $t > u$ [@kalbfleisch1980statistical, Section 6.3]. However, the type of **time-dependent covariates** that we have in longitudinal studies do not met this condition, this is due to the fact that they **are the output of a stochastic process generated by the subject**, which is directly related to the failure mechanism. Based on this, in order to produce correct inferences, we need to apply a joint model that takes into account the joint distribution of the longitudinal and survival outcomes.

**Another advantage** of these models is that they allow  to deal with the **error measurements** in the time dependent variables (longitudinal variable in this case). In a Cox model with time dependent covariates we assume that the variables are measured without error.



```{block2, type = "rmdhint_sestelo"}
When we think in time-dependent covariates, we should first distinguish between two different categories, namely, **internal or endogenous** covariates or **external or exogenous** covariates. Internal covariates are generated from the patient herself and therefore require the existence of the patient, for example *CD4 cell count* and the hazard for death by HIV are stochastic processes generated by the patient herself. On the other hand, *air pollution* is an external covariate to asthma attacks, since the patient has no influence on air pollution.
```






## Linear Mixed Models 

As we mentioned, **Joint Models** take two outcomes into account, the **longitudinal response** and the **survival time**. In order to estimate these type of models, we need first to fit  a model for the longitudinal response (usually a **linear mixed model**) and then for the survival time.   

I am assuming here that you have understood entirely the Chapter \@ref(cox) and you do not have any problem with the estimation of the Cox model by means of the `coxph` function. Regarding the linear mixed model you can see an brief introduction with  examples below using the [`nlme`](https://cran.r-project.org/web/packages/nlme/index.html) package. For a good overview you can consult the Chapter 2 of @book:1606416.




<br><br>



So, our focus in this part is on longitudinal data. This data can be defined as the data resulting from the **observations of subjects** (e.g., human beings, animals, etc.) that are **measured repeatedly over time**.  From this descriptions, it is evident that in a longitudinal setting we expect repeated measurements taken on the same subject to exhibit **positive correlation**. This feature implies that standard statistical tools, such as the t-test and simple linear regression that assume independent observations, are not appropriate for longitudinal data analysis (they  may produce invalid standard errors). In order to solve this situation and obtain valid inference, one possible approach is to use a **mixed model**, a regression method for continuous outcomes that models longitudinal data by assuming, for example, **random errors within a subject** and **random variation in the trajectory among subjects**.




We are going to explain briefly this approach. Figure \@ref(fig:mixed) shows an example with hypothetical longitudinal data for two subjects. In this figure, monthly observations are recorded for up to one year. Note that each subject appears to have their own linear trajectory but with small fluctuations about the line. This fluctuations are referred to as the **within-subject variation** in the outcomes. Note that if we only have data from one person these will be the typical error term in regression.  The dashed line in the center of the figure shows the average of individual linear-time trajectories. This line characterizes the **average for the population** as a function of time.  For example, the value of the dashed line at month 2 is the mean response if the observation (at two months) for all subjects was averaged. Thus, this line represents both the typical trajectory and the population average as a function of time.



The main idea of **Linear Mixed Model** is that they make specific assumptions about the variation in observations attributable
to **variation within a subject** and to **variation among subjects**. To formally introduce this representation of longitudinal data, we let $Y_{ij}$ denote the response of subject $i, i = 1, \ldots, n$ at time $X_{ij}, j = 1,...,n_i$ and $\beta_{i0} + \beta_{i1} X_{ij}$ denote the line that characterizes the observation path for $i$.  Note that each subject has an individual-specific intercept and slope. Note that

+ The **within-subject variation** is seen as the deviation between individual observations, $Y_{ij}$, and the individual linear trajectory, that is $Y_{ij} - (\beta_{i0} + \beta_{i1} X_{ij})$.

+ The **between-subject variation** is represented by the variation among the intercepts, $var(\beta_{i0})$ and the variation among subject in the slopes $var(\beta_{i1})$.


```{r, "mixed", fig.cap = "Hypothetical longitudinal data for two subjects.", echo = FALSE, fig.align = 'center'}
knitr::include_graphics("images//mixed_image.png")
```

Figure \@ref(fig:mixed) taken from @bio.





If **parametric assumptions** are made regarding the within- and between-subject components of variation, it is possibel to use  *maximum likelihood methods* for estimating the regression parameters (which characterize the population average), and the variance components (which characterize the magnitude of within- and between-subject heterogeneity). For continuous outcomes it is a good idea to assume that **within-subject errors** are **normally distributed** and to assume that **intercepts and slopes** are **normally distributed** among subjects. This will be


+ within-subjects:
\[
E(Y_{ij}|\beta_i) = \beta_{i,0} + \beta_{i, 1} X_{ij}
\]


\[
Y_{ij} = \beta_{i,0} + \beta_{i, 1} X_{ij} + \varepsilon_{ij}
\]

\[
 \varepsilon_{ij} \sim N(0, \sigma^2)
\]


+ between-subjects:


\[
 \bigg(\begin{array}{c} \beta_{i,0}\\ \beta_{i,1}\\ \end{array} \bigg) \sim N \bigg[ \bigg(\begin{array}{c} \beta_{0}\\ \beta_{1}\\ \end{array} \bigg),  \bigg(\begin{array}{c} D_{00} & D_{01}\\ D_{10} & D_{11}\\ \end{array} \bigg)   \bigg]
\]
where $D$ is the variance-covariance matrix of the random effects, with $D_{00}= var(b_{i,0})$  and $D_{11}= var(b_{i,1})$.




If we think in $b_{i,0}= (\beta_{i,0} - \beta_0)$ and $b_{i,1}= (\beta_{i,1} - \beta_1)$, the model can be written as

\[
Y_{ij} = \beta_0 + \beta_1 X_{ij} + b_{i,0} + b_{i,1} X_{ij} + \varepsilon_{ij}
\]
where $b_{i,0}$ and  $b_{i,1}$ represent **deviations from the population average intercept and slope** respectively. 


Note taht in this equation there is a **systematic** variation (given by the two first betas) and a **random** variation (the rest). Additionally, the **random component** is partitioned into the observation level and subject level fluctuations: that is, the **between-subject** ($b_{i,0} + b_{i,1} X_{ij}$) and **within-subject** ($\varepsilon_{ij}$) variations.

A more general form including $p$ predictors is

\[
Y_{ij} = \beta_0 + \beta_1 X_{ij,1} +\ldots +  + \beta_p X_{ij,p} + b_{i,0} + b_{i,1} X_{ij,1} + \ldots + b_{i,p} X_{ij,p}+ \varepsilon_{ij}
\]

\[
Y_{ij} = X_{ij}'\beta + Z_{ij}' b_i + \varepsilon_{ij}
\]
where $X_{ij}'=[X_{ij,1}, X_{ij,2}, \ldots, X_{ij,p}]$ and $Z_{ij}'=[X_{ij,1}, X_{ij,2}, \ldots, X_{ij,q}]$. In general way, we assume that the covariates in $Z_{ij}$ are a subset of the variables in $X_{ij}$ and thus $q < p$.


```{block2, type = "rmdhint_sestelo"}
It is important to highlighted that based on this model **the coefficient of covariate $k$ for subject $i$** is given as $(\beta_k + b_{i,k})$ if $k \le q$, and is simply $\beta_k$ if $q < k \le p$. Therefore,in a linear mixed model there may be some regression parameters that vary among subjects while some regression parameters are common to all subjects. 
```

Moving again onto the example, it seems that each subject has their own intercept, but the subjects may have a common slope. So, a **random intercept model** assumes parallel trajectories for any two subjects and is given as a special case of the general mixed model:

\[
Y_{ij} = \beta_0 + \beta_1 X_{ij,1} + b_{i,0} + \varepsilon_{ij}.
\]

Using the above model, the intercept for subject $i$ is given by $\beta_0 + b_{i,0}$ while the slope for subject $i$ is simply $\beta_1$ since there is no additional random slope, $b_{i,1}$ in the random intercept model.


If we assume that the slope for each individual $i$ can also be different, we have to use a **random intercept and slope** model of the type

\[
Y_{ij} = \beta_0 + \beta_1 X_{ij,1} + b_{i,0} + b_{i,1} X_{ij,1}+ \varepsilon_{ij}.
\]
and now the intercept for subject $i$ is given by $\beta_0 + b_{i,0}$ while the slope for subject $i$ is  $\beta_1 + b_{i, 1}$.



In order to fit these models, we can use the `lme` function of the `nlme` package. 


```{r}
head(aids) 
# CD4: square root CD4 cell count measurements
# obstime: time points at which the corresponding longitudinal response was recorded
```

```{r}
# random-intercepts model (single random effect term for each patient)

fit1 <- lme(fixed = CD4 ~ obstime, random = ~ 1 | patient, data = aids)
summary(fit1)
```

Note that the **estimation for the variability** or the variance components, that is, the variance of the errors ($\varepsilon_{ij}$, within personal errors) and the variance between subject (the variance of the $b_{i, 0}$) are given under *Random effects* heading. Under `(Intercept)` we can see the estimated standard desviation for the $b_{i,0}$ coefficients and under `Residual`, the estimated desviation for $\varepsilon_{ij}$.



```{r}
# variance of the beta_i0
getVarCov(fit1)

# standard desviation of e_ij
fit1$sigma

# total variance of the model
getVarCov(fit1)[1] + fit1$sigma**2 

# % variance within person
(fit1$sigma**2/(getVarCov(fit1)[1] + fit1$sigma**2)) * 100 

# % variance between person
(getVarCov(fit1)[1]/(getVarCov(fit1)[1] + fit1$sigma**2)) * 100 
```



The **total variation** in CD4 is estimated as `r round(nlme::getVarCov(fit1)[1] + fit1$sigma**2, 2)`. So, the proportion of total variation that is attributed to **within-person** variability is `r round((fit1$sigma**2/(nlme::getVarCov(fit1)[1] + fit1$sigma**2)) * 100, 2)`% with `r round((nlme::getVarCov(fit1)[1]/(nlme::getVarCov(fit1)[1] + fit1$sigma**2)) * 100, 2)`% of total variation attributable to **individual variation** in their general level of CD4 (attributable to random intercepts).





The estimated regression coefficients $\beta$ are provided under the *Fixed effects* heading. As expected, the coefficient for the time effect has a negative sign indicating that on average the square root CD4 cell counts declines in time. 

Well, this random-intercepts model poses the **unrealistic restriction** that the correlation between the repeated measurements remains constant over time (we are not includiying the random slope yet). So, a natural extension  is a more flexible specification of the covariance structure with the **random-intercepts and random-slopes model**.  This model introduces an additional random effects term, and **assumes that the rate of change in the CD4 cell count is different from patient to patient**.



```{r}
# random-intercepts and random-slopes model
fit2 <- lme(CD4 ~ obstime, random = ~ obstime | patient, data = aids) # the intercept is  included by default
summary(fit2)
```
 
We observe very minor differences in the estimated fixed-effect parameters compared with the previous model. Maybe the AIC For example, for the random effects, we can observe that there is greater variability between patients in the baseline levels of CD4 (given by `(Intercept)` variance) than in the evolutions of the marker in time (`obstime` variance). 



```{r}
anova(fit1, fit2) # reduced vs full

# it is better to use the random-intercepts and random-slopes model 
```




```{r}
muhat <- predict(fit2)
aids2 <- aids
aids2$muhat <- muhat
lattice::xyplot(muhat ~ obstime, group = patient,
                data = aids2[aids2$patient %in% c(1:10), ],type = "l")

```




## Estimation of the Joint Model


In this section we are going to present the joint modelling framework motivated by the **time-to-event point of view**, that is, we want to add a time-dependent covariate measured with error in a survival model. 

Let $T_i$ denote the observed failure time for the $i$-th subject $(i = 1,...,n)$, which is taken as the minimum of the true event time $T_i$ and the censoring time $C_i$, i.e., $\widetilde T_i = \min(T_i,C_i)$. Furthermore, we define the event indicator as $\Delta_i = I(T_i \le C_i)$, where $I$ is the indicator function that takes the value 1 if the condition $T_i \le C_i$ is satisfied, and 0 otherwise.  So, the observed data for the time-to-event outcome consist of the pairs $\{(\widetilde T_i, \Delta_i), i = 1, . . . , n\}$. 

For the longitudinal responses, let $y_i(t)$ denote the value of the longitudinal outcome at time point $t$ for the $i$-th subject. Note that we do not actually observe $y_i(t)$ at all time points, but only at the very specific occasions $t_{ij}$ at which measurements were taken. Thus, the observed longitudinal data consist of the measurements $y_{ij} = \{yi(t_{ij}),j = 1,...,n_i\}$.


The objective is to associate the **true** and **unobserved** value of the longitudinal outcome at time $t$, denoted by $m_i(t)$, with the event outcome $\widetilde T_i$. Note that  $m_i(t)$ is different from $y_i(t)$ because this last is contaminated with measurement error value of the longitudinal outcome at time $t$.

In order to quantify the effect of  $m_i(t)$  on the risk of an event, we can use a relative risk model of the form:

\begin{equation}
h_i(t|\mathcal{M}_i(t),w_i) = h_0(t) \exp \{ \gamma^t w_i + \alpha m_i(t) \}
(\#eq:joint)
\end{equation}
where $\mathcal{M}_i(t)=\{ m_i(s), 0 \le s < t\}$ denotes the denotes the history of the true unobserved longitudinal process up to time point $t$, **$h_0(\cdot)$ denotes the baseline risk function**, and $w_i$ is a vector of baseline covariates (such as a treatment indicator, history of diseases, etc.) with a corresponding vector of regression coefficients $\gamma$. Similarly, parameter $\alpha$ **quantifies the effect of the underlying longitudinal outcome** to the risk for an event.  



```{block2, type = "rmdhint_sestelo"}
The **interpretation of $\gamma$ and $\alpha$ is exactly the same** as we have seen in Chapter \@ref(cox). In particular, $exp(\gamma_j)$ denotes the ratio of hazards for one unit change in $w_{ij}$ at any time $t$, whereas $exp(\alpha)$ denotes the relative increase in the risk for an event at time $t$ that results from one unit increase in $m_i(t)$ at the same time point. 
```

In order to complete the specification of the above model, we need to think about **the choice for the baseline risk function** $h_0(\cdot)$. In standard survival analysis it is customary to leave $h_0(\cdot)$ completely unspecified in order to avoid the impact of misspecifying the distribution of survival times. However, within the joint modeling framework, it turns out that following such a route may lead to an **underestimation** of the standard errors of the parameter estimates [@BIOM:BIOM570]. To avoid such problems we will need to explicitly define $h_0(\cdot)$, for example with a **known parametric** distribution or  alternatively, and even more preferably, we can opt for a **parametric but flexible specification** of the baseline risk function. Several approaches are implemented in the `JM` package under the argument `method`.



#### The longitudinal submodel {-}

In the model above we use $m_i(t)$ to denote the **true value of the underlying longitudinal covariate** at time point $t$. However, and as mentioned earlier, longitudinal information is actually collected intermittently and with error at a set of a few time points $t_{ij}$ for each subject. So, to messure the effect of the longitudinal variable on the risk dor an event, we need to estimate $m_i(t)$. To do this, we are going to use the linear mixed models of the form

\[
y_i(t) = m_i(t) + \varepsilon_i(t),
\]

\[
m_i(t) = x_i^T(t)\beta + z_i^T(t)b_i + \varepsilon_i(t),
\]

\[
b_i \sim N(0, D), \quad \varepsilon_i(t) \sim N(0, \sigma^2),
\]
where $\beta$ denotes the vector of the unknown fixed effects parameters, $b_i$ denotes a vector of random effects, $x_i(t)$ and $z_i(t)$ denote row vectors of the design matrices for the fixed and random effects, respectively, and $\varepsilon_i(t)$ is the measument error term, which is assumed independent of $b_i$.




The main estimation methods for joint models are based on **(semiparametric) maximum likelihood** and **Bayes using MCMM techniques**. The `JM` package that we are going to use is based on maximum likelihood. The idea is the maximization of the log-likelihood corresponding to the joint distribution of the time-to-event and longitudinal out-comes $\{\widetilde T_i,\Delta_i,y_i\}$. Standard numerical integration techniques such as Gaussian quadrature and Monte Carlo have been successfully applied in the joint modelling framework. See Section 4.3 of @book:1606416 for details.








## The `JM` package



Now it is time to fit these models in `R`. To this end, we need first to **fit separately** the linear mixed effect model and the Cox model, and then take the returned objects and use them as main arguments in the `jointModel` function.  The dataset used is the same that the one seen with the mixed model, `aids`.  The survival information can be found in `aids.id`. 
```{r}
head(aids)

head(aids.id)
```





The idea here is to test for a **treatment effect on survival** after adjusting for the CD4 cell count.[^6]

[^6]: The CD4 cell counts are known to exhibit right skewed shapes of distribution, and therefore, for the remainder of this analysis we will work with the square root of the CD4 cell values.

```{r}
lattice::xyplot(sqrt(CD4) ~ obstime | drug, group = patient, data = aids, 
    xlab = "Months", ylab = expression(sqrt("CD4")), col = 1, type = "l")


lattice::xyplot(sqrt(CD4) ~ obstime | patient, group = patient, 
       data = aids[aids$patient %in% c(1:10),], 
       xlab = "Months", ylab = expression(sqrt("CD4")), col = 1, type = "b")

```


Now we are going to specify and fit a joint model. The linear mixed effects model for the CD4 cell counts include:

* Fixed-effects part:  main effect of time and the interaction with the treatment. 
* random-effects design matrix: an intercept and a time term.  

The survival submodel include: treatment effect (as a time-independent covariate) and the true underlying effect of CD4 cell count as estimated from the longitudinal model (as time-dependent). The baseline risk function is assumed piecewise constant. 


```{r}
fitLME <- lme(sqrt(CD4) ~ obstime : drug, random = ~ obstime | patient, data = aids)
fitSURV <- coxph(Surv(Time, death) ~ drug, data = aids.id, x = TRUE)
fitJM <- jointModel(fitLME, fitSURV, timeVar = "obstime", method = "piecewise-PH-GH")
summary(fitJM)
```





```{block2, type = "rmdhint_sestelo"}
Remember that, due to the fact that the `jointModel` function extracts all the required information from these two objects (e.g., response vectors, design matrices, etc.), in the call to the `coxph` function we need to specify the argument `x = TRUE`. With this, the design matrix of the Cox model is included in the returned object.

Additionally, the main argument `timeVar` of `jointModel` function  is used to specify the name of the time variable in the linear mixed effects model, which is required for the computation of $m_i(t)$.
```




Note that in the results of the event process the parameter labeled `Assoct` is the parameter $\alpha$ in the equation \@ref(eq:joint) that measures the effect of $m_i(t)$ (i.e., in our case of the true square root CD4 cell count) in the risk for death. The parameters $x_i$ are the  parameters for the piecewise constant baseline risk function. As we can see there is a significant effect of longitudinal outcome on the risk. For obtaining the Hazard Ratio for this variable we have to exponenciate the value exposed in the table. In this case the result is  `r round(exp(fitJM$coeff$alpha), 2)`. According to this, one unit increse on the CD4 count cell decreases the risk  `r  (1 - round(exp(fitJM$coeff$alpha), 2)) * 100`%.


If we want to test for a treatment effect, an alternative to the Wald test with a pvalue around 0.03, is the Likelihood Ratio Test (LRT). To perform it we need to fit the joint model under the null hypothesis of no treatment effect in the survival submodel, and then use the `anova`  function


```{r}
fitSURV2 <- coxph(Surv(Time, death) ~ 1, data = aids.id, x = TRUE)
fitJM2 <- jointModel(fitLME, fitSURV2, timeVar = "obstime", method = "piecewise-PH-GH")
anova(fitJM2, fitJM) # the model under the null is the first one
```

According to the pvalue (as with the Wald test) we arrive to the same conclusion, there exist an affect of the treatment on the risk.





Additionally, if we want to obtain **estimates of the Hazard Ratio with confidence intervals** for the final model it is possible ti apply the `confint` function to the created object

```{r}
confint(fitJM, parm = "Event")
exp(confint(fitJM, parm = "Event"))
```






<!-- As usually, we can check the fit of the model using residuals plots using the `plot` function. This include the plots of the subject-specific residuals versus the corresponding fitted values, the Q-Q plot of the subject-specific residuals, and the marginal survival and cumulative risk functions for the event process. -->

<!-- ```{r} -->
<!-- par(mfrow = c(2, 2)) -->
<!-- plot(fitJM) -->
<!-- ``` -->


<!-- One problem associated with these models is the **nonrandom dropout** in the longitudinal outcome caused by the occurrence of events. This can be seen on the following plot -->




<!-- ```{r} -->
<!-- # a useful function used in the residual plots below -->
<!-- plotResid <- function (x, y, ...) { -->
<!--     plot(x, y, ...) -->
<!--     lines(lowess(x, y), col = "red", lwd = 2) -->
<!--     abline(h = 0, lty = 3, col = "grey", lwd = 2) -->
<!-- } -->

<!-- # Marginal Residuals vs Fitted Values -->
<!-- resMargY <- residuals(fit.JM, process = "Longitudinal", type = "stand-Marginal") -->
<!-- fitMargY <- fitted(fit.JM, process = "Longitudinal", type = "Marginal") -->
<!-- plotResid(fitMargY, resMargY, xlab = "Fitted Values", ylab = "Residuals", -->
<!--     main = "Marginal Residuals vs Fitted Values") -->


<!-- ``` -->

<!-- We can observe a systematic trend with more positive residuals for small fitted values. However, due to the **nonrandom dropout** in the longitudinal outcome caused by the occurrence of events, conclusions from residuals based on the observed data alone should be extracted with caution. Note that the problem occurs in low values of CD4 that are related with higher times (thus we have less individuals).  -->

<!-- Based on the above, it is important to highlight that to **take dropout into account** we will use the multiply-imputed residuals.   -->







Finally, we will focus on the calculation of **expected survival probabilities**. For this we have to use the `survfitJM` function that accepts as main arguments a fitted joint model, and a data frame that contains the longitudinal and covariate information for the subjects for which we wish to calculate the predicted survival probabilities.


Here we compute the expected survival probabilies for two patients in the data set who **has not died** by the time of loss to follow-up. The function assumes that the patient has survived up to the last time point $t$ in newdata for which a CD4 measurement was recorded, and will produce survival probabilities for a set of predefined $u > t$ values


```{r}
set.seed(300716) # it uses Monte Carlo samples
preds <- survfitJM(fitJM, newdata = aids[aids$patient %in% c("7", "15"), ],
          idVar = "patient")  # last.time = "Time"

survfitJM(fitJM, newdata = aids[aids$patient %in% c("7", "15"), ], idVar = "patient", 
          survTimes = c(20, 30, 40))  # you can specify the times
```


Note that the first time of the output is the last time observed in the longitudinal study. This is because for the time points that are earlier than this time we know that this subject was alive and therefore survival probability is 1. 



```{r}
par(mfrow=c(1,2))
plot(preds, which = "7", conf.int = TRUE)

plot(preds, which = "7", conf.int = TRUE, 
     fun = function (x) -log(x), ylab = "Cumulative Risk")
```











