[
["index.html", "A short course on Survival Analysis applied to the Financial Industry BBVA Data &amp; Analytics, Madrid Preface", " A short course on Survival Analysis applied to the Financial Industry BBVA Data &amp; Analytics, Madrid Marta Sestelo 27-28 /11/2017, v1.2 Preface This book is designed to provide a guide for a short course on survival analysis. It is mainly focussed on applying the stastical tecnquines developed in the survival field to the financial industry. The emphasis is placed in understanding the methods, building intuition about when aplying each of them and showing their application through the use of statistical software. "],
["programing-language-and-software.html", "Programing language and software", " Programing language and software The software used in the course is the statistical language R and the IDE (Integrated Development Environment) used is RStudio. A basic prior knowledge of both is assumed. Basic introductions to R and RStudio are presented in the Appendix C and B for those students lacking basic expertise on them. The required packages for the course are: # Install packages install.packages(c(&quot;survival&quot;, &quot;condSURV&quot;, &quot;JM&quot;, &quot;dplyr&quot;, &quot;survminer&quot;, &quot;ggplot2&quot;)) devtools::install_github(&quot;noramvillanueva/clustcurv&quot;) The codes in the notes may assume that the packages have been loaded, so it is better to do it now: # Load packages library(survival) library(condSURV) library(JM) library(dplyr) library(survminer) library(clustcurv) library(ggplot2) Links: survival(Therneau 2015), condSURV(Meira-Machado and Sestelo 2016a; Meira-Machado and Sestelo 2016b), JM(Rizopoulos 2010), dplyr(Wickham et al. 2017), survminer(Kassambara and Kosinski 2017), ggplot2(Wickham 2009), and clustcurv. References "],
["main-references-and-credits.html", "Main references and credits", " Main references and credits Several reference books have been used for preparing these notes. The following list details the most important ones: Kalbfleisch and Prentice (2002) Klein et al. (2013) Therneau and Grambsch (2000) Klein and Moeschberger (2005) Kleinbaum and Klein (2011) Rizopoulos (2012) In addition, this material is possible due to the work of persons who contribute greatly to the open source software with incredible pieces of software: Xie (2015), Xie (2016), Allaire et al. (2017) and R Core Team (2017). The icons used in the notes were designed by Gregor Cresnar from Flaticon. Some inspiration taken from the repository egarpor/nonpar-eafit. Feedback, comments and some help from Luís Meira-Machado and Nora M. Villanueva. All material in these notes is licensed under CC BY-NC-SA 4.0. References "],
["about-the-author.html", "About the Author", " About the Author Marta Sestelo is a Senior Researcher and Data Scientist at the Galician Research and Development Center in Advanced Telecommunications (Gradiant) and Lecturer at the Department of Statistics and O. R. at University of Vigo (accredited by the ANECA - Spanish National Agency for Quality Assessment and Accreditation - as Associate Professor). In 2013, she received her PhD in Statistics from this university, with international accreditation, with the title ‘Development and computational implementation of estimation and inference methods in flexible regression models. Applications in Biology, Engineering and Environment’. During this period, and as part of her learning process in the field of computational development, she carried out a predoctoral research stay at the Department of Mathematics and Applications at University of Minho. After that, she worked under the supervision of the professor L. Meira-Machado as postdoctoral researcher at the Center of Mathematics (CMAT) at the same university, funded by a highly competitive fellowship of the Fundação para a Ciência e a Tecnologia (FCT) of Portugal (2014-2018). Just before getting this opportunity, she combined her research with some teaching experience at the Department of Mathematics at Autonomous University of Barcelona, by means of a postdoctoral contract during the 2013/2014 academic year. During these last few years she has received institutional support, both from Spain and other countries, to develop her scientific work (4 predoctoral and 1 postdoctoral fellowships). She has also taken part in 23 research projects, presented 43 papers in national and international congresses, developed 6 R packages and, so far, written 15 articles either published or accepted on JCR journals such as Journal of Statistical Software, Biometrical Journal, The R Journal, etc. (Statistics &amp; Probability) or Applied Mathematics and Computation, Fisheries Research, Atmospheric Environment, etc. On an ongoing basis she collaborates with several researchers in the field of Statistics, as well as in other fields (Luís Meira Machado, CMAT; Isabel Serra Mochales, Centre de Recerca Matemàtica; Gorka Bidegain, University of Southern Mississippi; Ángel Guerra, Consejo Superior de Investigaciones Científicas; Celestino Ordóñez, Universidad de Oviedo; Turgut Durduran, Institute of Photonic Sciences; Jorge Hernández Urcera, Instituto Español de Oceanografía, etc.) obtaining as a result a multidisciplinary research. She has broad experience in the development of new methodologies in the field of nonparametric statistics, applied both to regression and survival analysis. She has worked on variable selection, regression, hypothesis testing, etc. She is currently focusing her interest on the study of new machine learning techniques for data analysis and on the resolution of real life problems throughout statistics. Besides, she strongly believes in the need for a more open, transparent and reproducible science, reason why her research relates to computational statistics, and more especially to the development of free software. You can see some topics of her cv at http://sestelo.github.io. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction This introduction to survival analysis tries to give a small overview of the statistical approach called survival analysis. This approach includes the type of problem addressed by survival analysis, the outcome variable considered, the need to take into account censored data, what a survival function and a hazard function represent, the goals of survival analysis, and some examples of survival analysis. "],
["intro-what.html", "1.1 What is survival analysis?", " 1.1 What is survival analysis? In a general way, survival analysis is a collection of statistical procedures for data analysis for which the outcome variable of interest is time until an event occurs, often referred to as a failure time, survival time, or event time. Survival time refers to a variable which measures the time from a particular starting time (e.g., time initiated the treatment) to a particular endpoint of interest: time-to-event. The problem of analyzing time to event data arises in a number of applied fields, such as: medicine, biology, public health (time to death) social sciences (time for doing some task) economics (time looking for employment) financial or credit scoring (time to default) engineering (time to a failure of some electronic component) 1.1.1 Time, time origen, time scale, event In survival analysis three requirements are needed for the precise definition of the failure time of an individual. A time origin must be specified, a time scale for measuring time must be agreed upon and the meaning of failure - event must be clear. By time, we mean years, months, weeks, or days from the beginning of follow-up of an individual until the event of study occurs, but we need to specify the scale. By time origin we understand the time of entry into the study. By event, we mean –it depends on the field– death, disease incidence, recovery (e.g., return to work) if we focus on biomedical applications, default in the credit scoring field, renewals in insurance framework, fault in the engeniering field, etc. Generally, we will assume that only one event is of designated interest. When more than one event is considered (e.g., death from any of several causes), the statistical problem can be characterized as either a recurrent event or a competing risk. We will see the case of the recurrence event using the condSURV package in the Chapter 5. It is time to see now an example in a real dataset. This is the Prosper Loan data provided by Udacity Data Analyst Nanodegree (last updated 3/11/14). It is also at Kaggle. Prosper.com is a peer-to-peer lending marketplace. Borrowers make loan requests and investors contribute as little as $25 towards the loans of their choice. Historically, Prosper made their loan data public nightly, however, effective January 2015, information will be made available 45 days after the end of the quarter. A link to the data is here and a variable dictionary can be found here. # Prosper Loan data web &lt;- &quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/prosperLoanData.csv&quot; loan &lt;- read.csv(web) head(loan)[, c(51, 65, 6, 7, 19, 18, 50)] ## LoanKey LoanOriginationDate LoanStatus ## 1 E33A3400205839220442E84 2007-09-12 00:00:00 Completed ## 2 9E3B37071505919926B1D82 2014-03-03 00:00:00 Current ## 3 6954337960046817851BCB2 2007-01-17 00:00:00 Completed ## 4 A0393664465886295619C51 2012-11-01 00:00:00 Current ## 5 A180369302188889200689E 2013-09-20 00:00:00 Current ## 6 C3D63702273952547E79520 2013-12-24 00:00:00 Current ## ClosedDate Occupation BorrowerState StatedMonthlyIncome ## 1 2009-08-14 00:00:00 Other CO 3083.333 ## 2 Professional CO 6125.000 ## 3 2009-12-17 00:00:00 Other GA 2083.333 ## 4 Skilled Labor GA 2875.000 ## 5 Executive MN 9583.333 ## 6 Professional NM 8333.333 1.1.2 Goals of the survival analysis Estimate time-to-event for a group of individuals, such as time until default for a group of clients. Compare time-to-event between two or more groups, such as residence place for clients. Assess the relationship of covariates to time-to-event, such as: occupation, state, income, etc. "],
["intro-censor.html", "1.2 Censoring", " 1.2 Censoring The distinguishing feature of survival analysis is that it incorporates a phenomen called censoring. Censoring occurs when we have some information about individual survival time, but we don’t know the time exactly. There are generally several reasons why censoring may occur: a person does not experience the event before the study ends a person is lost to follow-up during the study period a person withdraws from the study because of death (if death is not the event of interest) or some other reason There are three types: Right censoring: Random right censoring arise often in medical, biological and financial applications. In this studies, patients may enter the study at different times and the real event time is greater than the observed time. We know that the person’s true survival time becomes incomplete at the right side of the follow-up period, occurring when the study ends or when the person is lost to follow-up or is withdrawn. For these data, the complete survival time interval, which we don’t really know, has been cut off (i.e., censored) at the right side of the observed survival time interval. This is the assumed censoring in the case of credit scoring. Left censoring: The survival time of some subject is considered to be left censored if it is less than the value observed. That is, the event of interest has already occurred for the individual before the observed time (not easy to deal with). For example, if we are following persons until they become HIV positive, we may record a failure when a subject first tests positive for the virus. However, we may not know the exact time of first exposure to the virus, and therefore do not know exactly when the failure occurred. Thus, the survival time is censored on the left side since the true survival time, which ends at exposure, is shorter than the follow-up time, which ends when the subject’s test is positive. Interval censoring: When the survival time is only known to occur within an interval. Such interval censoring occurs when patients in a clinical trial or longitudinal study have periodic follow-up and the patient’s event time is only known to fall in some interval. As an example, again considering HIV, a subject may have had two HIV tests, where he/she was HIV negative at the time (say, \\(t_1\\)) of the first test and HIV positive at the time (\\(t_2\\)) of the second test. In such a case, the subject’s true survival time occurred after time \\(t_1\\) and before time \\(t_2\\), i.e., the subject is interval-censored in the time interval (\\(t_1\\), \\(t_2\\)). Figure 1.1: Illustration of censoring. It is important to highlight in this context (time-to-default) which situations we are going to considered as censoring. The bank has special characteristics that are not seen in other applications. Censored cases are considered to be loans that did not experience default by the moment of data gathering. Additionally, early repayment and mature cases (or complete, those ones who reach their predefined end date before the moment of data gathering) are also marked censored. Another classification: Random type I censoring: Also known as Generalized Type I Censoring. When individuals enter the study at different times and the terminal point of the study is predetermined by the investigator, so that the censoring times are known when an individual is entered into the study. Type II censoring: The study continues until the failure of the first \\(r\\) individuals, where \\(r\\) is some predetermined integer (\\(r&lt;n\\)). All subjects are put on test at the same time, and the test is terminated when \\(r\\) of the \\(n\\) subjects have “failed”. "],
["intro-notation.html", "1.3 Some notation", " 1.3 Some notation We are now ready to introduce basic mathematical terminology and notation for survival analysis. Let \\(T\\) the random variable that denotes the survival time, i.e., the time to an event. Since \\(T\\) denotes time, its possible values include all nonnegative numbers; that is, \\(T\\) can be any number equal to or greater than zero. Furthermore, \\(t\\) will be any specific value of interest for the random variable \\(T\\). Additionally, when each subject has a random right censoring time \\(C_i\\) that is independent of their failure time \\(T_i\\), the data is represented by \\((Y_i, \\Delta_i)\\) where \\(Y_i = \\min(T_i, C_i)\\) and \\(\\Delta_i = I(T_i \\le C_i)\\), this \\(\\Delta\\) define a \\((0,1)\\) random variable indicating either failure or censorship. That is, \\(\\Delta = 1\\) for failure if the event occurs during the study period, or \\(\\Delta = 0\\) if the survival time is censored by the end of the study period. "],
["intro-functions.html", "1.4 Survival/hazard functions", " 1.4 Survival/hazard functions Assuming that \\(T\\) is a continuous non-negative random variable which denote the time-to-event. There is a certain probability that an individual will have an event at exactly time \\(t\\). For example, about human longevity, human beings have a certain probability of dying at ages \\(2\\), \\(20\\), \\(80\\), and \\(140\\), that could be: \\(P(T=2)\\), \\(P(T=20)\\), \\(P(T=80)\\) and \\(P(T=140)\\). Similarly, human beings have a certain probability of being alive at those same ages: \\(P(T&gt;2)\\), \\(P(T&gt;20)\\), \\(P(T&gt;80)\\), and \\(P(T&gt;140)\\). Here an example with same real data:1 data &lt;- read.table(&quot;data/deaths_esp.txt&quot;, header = TRUE, sep = &quot;&quot;) data &lt;- data[!data$Age == &quot;110+&quot;, ] # to avoid errors data$Age_cut &lt;- cut(as.numeric(as.character(data$Age)), breaks = seq(0,110, 10), right = FALSE) by_age &lt;- data %&gt;% group_by(Age_cut) %&gt;% summarise (sum_deaths = sum(Total, na.rm = TRUE)) barplot(by_age$sum_deaths/sum(data$Total), names.arg = by_age$Age_cut, ylab= &quot;Relative frequency&quot;) Figure 1.2: Relative frequiencies for grouped ages. In the case of human longevity, the probability of death is higher at the beginning and end of life (in Spain). Therefore, \\(T\\) is unlikely to follow a normal distribution. We can see a higher chance of dying (the event of interest) in their 70’s and 80’s and smaller chance of dying in their 100’s and 110’s, because few people make it long enough to die at these age. The function that gives the probability of the failure time occurring at exactly time \\(t\\) is the density function \\(f(t)\\)2 \\[ f(t) = \\displaystyle{lim_{\\Delta_t \\to 0}} \\frac{P(t \\le T &lt; t + \\Delta t)}{\\Delta t} \\] and the function that gives the probability of the failure time occur before or exactly at time \\(t\\) is the cumulative distribution function \\(F(t)\\) \\[ F(t) = P(T \\le t) = \\int_{0}^{t} f(u) du. \\] Note that \\(F(t)\\) is more interesting than \\(f(t)\\)… And why? Well, as we said, the main goal of survival analysis is to estimate and compare survival experiences of different groups and the survival experience is described by the survival function \\(S(t)\\) \\[ S(t) = P(T &gt; t) = 1 - F(t) \\] The survival function gives the probability that a person survives longer than some specified time \\(t\\): that is, \\(S(t)\\) gives the probability that the random variable \\(T\\) exceeds the specified time \\(t\\). And here, some important characteristics: It is nonincreasing; that is, it heads downward as \\(t\\) increases. At time \\(t = 0\\), \\(S(t) = S(0)= 1\\); that is, at the start of the study, since no one has gotten the event yet, the probability of surviving past time zero is one. At time \\(t = \\inf\\), \\(S(t) = S(\\inf) = 0\\); that is, theoretically, if the study period increased without limit, eventually nobody would survive, so the survival curve must eventually fall to zero. t &lt;- seq(0, 110, 1) tdf &lt;- pweibull(t, scale = 80, shape = 5) # weibull dist d &lt;- reshape2::melt(data.frame(x = t, dist = tdf, surv = 1 - tdf), id = &quot;x&quot;) qplot(x = x, y = value, col = variable, data = d, geom = &quot;line&quot;, ylab = &quot;probability&quot;, xlab = &quot;t&quot;) + scale_colour_discrete(labels= c(&quot;F(t)&quot;, &quot;S(t)&quot;), name = &quot;&quot;) Note that these are theoretical properties of survival curves. In practice, when using actual data, we usually obtain graphs that are step functions, rather than smooth curves. Moreover, because the study period is never infinite in length and there may be competing risks for failure, it is possible that not everyone studied gets the event. The estimated survival function, \\(\\hat{S}(t)\\) thus may not go all the way down to zero at the end of the study. by_age &lt;- data %&gt;% group_by(Age) %&gt;% summarise (sum_deaths = sum(Total, na.rm = T)) t &lt;- rep(as.numeric(as.character(by_age$Age)), by_age$sum_deaths) # real times aux &lt;- ecdf(t) x &lt;- seq(0, 110, 1) edf &lt;- aux(x) # evaluating the ecdf in some points esf &lt;- 1- edf d &lt;- reshape2::melt(data.frame(x = x, dist = edf, surv = esf), id = &quot;x&quot;) qplot(x = x, y = value, col = variable, data = d, geom = &quot;step&quot;, ylab = &quot;Probability&quot;, xlab = &quot;t&quot;) + scale_colour_discrete(labels = c(&quot;F(t)&quot;, &quot;S(t)&quot;), name = &quot;&quot;) The hazard function \\(h(t)\\), is given by the formula: \\[ h(t) = \\displaystyle{lim_{\\Delta_t \\to 0}} \\frac{P(t \\le T &lt; t + \\Delta t | T \\ge t)}{\\Delta t} \\] This mathematical formula is difficult to explain in practical terms. We could say that the hazard function is the probability that if you survive to time \\(t\\), you will experience the event in the next instant, or in other words, the hazard function gives the instantaneous potential per unit time for the event to occur, given that the individual has survived up to time \\(t\\). Because of the given sign here, the hazard function is sometimes called a conditional failure rate. Note that, in contrast to the survival function, which focuses on not failing, the hazard function focuses on failing, that is, on the event occurring. Thus, in some sense, the hazard function can be considered as giving the opposite side of the information given by the survivor function. Additionally, in contrast to a survival function, the graph of \\(h(t)\\) does not have to start at one and go down to zero, but rather can start anywhere and go up and down in any direction over time. In particular, for a specified value of \\(t\\), the hazard function \\(h(t)\\) has the following characteristics: It is always nonnegative, that is, equal to or greater than zero. It has no upper bound. Finally note that the hazard function can be expressed as the probability density function divided by the survival function, \\(h(t) = \\frac{f(t)}{S(t)}\\): \\[ P(t \\le T \\lt t + dt | T \\ge t) = \\frac{P(t \\le T \\lt t + dt, T \\ge t)}{P(T \\ge t)} = \\frac{P(t \\le T \\lt t + dt)}{P(T \\ge t)} \\] h &lt;- hist(t, plot = FALSE) x &lt;- h$mids dens &lt;- h$density surv &lt;- 1 - aux(x) hazard &lt;- dens/surv qplot(x = x, y = hazard, geom = &quot;line&quot;, ylab = &quot;Conditional probability of death&quot;, xlab = &quot;Age&quot;) In some cases it can be more interesting to present the cumulative hazard. It will be \\(H(t) = \\int_{0}^{t} h(u) du\\). Hazard vs. density function According to the human longevity study, note that when you are born, you have a certain probability of dying at any age, that will be \\(P(T = t)\\), i.e. the density function. A woman born today has, say, a 1% chance of dying at 80 years. However, as you survive for a while, your probabilities keep changing, and these new conditional probabilities are given by the hazard function. In such case, we have a woman who is 79 today and has, say, a 7% chance of dying at 80 years. Data from The Human Mortality Database at http://www.mortality.org.↩ The probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value.↩ "],
["relation-between-functions.html", "1.5 Relation between functions", " 1.5 Relation between functions For parametric survival models, time is assumed to follow some well-known distribution whose probability density function \\(f(t)\\) can be expressed in terms of unknown parameters. Once a probability density function is specified for survival time, the corresponding survival and hazard functions can be determined. For example, the survival function can be ascertained from the probability density function by integrating over the probability density function from time \\(t\\) to infinity, or by calculating the difference between one and the cumulative distribution function \\(F(t)\\). The hazard can then be found by dividing the negative derivative of the survival function by the survival function. Note that the functions \\(f(t)\\), \\(F(t)\\), \\(h(t)\\), and \\(H(t)\\) are all related. Assume that \\(T\\) is non-negative and continuos: Probability density function: \\(f(t) = F&#39;(t) = \\frac{dF(t)}{dt}\\) Cumulative distribution function: \\(F(t) = P(T \\le t) = \\int_0^t{f(u)}{du}\\) Survival function \\(S(t) = 1 - F(t)\\) \\(S(t) = P(T &gt; t) = \\int_t^{+\\infty}{f(u)}{du}\\) \\(S(t) = exp \\left( - \\int_0^t h(u) du \\right)\\) \\(S(t) = \\exp(-H(t))\\) Hazard function \\(h(t) = \\frac{ f(t)}{S(t)}= \\frac{ -d[S(t)]/dt}{S(t)}\\) Cumulative hazard function \\(H(t) = \\int_0^t h(u) du\\) Assume that \\(T\\) is non-negative and discrete, Probability mass function: \\(p(t_i) = P(T = t_i)\\) \\(p(t_i) = S(t_{i-1}) - S(t_i)\\) \\(p(t_i) = F(t_i) - F(t_{i-1})\\) Cumulative distribution function: \\(F(t) = P(T \\le t) = \\sum_{t_i \\le t}{p(t_i)}\\) Survival function \\(S(t) = \\prod_{t_i \\le t} \\left( 1 - h(t_i) \\right)\\) Hazard function \\(h(t) = \\frac{ p(t_i)}{S(t_{i-1})}= \\frac{ -d[S(t)]/dt}{S(t)}\\) \\(h(t) = 1- \\frac{ S(t_i)}{S(t_{i-1})}\\) Cumulative hazard function \\(H(t) = \\sum_{t_i \\le t} h(t_i)\\) "],
["intro-distri.html", "1.6 Some common distributions", " 1.6 Some common distributions Definition Functions Measures Exponential \\(T\\sim Exp( \\lambda)\\) \\(f(t)=\\lambda exp(-\\lambda t)\\) where \\(t \\ge 0 and \\lambda &gt; 0\\) \\(F(t)=1-exp(-\\lambda t)\\) \\(S(t)=exp(-\\lambda t)\\) \\(h(t) = \\lambda\\) \\(H(t) = \\lambda t\\) \\(E(T)=\\int_0^{+\\infty}uf(u) du= \\frac{1}{\\lambda}\\) \\(Var(T)=E(T^2)-E(T)^2 = \\ldots = \\frac{1}{\\lambda^2}\\) Weibull \\(T\\sim Weib(a,b)\\) with \\(a\\) shape and \\(b\\) scale \\(f(t)=\\frac{a}{b} (\\frac{t}{b})^{a-1} exp^{-\\left(\\frac{t} {b} \\right)^a}\\) where \\(t\\ge 0\\) and \\(a,b&gt; 0\\) \\(F(t)= 1-exp^{- \\left(\\frac{t}{b} \\right)^a}\\) \\(S(t)=exp^{-\\left( \\frac{t}{b} \\right)^a}\\) \\(h(t)=ab^{-a}t^{a-1}\\) \\(H(t)=(\\frac{t} {b})^a\\) \\(E(T)=b\\Gamma \\left(1+ \\frac{1}{a}\\right)\\) \\(Var(T) = b^2 \\Gamma \\left(1+ \\frac{2}{a}\\right) - b^2 \\left [ \\Gamma \\left(1+ \\frac{1}{a}\\right)\\right]^2\\) where, \\(\\Gamma(k)\\) is the gamma function. \\(\\Gamma (k) = \\int_0^{+\\infty} u^{k-1} exp^{-u}du\\) There are other distributions such as Log-Normal, Log-Logistic, Pareto, Rayleigh, Gomptertz, or even more. For more details see http://data.princeton.edu/pop509/ParametricSurvival.pdf. "],
["km.html", "Chapter 2 Kaplan Meier estimator", " Chapter 2 Kaplan Meier estimator Once we have explained what is the survival curve and other introductory questions, we move on the estimation. Note that we can estimate the survival (or hazard) function in two ways: by specifying a parametric model for \\(\\lambda(t)\\) based on a particular density function \\(f(t)\\) (parametric estimation) by developing an empirical estimate of the survival function (i.e., nonparametric estimation) This Chapter describes how to plot and interpret survival data using the Kaplan-Meier (KM) estimator (nonparametric) and how to test whether or not two or more KM curves are equivalent using the log–rank test. Alternative tests to the log–rank test are also described. Furthermore, methods for computing \\((1-\\alpha)\\)% confidence intervals for a KM curve are afforded. "],
["estimating-survival-by-means-of-the-kaplan-meier-estimator.html", "2.1 Estimating survival by means of the Kaplan Meier estimator", " 2.1 Estimating survival by means of the Kaplan Meier estimator If there are no censored observations in a sample of dimension \\(n\\), the most natural estimator for survival is the empirical estimator, given by \\[ \\hat S(t) = P(T \\gt t) = \\frac{1}{n} \\sum_{i=1}^{n} I(t_i \\gt t) \\] that is, the proportion of observations with failure times greater than \\(t\\). x &lt;- c(1, 1, 2, 2, 3, 4, 4, 5, 5, 8, 8, 8, 8, 11, 11, 12, 12, 15, 17, 22, 23) sum(x &gt; 8)/length(x) # hat S(8) ## [1] 0.3809524 sum(x &gt; 12)/length(x) # hat S(12) ## [1] 0.1904762 Another option for estimating survival could be to use the hazard: \\[ \\hat S(t) = \\prod_{k = 1}^{t-1} \\bigg [ 1- \\hat \\lambda(k)\\bigg] \\quad {\\text{where}} \\quad \\hat \\lambda(t) = \\frac{\\sum_{i=1}^{n} I(Y_i = t)}{\\sum_{i=1}^{n} I (Y_i \\ge t)} \\] Note that \\(\\hat \\lambda(t)\\) is obtained as the number of individuals that die at time \\(t\\) divided by the number of individuals that survive to \\(t\\), the number of individuals at risk at time \\(t\\) (using the death as event). However, alternative methods are necessary to incorporate censoring (censored times are different than event times). # preprocesing data head(loan)[, c(51, 65, 6, 7, 19, 18, 50)] ## LoanKey LoanOriginationDate LoanStatus ## 1 E33A3400205839220442E84 2007-09-12 00:00:00 Completed ## 2 9E3B37071505919926B1D82 2014-03-03 00:00:00 Current ## 3 6954337960046817851BCB2 2007-01-17 00:00:00 Completed ## 4 A0393664465886295619C51 2012-11-01 00:00:00 Current ## 5 A180369302188889200689E 2013-09-20 00:00:00 Current ## 6 C3D63702273952547E79520 2013-12-24 00:00:00 Current ## ClosedDate Occupation BorrowerState StatedMonthlyIncome ## 1 2009-08-14 00:00:00 Other CO 3083.333 ## 2 Professional CO 6125.000 ## 3 2009-12-17 00:00:00 Other GA 2083.333 ## 4 Skilled Labor GA 2875.000 ## 5 Executive MN 9583.333 ## 6 Professional NM 8333.333 table(loan$LoanStatus) ## ## Cancelled Chargedoff Completed ## 5 11992 38074 ## Current Defaulted FinalPaymentInProgress ## 56576 5018 205 ## Past Due (&gt;120 days) Past Due (1-15 days) Past Due (16-30 days) ## 16 806 265 ## Past Due (31-60 days) Past Due (61-90 days) Past Due (91-120 days) ## 363 313 304 # removing duplicates loan_nd &lt;- loan[unique(loan$LoanKey), ] # removing LoanStatus no needed sel_status &lt;- loan_nd$LoanStatus %in% c(&quot;Completed&quot;, &quot;Current&quot;, &quot;ChargedOff&quot;, &quot;Defaulted&quot;, &quot;Cancelled&quot;) loan_filtered &lt;- loan_nd[sel_status, ] # creating status variable for censoring loan_filtered$status &lt;- ifelse( loan_filtered$LoanStatus == &quot;Defaulted&quot; | loan_filtered$LoanStatus == &quot;Chargedoff&quot;, 1, 0) # adding the final date to &quot;current&quot; status head(levels(loan_filtered$ClosedDate)) ## [1] &quot;&quot; &quot;2005-11-25 00:00:00&quot; &quot;2005-11-29 00:00:00&quot; ## [4] &quot;2005-11-30 00:00:00&quot; &quot;2005-12-08 00:00:00&quot; &quot;2005-12-28 00:00:00&quot; levels(loan_filtered$ClosedDate)[1] &lt;- &quot;2014-11-03 00:00:00&quot; # creating the time-to-event variable loan_filtered$start &lt;- as.Date(loan_filtered$LoanOriginationDate) loan_filtered$end &lt;- as.Date(loan_filtered$ClosedDate) loan_filtered$time &lt;- as.numeric(difftime(loan_filtered$end, loan_filtered$start, units = &quot;days&quot;)) # there is an error in the data (time to event less than 0) loan_filtered &lt;- loan_filtered[-loan_filtered$time &lt; 0, ] # just considering a year of loans creation ii &lt;- format(as.Date(loan_filtered$LoanOriginationDate),&#39;%Y&#39;) %in% c(&quot;2006&quot;) loan_filtered &lt;- loan_filtered[ii, ] dim(loan_filtered) ## [1] 4923 85 head(loan_filtered)[, c(51, 65, 6, 7, 19, 18, 50, 83, 84, 85)] ## LoanKey LoanOriginationDate LoanStatus ## 55706 569F3376160094112B0CCBC 2006-12-07 00:00:00 Completed ## 5258 E3C433749566192177F6A25 2006-11-21 00:00:00 Completed ## 64330 3E5A33783711441966A924A 2006-12-29 00:00:00 Completed ## 1485 AC4533744391314602B8E3A 2006-12-07 00:00:00 Completed ## 22540 08B63364821540522E94FD2 2006-06-13 00:00:00 Completed ## 50637 31C9337247671326054DF29 2006-11-03 00:00:00 Completed ## ClosedDate Occupation BorrowerState StatedMonthlyIncome ## 55706 2009-07-27 00:00:00 Professional 4534.250 ## 5258 2008-07-03 00:00:00 Other 3833.333 ## 64330 2009-12-29 00:00:00 Doctor 18083.333 ## 1485 2008-11-21 00:00:00 Other IN 4576.000 ## 22540 2007-09-05 00:00:00 3458.333 ## 50637 2009-11-03 00:00:00 Professional IN 15666.667 ## start end time ## 55706 2006-12-07 2009-07-27 963 ## 5258 2006-11-21 2008-07-03 590 ## 64330 2006-12-29 2009-12-29 1096 ## 1485 2006-12-07 2008-11-21 715 ## 22540 2006-06-13 2007-09-05 449 ## 50637 2006-11-03 2009-11-03 1096 #------ # censoring status 0 = censored, 1 = no censored (default) table(loan_filtered$status) ## ## 0 1 ## 3560 1363 prop.table(table(loan_filtered$status)) ## ## 0 1 ## 0.7231363 0.2768637 # median time until default (taking into account just no cendored data) median(loan_filtered$time[loan_filtered$status==1]) # I&#39;m underestimating ## [1] 333 # median time until default (with all data) mean(loan_filtered$time) # I&#39;m underestimating the median survival too ## [1] 633.4331 # (in censored times, the real time is bigger) Kaplan and Meier (1958) obtained a nonparametric estimate of the survival function, called product-limit, which is the generalization of the empirical estimator for censored data \\[ \\hat S(t) = P(T \\gt t) = \\prod_{i:t_i \\le t} \\bigg[1-\\frac{d_i}{n_i} \\bigg] \\] where \\(t_1, t_2, \\ldots,t_n\\) are the observed event times, \\(d_i\\) is the number of events at time \\(t_i\\), and \\(n_i\\) is the number of individuals at risk at time \\(t_i\\) (i.e, the original sample minus all those who had the event before \\(t_j\\).) Note that \\(d_i/n_i\\) is the proportion that failed at the event time \\(t_i\\) and \\(1 - d_i/n_i\\) is the proportion surviving the event time \\(t_j\\). The Kaplan-Meier estimate is a step function with jumps at event times. The size of the steps depend on the number of events and the number of individuals at risk at the corresponding time. Note that if the last data is censored, the estimator will not reach the zero value. Without censoring, the estimator is equivalent to the empirical survival function \\(\\hat S(t) = \\frac{1}{n} \\sum_{i=1}^{n} I(t_i \\gt t)\\) or to the one using the risk estimates \\(\\hat S(t) = \\prod_{k = 1}^{t-1} \\bigg [ 1- \\hat \\lambda(k)\\bigg]\\). km &lt;- survfit(Surv(time, status) ~ 1, data = loan_filtered) km # we can see the correct estimated median ## Call: survfit(formula = Surv(time, status) ~ 1, data = loan_filtered) ## ## n events median 0.95LCL 0.95UCL ## 4923 1363 1189 1158 1217 print(km, print.rmean = TRUE) ## Call: survfit(formula = Surv(time, status) ~ 1, data = loan_filtered) ## ## n events *rmean *se(rmean) median 0.95LCL ## 4923.00 1363.00 926.53 6.85 1189.00 1158.00 ## 0.95UCL ## 1217.00 ## * restricted mean with upper limit = 1224 Take a look at ?Surv of the survival package. 2.1.1 Other representation Assume that \\(\\widetilde T_i = min (T_i, C_i)\\) and \\(\\Delta_i = I (T_i \\le C_i)\\), we introduce a weighted average representation of the Kaplan-Meier estimator which will be used later to introduce estimators for the conditional survival function \\[\\begin{equation*} \\widehat S(y)=1-\\sum_{i=1}^{n}W_{i}I(\\widetilde T_{(i)}\\leq y), \\end{equation*}\\] where \\(\\widetilde T_{\\left( 1\\right) }\\leq ...\\leq \\widetilde T_{\\left( n\\right) }\\) denotes the ordered \\(\\widetilde T\\)-sample and \\[\\begin{equation*} W_{i}=\\frac{\\Delta_{\\left[ i\\right] }}{n-i+1}\\prod_{j=1}^{i-1}\\left[ 1-\\frac{% \\Delta _{\\left[ j\\right] }}{n-j+1}\\right] \\end{equation*}\\] is the Kaplan-Meier weight attached to \\(\\widetilde T_{\\left( i\\right) }\\). In the expression of \\(W_{i}\\) notation \\(\\Delta_{\\left[ i\\right] }\\) is used for the \\(i\\)-th concomitant value of the censoring indicator (that is, \\(\\Delta_{\\left[ i \\right] }=\\Delta _{j}\\) if \\(\\widetilde T_{\\left( i\\right) }=\\widetilde T_{j}\\)). References "],
["pointwise-confidence-interval-for-st.html", "2.2 Pointwise confidence interval for \\(S(t)\\)", " 2.2 Pointwise confidence interval for \\(S(t)\\) For the contruction of the confidence interval for the estimated survival we can use a well-know estimator of the variance, the Greenwood estimator (Greenwood 1926). The Greenwood variance estimate for a Kaplan-Meier curve is defined as \\[ \\hat \\sigma^2[\\hat S(t)] = \\widehat var[\\hat S(t)] = \\hat S(t)^2 \\sum_{i:t_i \\le t} \\frac{d_i}{n_i(n_i-d_i)} \\] In case of no censoring, this estimator reduces to \\(\\hat \\sigma^2[\\hat S(t)] = \\frac{\\hat S(t) [1- \\hat S(t)]}{n}\\). It is possible to use this estimator to derive a confidence interval for all time points \\(t\\). Assuming asintotic normality (\\(\\hat S(t) \\simeq N(\\hat S(t), \\sigma(t)/\\sqrt(n))\\)) and let \\(\\sigma\\) denotes the Greenwood’s standard deviation. Then confidence intervals for the survival function are then computed as follows (plain) \\[ \\bigg(\\hat S(t) \\pm z_{1-\\alpha/2} \\cdot \\hat \\sigma/\\sqrt(n) \\bigg), \\] where \\(\\hat \\sigma = se(\\hat S(t))\\) is calculated using Greenwood’s formula. It is important to hightlight here that this confidence interval may be out of the (0,1) interval! For solve this, the approximation to the normal distribution is improved by using the log-minus-log transformation \\[ \\bigg(\\hat S(t) \\pm e^{z_{1-\\alpha/2} \\cdot \\frac{\\hat\\sigma}{\\hat S(t) ln \\hat S(t)}} \\bigg). \\] Other options include the log transformation \\[ \\exp \\bigg( \\ln(\\hat S(t)) \\pm z_{1-\\alpha/2} \\cdot \\hat\\sigma/ \\hat S(t) \\bigg). \\] In R we can select these options as: log(default), log-log and plain. km1 &lt;- survfit(Surv(time, status) ~ 1, data = loan_filtered) # conf.type = &quot;log&quot; (default) summary(km1, times = c(200, 1100)) ## Call: survfit(formula = Surv(time, status) ~ 1, data = loan_filtered) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 200 4207 201 0.955 0.00309 0.949 0.961 ## 1100 143 1130 0.626 0.01369 0.600 0.653 km2 &lt;- survfit(Surv(time, status) ~ 1, data = loan_filtered, conf.type = &quot;plain&quot;) summary(km2, times = c(200, 1100)) ## Call: survfit(formula = Surv(time, status) ~ 1, data = loan_filtered, ## conf.type = &quot;plain&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 200 4207 201 0.955 0.00309 0.949 0.961 ## 1100 143 1130 0.626 0.01369 0.599 0.653 km3 &lt;- survfit(Surv(time, status) ~ 1, data = loan_filtered, conf.type = &quot;log-log&quot;) summary(km3, times = c(200, 1100)) ## Call: survfit(formula = Surv(time, status) ~ 1, data = loan_filtered, ## conf.type = &quot;log-log&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 200 4207 201 0.955 0.00309 0.949 0.961 ## 1100 143 1130 0.626 0.01369 0.598 0.652 See arguments times and censored of the function summary.survfit. And now… what about the empirical distribution (without taking into account the censored data)? We can compare both! With the Prosper dataset, try to compare in a graphical manner the survival function based on empirical distribution function of the time to default and based on the Kaplan-Meier estimator. References "],
["comparing-survival-curves.html", "2.3 Comparing survival curves", " 2.3 Comparing survival curves As we have seen before, we can use the survfit function to estimate the survival using the Kaplan-Meier estimator taking into account the censored data. Additionally, it is possible to include a factor in the model and to obtain the estimated survival for each of the levels of the factor. model &lt;- survfit(Surv(time, status) ~ IsBorrowerHomeowner, data = loan_filtered) plot(model, ylab = &quot;Survival&quot;, xlab = &quot;Time (in days)&quot;, col = 1:2, mark.time = TRUE) legend(&quot;topright&quot;, col = 1:2, legend = levels(factor(lung$sex)), bty = &quot;n&quot;, pch = 19) Now, the questions that arises is if these two curves are statistivally equivalent. For answering it, we can use the log-rank test (Mantel 1966; Peto and Peto 1972). This is the most well-known and widely used method to test the null hypothesis of no difference in survival between two or more independent groups. It is a large-sample chi-square test that is obtained by constructing a two by two contingency table at each distinct event time, and comparing the failure rates between the two groups, conditional on the number at risk in each group. The test compares the entire survival experience between groups and can be thought of as a test of whether the survival curves are identical or not. When we state that two KM curves are statistically equivalent, we mean that, based on a testing procedure that compares the two curves in some overall sense, we do not have evidence to indicate that the true (population) survival curves are different. The null hypothesis (\\(H_0\\)) of the testing procedure is that there is no overall difference between the two (or \\(k\\)) survival curves. Under this \\(H_0\\), the log–rank statistic is approximately a chi-square with \\(k-1\\) degree of freedom. Thus, tables of the chi-square distribution are used to determine the pvalue. This test is the one with most power to test differences that fit the proportional hazards model - so works well as a set-up for subsequent Cox regression. It gives equal weight to early and late failures. An alternative test that is often used is the Peto &amp; Peto (Peto and Peto 1972) modification of the Gehan-Wilcoxon test (Gehan 1965). This last one is a variation of the log-rank test statistic and is derived by applying different weights at the \\(f-\\)th failure time. This approach is most sensitive to early differences (or earlier time points) between survival. This type of weighting may be used to assess whether the effect of a treatment/marketing campaing on survival is strongest in the earlier phases of administration/contacto and tends to be less effective over time. In the absence of censoring, these methods reduce to the Wilcoxon-Mann-Whitney rank-sum test (Mann and Whitney 1947) for two samples and to the Kruskal-Wallis test (Kruskal and Wallis 1952) for more than two groups of survival times. Of course, several other variations of the log-rank test statistic using weights on each event time have been proposed in the literature [Tarone and Ware (1977); doi:10.1093/biomet/69.3.553; 10.2307/2289169]. The log-rank test and the Peto &amp; Peto modification of the log-rank test are both implemented in the survdiff function in library survival. survdiff(Surv(time, status) ~ IsBorrowerHomeowner, data = loan_filtered, rho = 0) # log-rank ## Call: ## survdiff(formula = Surv(time, status) ~ IsBorrowerHomeowner, ## data = loan_filtered, rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## IsBorrowerHomeowner=False 3342 1001 926 6.13 19.4 ## IsBorrowerHomeowner=True 1581 362 437 12.98 19.4 ## ## Chisq= 19.4 on 1 degrees of freedom, p= 1.04e-05 survdiff(Surv(time, status) ~ IsBorrowerHomeowner, data = loan_filtered, rho = 1)# peto &amp; peto ## Call: ## survdiff(formula = Surv(time, status) ~ IsBorrowerHomeowner, ## data = loan_filtered, rho = 1) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## IsBorrowerHomeowner=False 3342 846 774 6.71 24.8 ## IsBorrowerHomeowner=True 1581 294 366 14.18 24.8 ## ## Chisq= 24.8 on 1 degrees of freedom, p= 6.37e-07 # with more than 2 groups survdiff(Surv(time, status) ~ CreditGrade, data = loan_filtered) ## Call: ## survdiff(formula = Surv(time, status) ~ CreditGrade, data = loan_filtered) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## CreditGrade=A 428 42 115.5 46.77 51.8 ## CreditGrade=AA 481 21 115.0 76.81 85.1 ## CreditGrade=B 535 88 156.1 29.74 34.0 ## CreditGrade=C 749 141 237.3 39.05 48.9 ## CreditGrade=D 808 195 240.6 8.63 10.6 ## CreditGrade=E 929 339 254.6 27.99 35.0 ## CreditGrade=HR 915 487 223.8 309.49 378.5 ## CreditGrade=NC 78 50 20.2 44.09 47.7 ## ## Chisq= 597 on 7 degrees of freedom, p= 0 If the null hyphotesis is rejected, we can apply a post-hoc analysis. One approach would be to perform pairwise comparisons. This can be achieved with the pairwise_survdiff function of the package survminer which calculates pairwise comparisons between group levels with corrections for multiple testing. Use the function pairwise_survdiff of the library survminer in order to perform pairwise comparisons. More beaitiful plots… autoplot(model) #using ggplot2 survminer::ggsurvplot(model) survminer::ggsurvplot(model, conf.int = TRUE) References "],
["pros-and-cons-of-the-kaplan-meirs-estimator.html", "2.4 Pros and Cons of the Kaplan-Meirs estimator", " 2.4 Pros and Cons of the Kaplan-Meirs estimator Pros: It is commonly used to describe survival. It is commonly used to compare two study populations. It is intuitive graphical presentation. Cons: It is mainly descriptive. It does not control for covariates. It can not accommodate time-dependent variables. With your dataset, obtain the estimated survival curve with the Kaplan-Meier estimator for the time-to-event “bring the payroll to the BBVA”. Try to find some differences between type of client. "],
["cox.html", "Chapter 3 The Cox Proportional Hazards Model", " Chapter 3 The Cox Proportional Hazards Model This Chapter describes the Cox Proportional Hazards model, a very popular statistical model used for analyzing survival data. As we seen before, the survival function is the probability that the time-to-event will be greater than some specified time and this probability depends on: the underlying hazard function (how the risk of occurs the event per unit time changes over time at baseline covariates) the effect parameters (how the hazard varies in response to the covariates) We are going to use the Cox proportional hazards model to determine the effect of the covariates on survival. "],
["the-semiparametric-model.html", "3.1 The semiparametric model", " 3.1 The semiparametric model A parametric survival model is one in which survival time (the outcome) is assumed to follow a known distribution. Examples of distributions that are commonly used for survival time are: the Weibull, the exponential (a special case of the Weibull), the log-logistic, the log-normal, etc. The Cox proportional hazards model, by contrast, is not a fully parametric model. Rather it is a semi-parametric model because even if the regression parameters (the betas) are known, the distribution of the outcome remains unknown. The baseline survival (or hazard) function is not specified in a Cox model (we do not assume any shape or form). As before, let \\(T\\) denote the time to some event. Our data, based on a sample of size \\(n\\), consists of the triple \\((\\widetilde{T}_i, \\Delta_i, \\textbf{X}_i\\), \\(i = 1,...,n\\) where \\(\\widetilde{T}_i\\) is the time on study for the \\(i\\)-th patient, \\(\\Delta_i\\) is the event indicator for the \\(i\\)-th patient (\\(\\Delta_i=1\\) if the event has occurred and \\(\\Delta_i=0\\) if the lifetime is right-censored) and \\(\\textbf{X}_i= (X_{i1},\\ldots, X_{ip})^t\\) is the vector of covariates or risk factors for the \\(i\\)-th individual which may affect the survival distribution of \\(T\\). Note that the covariates \\(X_{ij}\\), with \\(j = 1, \\ldots, p\\), may be time-dependent as \\(\\textbf X_i(t)=(X_{i1},\\ldots,X_{ip})^t\\) whose value changes over time. This situation must be analyzed using the Extended Cox PH model. However, for ease of presentation, we shall consider the fixed-covariate case. The Cox PH regression model (Cox 1972) is usually written in terms of the hazard model formula as follows \\[ h(t, \\textbf X) = h_0(t) e^{\\sum_{j=1}^p \\beta_j X_j}. \\] This model gives an expression for the hazard at time \\(t\\) for an individual with a given specification of a set of explanatory variables denoted by the bold \\(\\textbf X\\). Based on this model we can say that the hazard at time \\(t\\) is the product of two quantities: The first of these, \\(h_0(t)\\), is called the baseline hazard function or the hazard for a reference individual with covariate values 0. The second quantity is a parametric component which is a linear function of a set of \\(p\\) explanatory \\(X\\) variables that is exponentiated (it will be the relative risk associated with covariate values \\(X\\)). Note that an important feature of this model, which concerns the proportional hazards (PH) assumption, is that the baseline hazard is a function of \\(t\\), but does not involve the covariates. By contrast, the exponential expresion involves the \\(X\\)’s but not the time. The covariates here have a multiplicative effect and are called time-independent.3 Note that the model is assuming proportional hazards (the hazard for any individual \\(i\\) is a fixed proportion of the hazard for any other individual \\(j\\)), that is: \\[ \\frac{h_i(t|\\textbf X_i)}{h_j(t|\\textbf X_j)} = exp(\\boldsymbol \\beta(\\textbf X_i - \\textbf X_j)) \\] or \\[ h_i(t|\\textbf X_i) = \\exp( \\boldsymbol \\beta(\\textbf X_i - \\textbf X_j)) h_j(t|\\textbf X_j) \\] so hazard functions for each individual should be strictly parallel and the hazard ratio is constant over time. References "],
["estimation.html", "3.2 Estimation", " 3.2 Estimation The estimation of the model is obtained by Maximun Likelihood, particularly maximazing the “partial” likelihood function rather than a (complete) likelihood function. The term “partial” likelihood is used because the likelihood formula considers probabilities only for those subjects who fail, and does not explicitly consider probabilities for those subjects who are censored. The “partial” likekihood is given by: \\[ L(\\boldsymbol \\beta) = \\prod_{i:\\Delta_i = 1} \\frac{\\exp\\bigg[ \\sum_{j=1}^{p}\\beta_j X_{(i)j} \\bigg]}{\\sum_{k \\in R(t_i)} \\exp \\bigg[ \\sum_{j=1}^{p}\\beta_j X_{(k)j} \\bigg]} \\] being \\(t_1 &lt; t_2 &lt; \\ldots &lt; t_D\\) the ordered event times, \\(X_{(i)j}\\) the \\(j\\)-th covariate associated with the individual whose failure time is \\(t_i\\) and \\(R(t_i)\\) the risk set at time \\(t_i\\), that is, the the set of all individuals who are still under study at a time just prior to \\(t_i\\). Note that the numerator of the likelihood depends only on information from the individual who experiences the event, whereas the denominator uses information about all individuals who have not yet experienced the event (including some individuals who will be censored later). The (partial) maximum likelihood estimates are found by maximizing the \\(ln (L(\\boldsymbol \\beta))\\) particularly, by taking partial derivatives of \\(ln (L(\\boldsymbol \\beta))\\) with respect to each parameter in the model, and then solving a system of equations. For this algorithm such as Newton–Raphson (Ypma 1995) or Expectation-Maximitazion (Dempster, Laird, and Rubin 1977) are used.4 In R, we can estimate this model using the coxph function of the survival package. loan_filtered$LoanOriginalAmount2 &lt;- loan_filtered$LoanOriginalAmount/10000 model &lt;- coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner + IncomeVerifiable, data = loan_filtered) For taking the ties into account we can use the method argument coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner + IncomeVerifiable, data = loan_filtered, method = &quot;efron&quot;) coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner + IncomeVerifiable, data = loan_filtered, method = &quot;breslow&quot;) coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner + IncomeVerifiable, data = loan_filtered, method = &quot;exact&quot;) References "],
["computing-the-hazard-ratio.html", "3.3 Computing the Hazard Ratio", " 3.3 Computing the Hazard Ratio One of the main goals of the Cox PH model is to compare the hazard rates of individuals who have different values for the covariates. The idea is that we care more about comparing groups than about estimating absolute survival. To this end, we are going to use the Hazard Ratio (HR). A hazard ratio is defined as the hazard for one individual divided by the hazard for a different individual. The two individuals being compared can be distinguished by their values for the set of predictors, that is, the \\(X\\)’s. We can write the hazard ratio as the estimate of \\[ \\widehat{HR} = \\frac{\\hat h_i(t|\\textbf X_i)}{h_j(t|\\textbf X_j)} = \\frac{\\hat h_0(t) \\exp (\\boldsymbol{\\hat \\beta} \\textbf X_i)}{\\hat h_0(t) \\exp (\\boldsymbol{\\hat \\beta}\\textbf X_j)}=exp(\\boldsymbol{\\hat \\beta}(\\textbf X_i - \\textbf X_j)). \\] Additionally, we can construct a \\((1-\\alpha)\\)% confidence interval for the hazard ratio as \\[ \\exp( \\boldsymbol{\\hat \\beta}(\\textbf X_i - \\textbf X_j) \\pm z_{1-\\alpha/2} \\hspace{0.2cm} \\widehat{se}(\\boldsymbol{\\hat \\beta}(\\textbf X_i - \\textbf X_j)), \\] where \\(\\widehat{se}(\\boldsymbol{\\hat \\beta}(\\textbf X_i - \\textbf X_j))\\) is equal to \\(\\sqrt{ \\widehat{Var}(\\boldsymbol{\\hat \\beta}(\\textbf X_i - \\textbf X_j))}\\). In order to understand what this hazard ratio means, we are going to see same examples. In the first one we are using a discrete predictor (smoking) and we will see the hazard ratio for smoking versus not smoking adjusted by the age. So, let \\(\\textbf X_i:(smoking=1, age = 60)\\) and \\(\\textbf X_j:(smoking=0, age = 60)\\), the hazard ratio is \\[ HR= \\frac{h_i(t|\\textbf X_i)}{h_j(t|\\textbf X_j)} = \\frac{h_0(t) e^{\\beta_{smoking} \\cdot 1 + \\beta_{age} \\cdot 60}}{h_0(t) e^{\\beta_{age} \\cdot 60}} = e^ {\\beta_{smoking}} \\] For example, if \\(\\beta_{smoking}= 0.5\\), the hazard ratio for smoking adjusted for age will be \\(exp(0.5)= 1.65\\). That is, the hazard of death increases 65% for smokers. In the second example we use a continuous predictor, age of the individuals. Let \\(\\textbf X_i:(smoking=0, age = 70)\\) and \\(\\textbf X_j:(smoking=0, age = 60)\\), the hazard ratio for a ten years increase in age adjusted by smoking is \\[ HR= \\frac{h_i(t|\\textbf X_i)}{h_j(t|\\textbf X_j)} = \\frac{h_0(t) e^{\\beta_{smoking} \\cdot 0 + \\beta_{age} \\cdot 70}}{h_0(t) e^{\\beta_{smoking} \\cdot 0+ \\beta_{age} \\cdot 60}} = e^{\\beta_{age}(70-60)} = e^{\\beta_{age}\\cdot 10 = (e^{\\beta_{age}})^{10} } \\] Note that \\(e^{\\beta_{age}}\\) is the hazard ratio for a 1-unit increase in the predictor. Interpretation of the hazard ratio (like Odds Ratio in Logistic Models) HR = 1: no effect HR &gt; 1: increase in the hazard HR &lt; 1: reduction in the hazard Moving again on the R code, we can see (by means of the summary function) the hazard ratios for the covariates included in the model m1 &lt;- coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner + IncomeVerifiable, data = loan_filtered) summary(m1) ## Call: ## coxph(formula = Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner + ## IncomeVerifiable, data = loan_filtered) ## ## n= 4923, number of events= 1363 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## LoanOriginalAmount2 -0.12177 0.88535 0.06661 -1.828 0.0675 . ## IsBorrowerHomeownerTrue -0.24815 0.78025 0.06231 -3.982 6.82e-05 *** ## IncomeVerifiableTrue 0.29263 1.33995 0.30286 0.966 0.3339 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## LoanOriginalAmount2 0.8854 1.1295 0.7770 1.0088 ## IsBorrowerHomeownerTrue 0.7802 1.2816 0.6905 0.8816 ## IncomeVerifiableTrue 1.3399 0.7463 0.7401 2.4260 ## ## Concordance= 0.558 (se = 0.008 ) ## Rsquare= 0.005 (max possible= 0.988 ) ## Likelihood ratio test= 24.46 on 3 df, p=1.999e-05 ## Wald test = 23.33 on 3 df, p=3.44e-05 ## Score (logrank) test = 23.46 on 3 df, p=3.238e-05 termplot(m1, terms = &quot;IsBorrowerHomeowner&quot;) The estimated hazard ratio for IsBorrowerHomeowner == True vs IsBorrowerHomeowner == False is 0.78 with a 95% CI of (0.69, 0.88), that is, IsBorrowerHomeowner == True has 0.78 times the hazard of IsBorrowerHomeowner == False, a 22% lower hazard rate. The estimated hazard ratio for IsBorrowerHomeowner == False vs IsBorrowerHomeowner == True is 1.28. Note that the procedure is the same for the other covariates. "],
["hypothesis-testing.html", "3.4 Hypothesis testing", " 3.4 Hypothesis testing In order to test the significance of a variable or a interaction term in the model we can use two procedures: the Wald test (typically used with Maximun Likelihood estimates) the Likelihood Ratio test (LRT) (it uses the log likelihood to compare two nested models) The null hypothesis of the Wald test states that the coeficient \\(\\beta_j\\) is equal to 0. The test statistics is \\[ Z = \\frac{\\hat \\beta_j - 0}{Std. Err (\\hat \\beta_j)} \\sim N(0,1) \\] summary(m1)$coef ## coef exp(coef) se(coef) z ## LoanOriginalAmount2 -0.1217675 0.8853542 0.06661063 -1.828049 ## IsBorrowerHomeownerTrue -0.2481456 0.7802463 0.06231124 -3.982357 ## IncomeVerifiableTrue 0.2926323 1.3399500 0.30286111 0.966226 ## Pr(&gt;|z|) ## LoanOriginalAmount2 6.754227e-02 ## IsBorrowerHomeownerTrue 6.823526e-05 ## IncomeVerifiableTrue 3.339311e-01 # by hand... for IncomeVerifiable z &lt;- summary(m1)$coef[3, 1]/summary(m1)$coef[3, 3] pvalue &lt;- 2 * pnorm(z, lower.tail = FALSE) pvalue ## [1] 0.3339311 According to the pvalue of the test, the null hypothesis is accepted (for the IncomeVerifiable variable). Thus, the model must not include this variable. The other approach is to use the Likelihood Ratio test. In this case, we need to compute the difference between the log likelihood statistic of the reduced model which does not contain the variable that we want to test and the log likelihood statistic of the full model containing the variable. In general, the LRT statistic can be written in the form of \\[ LRT = -2 ln \\frac{L_R}{L_F}= 2 ln(L_F) - 2 ln(L_R) \\sim \\chi^2_p \\] where \\(L_R\\) denotes the log likelihood of the reduced model with \\(k\\) parameter and \\(L_F\\) is the log likelihood of the full model with \\(k + p\\) parameters. \\(\\chi^2_p\\) is a Chi-square with \\(p\\) degrees of freedom, where \\(p\\) denotes the number of predictors being assessed. In general, the Likelihood Ratio test and Wald statistics may not give exactly the same answer. It has been shown that of the two test procedures, the LR statistic has better statistical properties, so when in doubt, you should use the LRT. m_red &lt;- coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner, data = loan_filtered) anova(m_red, m1) #fist the reduced, second the full ## Analysis of Deviance Table ## Cox model: response is Surv(time, status) ## Model 1: ~ LoanOriginalAmount2 + IsBorrowerHomeowner ## Model 2: ~ LoanOriginalAmount2 + IsBorrowerHomeowner + IncomeVerifiable ## loglik Chisq Df P(&gt;|Chi|) ## 1 -10837 ## 2 -10836 1.0297 1 0.3102 # by hand... for IncomeVerifiable variable m1$loglik # the first is the log likelihood of a model that contains ## [1] -10848.75 -10836.52 # none of the predictors, so we need the second one chi &lt;- 2 * m1$loglik[2] - 2 * m_red$loglik[2] pvalue &lt;- 1 - pchisq(chi, df = 1) # df = 3 - 2 pvalue ## [1] 0.310227 In this case, using an \\(\\alpha = 0.05\\) and testing the significance of the IncomeVerifiable variable, we must remove it from the model. "],
["adjusting-survival-curves.html", "3.5 Adjusting Survival Curves", " 3.5 Adjusting Survival Curves From a survival analysis point of view, we want to obtain also estimates for the survival curve. Remember that if we do not use a model, we can apply the Kaplan-Meier estimator. However, when a Cox model is used to fit survival data, survival curves can be obtained adjusted for the explanatory variables used as predictors. These are called adjusted survival curves and, like Kaplan-Meier curves, these are also plotted as step functions. The hazard formula seen before can be converted to a survival function as \\[ S(t|\\textbf X) = \\bigg[ S_0(t) \\bigg]^{e^{\\sum_{j=1}^p \\beta_j X_j}}. \\] This survival function formula is the basis for determining adjusted survival curves. The estimates of \\(\\hat S_0(t)\\) and \\(\\hat b_j\\) are provided by the computer program that fits the Cox model. The \\(X\\)’s, however, must first be specified by the investigator before the computer program can compute the estimated survival curve. Typically, when computing adjusted survival curves, the value chosen for a covariate being adjusted is an average value like an arithmetic mean or a median. The survfit function estimates \\(S(t)\\), by default at the mean values of the covariates: m2 &lt;- m_red newdf &lt;- data.frame(IsBorrowerHomeowner = levels(loan_filtered$IsBorrowerHomeowner), LoanOriginalAmount2 = rep(mean(loan_filtered$LoanOriginalAmount2), 2)) fit &lt;- survfit(m2, newdata = newdf) #summary(fit) # to see the estimated values plot(fit, conf.int = TRUE, col = c(1,2)) legend(&quot;bottomleft&quot;, levels(newdf[,1]), col = c(1, 2), lty = c(1,1)) # another option using the survminer package survminer::ggsurvplot(fit, data = newdf) # easier... without refitting #survminer::ggadjustedcurves(m2, data = loan_filtered, # variable = loan_filtered$IsBorrowerHomeowner) For some help with the survminer package… Download the cheatsheet here. Try to estimate a Cox PH model using your dataset. "],
["how-to-evaluate-the-ph-assumption.html", "3.6 How to evaluate the PH assumption?", " 3.6 How to evaluate the PH assumption? Now we are going to illustrate two methods to evaluate the proportional hazards assumptions: one graphical approach and one goodness-of-fit test. Recall that the Hazard Ratio that compares two specifications of the covariates (defined as \\(\\textbf{X}^*\\) and \\(\\textbf{X}\\)) can be expressed as \\[ HR = \\exp(\\sum_{j=1}^p \\beta_j (X_j^* - X_j)) \\] where \\(\\textbf{X}^*=(X_1^*, X_2^*, \\ldots, X_j^*)\\) and \\(\\textbf{X}=(X_1, X_2, \\ldots, X_j)\\), and proportionally of hazards assumption indicates that this quantity is constant over time. Equivalently, this means that the hazard for one individual is proportional to the hazard for any other individual, where the proportionality constant is independent of time. Think about this… It is important to note that if the graph of the hazards cross for two or more categories of a predictor of interest, the PH assumption is not met. However, althought the hazard functions do not cross, it is possible that the PH assumption is not met. Thus, rather than checking for crossing hazards, we need to use other apporaches. 3.6.1 Graphical approach The most popular graphical techniques for evaluating the PH assumption involves comparing estimated –ln(–ln) survival curves over different (combinations of) categories of variables being investigated. A log–log survival curve is simply a transformation of an estimated survival curve that results from taking the natural log of an estimated survival probability twice.5 As we said, the hazard function can be rewritten as \\[ S(t|\\textbf X) = \\bigg[ S_0(t) \\bigg]^{e^{\\sum_{j=1}^p \\beta_j X_j}} \\] and once we applied the -ln(-ln), the expression can be rewritten as \\[ -\\ln \\bigg[-\\ln S(t|\\textbf X) \\bigg] = - \\sum_{j=1}^p \\beta_j X_j - \\ln \\bigg[-\\ln S_0(t|\\textbf X) \\bigg]. \\] Now, considering two different specifications of the covariates, corresponding to two different individuals, \\(\\textbf X_1\\) and \\(\\textbf X_2\\), and subtracting the second log–log curve from the first yields the expression \\[ -\\ln \\bigg[-\\ln S(t|\\textbf X_1) \\bigg] = -\\ln \\bigg[-\\ln S(t|\\textbf X_2) \\bigg] + \\sum_{j=1}^p \\beta_j (X_{1j} - X_{2j}) \\] This expression indicates that if we use a Cox model (well-used) and plot the estimated log-log survival curves for individuals on the same graph, the two plots would be approximately parallel. The distance between the two curves is the linear expression involving the differences in predictor values, which does not involve time. Note that there is an important problem associated with this approach, that is, how to decide “how parallel is parallel?”. This fact can be subjective, thus the proposal is to be conservative for this decision by assuming the PH assumption is satisfied unless there is strong evidence of nonparallelism of the log–log curves. Now we are going to check the proportinal hazards assumption for the variable IsBorrowerHomeowner. This can be done by plotting log-log Kaplan Meier survival estimates against time (or against the log of time) and evaluating whether the curves are reasonably parallel. km_home &lt;- survfit(Surv(time, status) ~ IsBorrowerHomeowner, data = loan_filtered) #autoplot(km_home) # just to see the km curves plot(km_home, fun = &quot;cloglog&quot;, xlab = &quot;Time (in days) using log&quot;, ylab = &quot;log-log survival&quot;, main = &quot;log-log curves by clinic&quot;) # another option ggsurvplot(km_home, fun = &quot;cloglog&quot;) It seems that the proportional hazards assumption is violated as the log-log survival curves are not parallel. Another graphical option could be to use the Schoenfeld residuals to examine model fit and detect outlying covariate values. Shoenfeld residuals represent the difference between the observed covariate and the expected given the risk set at that time. They should be flat, centered about zero. You can see the explanation in this paper. The main idea is that he defined a partial residual as the different between the observed value of \\(X_i\\) and its conditional expectation given the risk set \\(R_i\\) and demostrated that these residuals have to be independent of the time. So, if you represent them ranked by its event time, this plot must not show any pattern. ggcoxdiagnostics(m2, type = &quot;schoenfeld&quot;) # another option zph &lt;- cox.zph(m2) par(mfrow = c(1, 2)) plot(zph, var = 1) plot(zph, var = 2) 3.6.2 Goodness-of-fit test A second approach for assessing the PH assumption involves goodness-of-fit (GOF) tests. To this end, different test have been proposed in the literature (Grambsch and Therneau 1994). We focuss in the Harrell (1986), a variation of a test originally proposed by Schoenfeld (1982). This is a test of correlation between the Schoenfeld residuals and survival time. A correlation of zero indicates that the model met the proportional hazards assumption (the null hypothesis). This can be applied by means of the cox.zph function of the survival package. cox.zph(m2) ## rho chisq p ## LoanOriginalAmount2 0.130 27.1 1.96e-07 ## IsBorrowerHomeownerTrue 0.103 14.0 1.81e-04 ## GLOBAL NA 49.3 1.96e-11 It seems again that the proportional hazards assumption is not satisfied (as we saw with the log-log survival curves). References "],
["non-proportional-hazards-and-now-what.html", "3.7 Non-Proportional Hazards… and now what?", " 3.7 Non-Proportional Hazards… and now what? A insignificant nonproportionality may make no difference to the interpretation of a dataset, particularly for large sample sizes. What if the nonproportionality is large and real? Possible approaches are possible in the context of the Cox model itself: Stratify. Covariates with nonproportional effects may be incorporated into the model as stratification factors rather than predictors (but… be careful, stratification works naturally for categorical variables, however for quantitative variables you would have to discretize). Partition of the time axis, if the proportional hazards assumption holds for short time periods but not for the entire study. Nonlinear effect. Continuous covariates with nonlinear effect may lead to nonproportional effects. 3.7.1 An example… Stratified Proportional Hazards Models Sometimes the proportional hazard assumption is violated for some covariate. In such cases, it is possible to stratify taking this variable into account and use the proportional hazards model in each stratum for the other covariates. We include in the model predictors that satify the proportional hazard assumption and remove from it the predictor that is stratified. Now, the subjects in the \\(z\\)-th stratum have an arbitrary baseline hazard function \\(h_{0z}(t)\\) and the effect of other explanatory variables on the hazard function can be represented by a proportional hazards model in that stratum \\[ h_z(t, \\textbf X) = h_{0z}(t) e^{\\sum_{j=1}^p \\beta_j X_j} \\] with \\(z = 1, \\ldots, k\\) levels of the variable that is stratified. In the Stratified Proportional Hazards Model the regression coefficients are assumed to be the same for each stratum although the baseline hazard functions may be different and completely unrelated. m3 &lt;- coxph(Surv(time, status) ~ LoanOriginalAmount2 + strata(IsBorrowerHomeowner), data = loan_filtered) summary(m3) ## Call: ## coxph(formula = Surv(time, status) ~ LoanOriginalAmount2 + strata(IsBorrowerHomeowner), ## data = loan_filtered) ## ## n= 4923, number of events= 1363 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## LoanOriginalAmount2 -0.11967 0.88721 0.06667 -1.795 0.0726 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## LoanOriginalAmount2 0.8872 1.127 0.7785 1.011 ## ## Concordance= 0.54 (se = 0.01 ) ## Rsquare= 0.001 (max possible= 0.983 ) ## Likelihood ratio test= 3.35 on 1 df, p=0.06731 ## Wald test = 3.22 on 1 df, p=0.07264 ## Score (logrank) test = 3.22 on 1 df, p=0.07255 You can see that the output is similar to previous model without stratification however, in this case, we do not have information about the hazard ratio of the stratification variable, IsBorrowerHomeowner. This variable is not really in the model. In any case, you can plot it… ggsurvplot(survfit(m3), data = loan_filtered, conf.int = TRUE) Check the proportional hazard assumption of the Cox PH model estimated in your dataset. "],
["why-cox-ph-model-is-so-popular-pros-of-the-model.html", "3.8 Why Cox PH model is so popular? (pros of the model)", " 3.8 Why Cox PH model is so popular? (pros of the model) It is a “robust” model, so that the results from using the Cox model will closely approximate the results for the correct parametric model (even though the baseline hazard is not specified). Although the baseline hazard part of the model is unspecified, we can estimate the betas in the exponential part of the model (as we have seen). Then, the hazard function \\(h(t,\\textbf X)\\) and its corresponding survival curves \\(S(t, \\textbf X\\)) can also be estimated. Finally, it is preferred over the logistic model when survival time information is available and there is censoring. Because you can obtain more information! "],
["bonus-track-1-additive-cox-model.html", "3.9 Bonus track 1: Additive Cox model", " 3.9 Bonus track 1: Additive Cox model The Cox PH model assumes a linear effect of the predictors. If the true effect is highly nonlinear this can lead to a nonproportinal hazards or misleading statistical conclusions. One alternative approach is to use an Additive Cox model (Hastie and Tibshirani 1990) of the form \\[ h(t, \\textbf X) = h_0(t) e^{\\sum_{j=1}^p f_j(\\textbf X_j)} \\] with \\(f_j\\) being an unknown and smooth function. In order to estimate this model one could use the mgcv package as follows m4 &lt;- mgcv::gam(time ~ s(LoanOriginalAmount2) + IsBorrowerHomeowner, data = loan_filtered, family = &quot;cox.ph&quot;, weights = status) summary(m4) ## ## Family: Cox PH ## Link function: identity ## ## Formula: ## time ~ s(LoanOriginalAmount2) + IsBorrowerHomeowner ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## IsBorrowerHomeownerTrue -0.23344 0.06248 -3.736 0.000187 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(LoanOriginalAmount2) 4.853 5.857 26.91 0.000206 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Deviance explained = 0.818% ## -REML = 10848 Scale est. = 1 n = 4923 plot(m4, pages = 1, all.terms = TRUE) Note the change in the sintaxis compared with the previous examples. The status indicator in used in the weights argument. References "],
["bonus-track-2-machine-learning-for-estimating-the-cox-ph-model.html", "3.10 Bonus track 2: Machine Learning for estimating the Cox PH model", " 3.10 Bonus track 2: Machine Learning for estimating the Cox PH model The rpart package builds R’s basic tree models of survival data. For an overview you can consult the section 8.4 of the rpart vignette. Additionally, the new package ranger (Wright and Ziegler 2017) is a fast implementation of the Random Forests algorithm for building ensembles of classification and regression trees, working also with survival data. Since ranger() uses standard `Surv survival objects, it’s an ideal tool for getting acquainted with survival analysis in this machine-learning age. Under construction… References "],
["joint-models-for-longitudinal-and-time-to-event-data.html", "Chapter 4 Joint Models for Longitudinal and Time-to-Event Data", " Chapter 4 Joint Models for Longitudinal and Time-to-Event Data In this Chapter we will see a joint modelling approach in order to analyze two types of outcomes produced usually in longitudinal studies, particularly, a set of longitudinal response measurements and the time to an event of interest, such as default, death, etc. These two outcomes are usually analyzed separately, using a mixed effects model (Verbeke and Molenberghs 2000) for the longitudinal response and a survival model for the time-to-event. Here, we are going to see how we can analyze them jointly. Why should I use these type of models? As we mentioned in Chapter 3, the Cox PH hazard model can be extended in order to incorporate time-dependent variables. However, when we focus our interest in the time-to-event and we wish to take into account the effect of the longitudinal variable as a time-dependent covariate, traditional approaches for analyzing time-to-event data (such as the partial likelihood for the Cox proportional hazards models) are not applicable in all situations. In particular, standard time-to-event models require that time-dependent covariates are external; that is, the value of this covariate at time point \\(t\\) is not affected by the occurrence of an event at time point \\(u\\), with \\(t &gt; u\\) (Kalbfleisch and Prentice 2002, Section 6.3). However, the type of time-dependent covariates that we have in longitudinal studies do not met this condition, this is due to the fact that they are the output of a stochastic process generated by the subject, which is directly related to the failure mechanism. Based on this, in order to produce correct inferences, we need to apply a joint model that takes into account the joint distribution of the longitudinal and survival outcomes. Another advantage of these models is that they allow to deal with the error measurements in the time dependent variables (longitudinal variable in this case). In a Cox model with time dependent covariates we assume that the variables are measured without error. When we think in time-dependent covariates, we should first distinguish between two different categories, namely, internal or endogenous covariates or external or exogenous covariates. Internal covariates are generated from the patient herself and therefore require the existence of the patient, for example CD4 cell count and the hazard for death by HIV are stochastic processes generated by the patient herself. On the other hand, air pollution is an external covariate to asthma attacks, since the patient has no influence on air pollution. References "],
["linear-mixed-models.html", "4.1 Linear Mixed Models", " 4.1 Linear Mixed Models As we mentioned, Joint Models take two outcomes into account, the longitudinal response and the survival time. In order to estimate these type of models, we need first to fit a model for the longitudinal response (usually a linear mixed model) and then for the survival time. I am assuming here that you have understood entirely the Chapter 3 and you do not have any problem with the estimation of the Cox model by means of the coxph function. Regarding the linear mixed model you can see an brief introduction with examples below using the nlme package. For a good overview you can consult the Chapter 2 of Rizopoulos (2012). So, our focus in this part is on longitudinal data. This data can be defined as the data resulting from the observations of subjects (e.g., human beings, animals, etc.) that are measured repeatedly over time. From this descriptions, it is evident that in a longitudinal setting we expect repeated measurements taken on the same subject to exhibit positive correlation. This feature implies that standard statistical tools, such as the t-test and simple linear regression that assume independent observations, are not appropriate for longitudinal data analysis (they may produce invalid standard errors). In order to solve this situation and obtain valid inference, one possible approach is to use a mixed model, a regression method for continuous outcomes that models longitudinal data by assuming, for example, random errors within a subject and random variation in the trajectory among subjects. We are going to explain briefly this approach. Figure 4.1 shows an example with hypothetical longitudinal data for two subjects. In this figure, monthly observations are recorded for up to one year. Note that each subject appears to have their own linear trajectory but with small fluctuations about the line. This fluctuations are referred to as the within-subject variation in the outcomes. Note that if we only have data from one person these will be the typical error term in regression. The dashed line in the center of the figure shows the average of individual linear-time trajectories. This line characterizes the average for the population as a function of time. For example, the value of the dashed line at month 2 is the mean response if the observation (at two months) for all subjects was averaged. Thus, this line represents both the typical trajectory and the population average as a function of time. The main idea of Linear Mixed Model is that they make specific assumptions about the variation in observations attributable to variation within a subject and to variation among subjects. To formally introduce this representation of longitudinal data, we let \\(Y_{ij}\\) denote the response of subject \\(i, i = 1, \\ldots, n\\) at time \\(X_{ij}, j = 1,...,n_i\\) and \\(\\beta_{i0} + \\beta_{i1} X_{ij}\\) denote the line that characterizes the observation path for \\(i\\). Note that each subject has an individual-specific intercept and slope. Note that The within-subject variation is seen as the deviation between individual observations, \\(Y_{ij}\\), and the individual linear trajectory, that is \\(Y_{ij} - (\\beta_{i0} + \\beta_{i1} X_{ij})\\). The between-subject variation is represented by the variation among the intercepts, \\(var(\\beta_{i0})\\) and the variation among subject in the slopes \\(var(\\beta_{i1})\\). Figure 4.1: Hypothetical longitudinal data for two subjects. Figure 4.1 taken from Belle et al. (2004). If parametric assumptions are made regarding the within- and between-subject components of variation, it is possibel to use maximum likelihood methods for estimating the regression parameters (which characterize the population average), and the variance components (which characterize the magnitude of within- and between-subject heterogeneity). For continuous outcomes it is a good idea to assume that within-subject errors are normally distributed and to assume that intercepts and slopes are normally distributed among subjects. This will be within-subjects: \\[ E(Y_{ij}|\\beta_i) = \\beta_{i,0} + \\beta_{i, 1} X_{ij} \\] \\[ Y_{ij} = \\beta_{i,0} + \\beta_{i, 1} X_{ij} + \\varepsilon_{ij} \\] \\[ \\varepsilon_{ij} \\sim N(0, \\sigma^2) \\] between-subjects: \\[ \\bigg(\\begin{array}{c} \\beta_{i,0}\\\\ \\beta_{i,1}\\\\ \\end{array} \\bigg) \\sim N \\bigg[ \\bigg(\\begin{array}{c} \\beta_{0}\\\\ \\beta_{1}\\\\ \\end{array} \\bigg), \\bigg(\\begin{array}{c} D_{00} &amp; D_{01}\\\\ D_{10} &amp; D_{11}\\\\ \\end{array} \\bigg) \\bigg] \\] where \\(D\\) is the variance-covariance matrix of the random effects, with \\(D_{00}= var(b_{i,0})\\) and \\(D_{11}= var(b_{i,1})\\). If we think in \\(b_{i,0}= (\\beta_{i,0} - \\beta_0)\\) and \\(b_{i,1}= (\\beta_{i,1} - \\beta_1)\\), the model can be written as \\[ Y_{ij} = \\beta_0 + \\beta_1 X_{ij} + b_{i,0} + b_{i,1} X_{ij} + \\varepsilon_{ij} \\] where \\(b_{i,0}\\) and \\(b_{i,1}\\) represent deviations from the population average intercept and slope respectively. Note taht in this equation there is a systematic variation (given by the two first betas) and a random variation (the rest). Additionally, the random component is partitioned into the observation level and subject level fluctuations: that is, the between-subject (\\(b_{i,0} + b_{i,1} X_{ij}\\)) and within-subject (\\(\\varepsilon_{ij}\\)) variations. A more general form including \\(p\\) predictors is \\[ Y_{ij} = \\beta_0 + \\beta_1 X_{ij,1} +\\ldots + + \\beta_p X_{ij,p} + b_{i,0} + b_{i,1} X_{ij,1} + \\ldots + b_{i,p} X_{ij,p}+ \\varepsilon_{ij} \\] \\[ Y_{ij} = X_{ij}&#39;\\beta + Z_{ij}&#39; b_i + \\varepsilon_{ij} \\] where \\(X_{ij}&#39;=[X_{ij,1}, X_{ij,2}, \\ldots, X_{ij,p}]\\) and \\(Z_{ij}&#39;=[X_{ij,1}, X_{ij,2}, \\ldots, X_{ij,q}]\\). In general way, we assume that the covariates in \\(Z_{ij}\\) are a subset of the variables in \\(X_{ij}\\) and thus \\(q &lt; p\\). It is important to highlighted that based on this model the coefficient of covariate \\(k\\) for subject \\(i\\) is given as \\((\\beta_k + b_{i,k})\\) if \\(k \\le q\\), and is simply \\(\\beta_k\\) if \\(q &lt; k \\le p\\). Therefore,in a linear mixed model there may be some regression parameters that vary among subjects while some regression parameters are common to all subjects. Moving again onto the example, it seems that each subject has their own intercept, but the subjects may have a common slope. So, a random intercept model assumes parallel trajectories for any two subjects and is given as a special case of the general mixed model: \\[ Y_{ij} = \\beta_0 + \\beta_1 X_{ij,1} + b_{i,0} + \\varepsilon_{ij}. \\] Using the above model, the intercept for subject \\(i\\) is given by \\(\\beta_0 + b_{i,0}\\) while the slope for subject \\(i\\) is simply \\(\\beta_1\\) since there is no additional random slope, \\(b_{i,1}\\) in the random intercept model. If we assume that the slope for each individual \\(i\\) can also be different, we have to use a random intercept and slope model of the type \\[ Y_{ij} = \\beta_0 + \\beta_1 X_{ij,1} + b_{i,0} + b_{i,1} X_{ij,1}+ \\varepsilon_{ij}. \\] and now the intercept for subject \\(i\\) is given by \\(\\beta_0 + b_{i,0}\\) while the slope for subject \\(i\\) is \\(\\beta_1 + b_{i, 1}\\). In order to fit these models, we can use the lme function of the nlme package. head(aids) ## patient Time death CD4 obstime drug gender prevOI AZT ## 1 1 16.97 0 10.677078 0 ddC male AIDS intolerance ## 2 1 16.97 0 8.426150 6 ddC male AIDS intolerance ## 3 1 16.97 0 9.433981 12 ddC male AIDS intolerance ## 4 2 19.00 0 6.324555 0 ddI male noAIDS intolerance ## 5 2 19.00 0 8.124038 6 ddI male noAIDS intolerance ## 6 2 19.00 0 4.582576 12 ddI male noAIDS intolerance ## start stop event ## 1 0 6.00 0 ## 2 6 12.00 0 ## 3 12 16.97 0 ## 4 0 6.00 0 ## 5 6 12.00 0 ## 6 12 18.00 0 # CD4: square root CD4 cell count measurements # obstime: time points at which the corresponding longitudinal response was recorded # random-intercepts model (single random effect term for each patient) fit1 &lt;- lme(fixed = CD4 ~ obstime, random = ~ 1 | patient, data = aids) summary(fit1) ## Linear mixed-effects model fit by REML ## Data: aids ## AIC BIC logLik ## 7176.633 7197.618 -3584.316 ## ## Random effects: ## Formula: ~1 | patient ## (Intercept) Residual ## StdDev: 4.506494 1.961662 ## ## Fixed effects: CD4 ~ obstime ## Value Std.Error DF t-value p-value ## (Intercept) 7.188663 0.22061320 937 32.58492 0 ## obstime -0.148500 0.01218699 937 -12.18513 0 ## Correlation: ## (Intr) ## obstime -0.194 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.84004681 -0.44310988 -0.05388055 0.43593364 6.09265321 ## ## Number of Observations: 1405 ## Number of Groups: 467 Note that the estimation for the variability or the variance components, that is, the variance of the errors (\\(\\varepsilon_{ij}\\), within personal errors) and the variance between subject (the variance of the \\(b_{i, 0}\\)) are given under Random effects heading. Under (Intercept) we can see the estimated standard desviation for the \\(b_{i,0}\\) coefficients and under Residual, the estimated desviation for \\(\\varepsilon_{ij}\\). # variance of the beta_i0 getVarCov(fit1) ## Random effects variance covariance matrix ## (Intercept) ## (Intercept) 20.308 ## Standard Deviations: 4.5065 # standard desviation of e_ij fit1$sigma ## [1] 1.961662 # total variance of the model getVarCov(fit1)[1] + fit1$sigma**2 ## [1] 24.15661 # % variance within person (fit1$sigma**2/(getVarCov(fit1)[1] + fit1$sigma**2)) * 100 ## [1] 15.92988 # % variance between person (getVarCov(fit1)[1]/(getVarCov(fit1)[1] + fit1$sigma**2)) * 100 ## [1] 84.07012 The total variation in CD4 is estimated as 24.16. So, the proportion of total variation that is attributed to within-person variability is 15.93% with 84.07% of total variation attributable to individual variation in their general level of CD4 (attributable to random intercepts). The estimated regression coefficients \\(\\beta\\) are provided under the Fixed effects heading. As expected, the coefficient for the time effect has a negative sign indicating that on average the square root CD4 cell counts declines in time. Well, this random-intercepts model poses the unrealistic restriction that the correlation between the repeated measurements remains constant over time (we are not includiying the random slope yet). So, a natural extension is a more flexible specification of the covariance structure with the random-intercepts and random-slopes model. This model introduces an additional random effects term, and assumes that the rate of change in the CD4 cell count is different from patient to patient. # random-intercepts and random-slopes model fit2 &lt;- lme(CD4 ~ obstime, random = ~ obstime | patient, data = aids) # the intercept is included by default summary(fit2) ## Linear mixed-effects model fit by REML ## Data: aids ## AIC BIC logLik ## 7141.282 7172.76 -3564.641 ## ## Random effects: ## Formula: ~obstime | patient ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 4.5898645 (Intr) ## obstime 0.1728724 -0.152 ## Residual 1.7507904 ## ## Fixed effects: CD4 ~ obstime ## Value Std.Error DF t-value p-value ## (Intercept) 7.189048 0.22215494 937 32.36051 0 ## obstime -0.150059 0.01518146 937 -9.88435 0 ## Correlation: ## (Intr) ## obstime -0.218 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.31679141 -0.41425035 -0.05227632 0.41094183 4.37413201 ## ## Number of Observations: 1405 ## Number of Groups: 467 We observe very minor differences in the estimated fixed-effect parameters compared with the previous model. Maybe the AIC For example, for the random effects, we can observe that there is greater variability between patients in the baseline levels of CD4 (given by (Intercept) variance) than in the evolutions of the marker in time (obstime variance). anova(fit1, fit2) # reduced vs full ## Model df AIC BIC logLik Test L.Ratio p-value ## fit1 1 4 7176.633 7197.618 -3584.316 ## fit2 2 6 7141.282 7172.760 -3564.641 1 vs 2 39.35067 &lt;.0001 # it is better to use the random-intercepts and random-slopes model muhat &lt;- predict(fit2) aids2 &lt;- aids aids2$muhat &lt;- muhat lattice::xyplot(muhat ~ obstime, group = patient, data = aids2[aids2$patient %in% c(1:10), ],type = &quot;l&quot;) References "],
["estimation-of-the-joint-model.html", "4.2 Estimation of the Joint Model", " 4.2 Estimation of the Joint Model In this section we are going to present the joint modelling framework motivated by the time-to-event point of view, that is, we want to add a time-dependent covariate measured with error in a survival model. Let \\(T_i\\) denote the observed failure time for the \\(i\\)-th subject \\((i = 1,...,n)\\), which is taken as the minimum of the true event time \\(T_i\\) and the censoring time \\(C_i\\), i.e., \\(\\widetilde T_i = \\min(T_i,C_i)\\). Furthermore, we define the event indicator as \\(\\Delta_i = I(T_i \\le C_i)\\), where \\(I\\) is the indicator function that takes the value 1 if the condition \\(T_i \\le C_i\\) is satisfied, and 0 otherwise. So, the observed data for the time-to-event outcome consist of the pairs \\(\\{(\\widetilde T_i, \\Delta_i), i = 1, . . . , n\\}\\). For the longitudinal responses, let \\(y_i(t)\\) denote the value of the longitudinal outcome at time point \\(t\\) for the \\(i\\)-th subject. Note that we do not actually observe \\(y_i(t)\\) at all time points, but only at the very specific occasions \\(t_{ij}\\) at which measurements were taken. Thus, the observed longitudinal data consist of the measurements \\(y_{ij} = \\{yi(t_{ij}),j = 1,...,n_i\\}\\). The objective is to associate the true and unobserved value of the longitudinal outcome at time \\(t\\), denoted by \\(m_i(t)\\), with the event outcome \\(\\widetilde T_i\\). Note that \\(m_i(t)\\) is different from \\(y_i(t)\\) because this last is contaminated with measurement error value of the longitudinal outcome at time \\(t\\). In order to quantify the effect of \\(m_i(t)\\) on the risk of an event, we can use a relative risk model of the form: \\[\\begin{equation} h_i(t|\\mathcal{M}_i(t),w_i) = h_0(t) \\exp \\{ \\gamma^t w_i + \\alpha m_i(t) \\} \\tag{4.1} \\end{equation}\\] where \\(\\mathcal{M}_i(t)=\\{ m_i(s), 0 \\le s &lt; t\\}\\) denotes the denotes the history of the true unobserved longitudinal process up to time point \\(t\\), \\(h_0(\\cdot)\\) denotes the baseline risk function, and \\(w_i\\) is a vector of baseline covariates (such as a treatment indicator, history of diseases, etc.) with a corresponding vector of regression coefficients \\(\\gamma\\). Similarly, parameter \\(\\alpha\\) quantifies the effect of the underlying longitudinal outcome to the risk for an event. The interpretation of \\(\\gamma\\) and \\(\\alpha\\) is exactly the same as we have seen in Chapter 3. In particular, \\(exp(\\gamma_j)\\) denotes the ratio of hazards for one unit change in \\(w_{ij}\\) at any time \\(t\\), whereas \\(exp(\\alpha)\\) denotes the relative increase in the risk for an event at time \\(t\\) that results from one unit increase in \\(m_i(t)\\) at the same time point. In order to complete the specification of the above model, we need to think about the choice for the baseline risk function \\(h_0(\\cdot)\\). In standard survival analysis it is customary to leave \\(h_0(\\cdot)\\) completely unspecified in order to avoid the impact of misspecifying the distribution of survival times. However, within the joint modeling framework, it turns out that following such a route may lead to an underestimation of the standard errors of the parameter estimates (Hsieh, Tseng, and Wang 2006). To avoid such problems we will need to explicitly define \\(h_0(\\cdot)\\), for example with a known parametric distribution or alternatively, and even more preferably, we can opt for a parametric but flexible specification of the baseline risk function. Several approaches are implemented in the JM package under the argument method. The longitudinal submodel In the model above we use \\(m_i(t)\\) to denote the true value of the underlying longitudinal covariate at time point \\(t\\). However, and as mentioned earlier, longitudinal information is actually collected intermittently and with error at a set of a few time points \\(t_{ij}\\) for each subject. So, to messure the effect of the longitudinal variable on the risk dor an event, we need to estimate \\(m_i(t)\\). To do this, we are going to use the linear mixed models of the form \\[ y_i(t) = m_i(t) + \\varepsilon_i(t), \\] \\[ m_i(t) = x_i^T(t)\\beta + z_i^T(t)b_i + \\varepsilon_i(t), \\] \\[ b_i \\sim N(0, D), \\quad \\varepsilon_i(t) \\sim N(0, \\sigma^2), \\] where \\(\\beta\\) denotes the vector of the unknown fixed effects parameters, \\(b_i\\) denotes a vector of random effects, \\(x_i(t)\\) and \\(z_i(t)\\) denote row vectors of the design matrices for the fixed and random effects, respectively, and \\(\\varepsilon_i(t)\\) is the measument error term, which is assumed independent of \\(b_i\\). The main estimation methods for joint models are based on (semiparametric) maximum likelihood and Bayes using MCMM techniques. The JM package that we are going to use is based on maximum likelihood. The idea is the maximization of the log-likelihood corresponding to the joint distribution of the time-to-event and longitudinal out-comes \\(\\{\\widetilde T_i,\\Delta_i,y_i\\}\\). Standard numerical integration techniques such as Gaussian quadrature and Monte Carlo have been successfully applied in the joint modelling framework. See Section 4.3 of Rizopoulos (2012) for details. References "],
["the-jm-package.html", "4.3 The JM package", " 4.3 The JM package Now it is time to fit these models in R. To this end, we need first to fit separately the linear mixed effect model and the Cox model, and then take the returned objects and use them as main arguments in the jointModel function. The dataset used is the same that the one seen with the mixed model, aids. The survival information can be found in aids.id. head(aids) ## patient Time death CD4 obstime drug gender prevOI AZT ## 1 1 16.97 0 10.677078 0 ddC male AIDS intolerance ## 2 1 16.97 0 8.426150 6 ddC male AIDS intolerance ## 3 1 16.97 0 9.433981 12 ddC male AIDS intolerance ## 4 2 19.00 0 6.324555 0 ddI male noAIDS intolerance ## 5 2 19.00 0 8.124038 6 ddI male noAIDS intolerance ## 6 2 19.00 0 4.582576 12 ddI male noAIDS intolerance ## start stop event ## 1 0 6.00 0 ## 2 6 12.00 0 ## 3 12 16.97 0 ## 4 0 6.00 0 ## 5 6 12.00 0 ## 6 12 18.00 0 head(aids.id) ## patient Time death CD4 obstime drug gender prevOI AZT ## 1 1 16.97 0 10.677078 0 ddC male AIDS intolerance ## 2 2 19.00 0 6.324555 0 ddI male noAIDS intolerance ## 3 3 18.53 1 3.464102 0 ddI female AIDS intolerance ## 4 4 12.70 0 3.872983 0 ddC male AIDS failure ## 5 5 15.13 0 7.280110 0 ddI male AIDS failure ## 6 6 1.90 1 4.582576 0 ddC female AIDS failure ## start stop event ## 1 0 6.0 0 ## 2 0 6.0 0 ## 3 0 2.0 0 ## 4 0 2.0 0 ## 5 0 2.0 0 ## 6 0 1.9 1 The idea here is to test for a treatment effect on survival after adjusting for the CD4 cell count.6 lattice::xyplot(sqrt(CD4) ~ obstime | drug, group = patient, data = aids, xlab = &quot;Months&quot;, ylab = expression(sqrt(&quot;CD4&quot;)), col = 1, type = &quot;l&quot;) lattice::xyplot(sqrt(CD4) ~ obstime | patient, group = patient, data = aids[aids$patient %in% c(1:10),], xlab = &quot;Months&quot;, ylab = expression(sqrt(&quot;CD4&quot;)), col = 1, type = &quot;b&quot;) Now we are going to specify and fit a joint model. The linear mixed effects model for the CD4 cell counts include: Fixed-effects part: main effect of time and the interaction with the treatment. random-effects design matrix: an intercept and a time term. The survival submodel include: treatment effect (as a time-independent covariate) and the true underlying effect of CD4 cell count as estimated from the longitudinal model (as time-dependent). The baseline risk function is assumed piecewise constant. fitLME &lt;- lme(sqrt(CD4) ~ obstime : drug, random = ~ obstime | patient, data = aids) fitSURV &lt;- coxph(Surv(Time, death) ~ drug, data = aids.id, x = TRUE) fitJM &lt;- jointModel(fitLME, fitSURV, timeVar = &quot;obstime&quot;, method = &quot;piecewise-PH-GH&quot;) summary(fitJM) ## ## Call: ## jointModel(lmeObject = fitLME, survObject = fitSURV, timeVar = &quot;obstime&quot;, ## method = &quot;piecewise-PH-GH&quot;) ## ## Data Descriptives: ## Longitudinal Process Event Process ## Number of Observations: 1405 Number of Events: 188 (40.3%) ## Number of Groups: 467 ## ## Joint Model Summary: ## Longitudinal Process: Linear mixed-effects model ## Event Process: Relative risk model with piecewise-constant ## baseline risk function ## Parameterization: Time-dependent ## ## log.Lik AIC BIC ## -2107.647 4247.295 4313.636 ## ## Variance Components: ## StdDev Corr ## (Intercept) 0.8660 (Intr) ## obstime 0.0388 0.0680 ## Residual 0.3754 ## ## Coefficients: ## Longitudinal Process ## Value Std.Err z-value p-value ## (Intercept) 2.5558 0.0372 68.7961 &lt;0.0001 ## obstime:drugddC -0.0423 0.0046 -9.1931 &lt;0.0001 ## obstime:drugddI -0.0372 0.0050 -7.4577 &lt;0.0001 ## ## Event Process ## Value Std.Err z-value p-value ## drugddI 0.3511 0.1537 2.2839 0.0224 ## Assoct -1.1016 0.1180 -9.3388 &lt;0.0001 ## log(xi.1) -1.6489 0.2498 -6.6000 ## log(xi.2) -1.3393 0.2394 -5.5940 ## log(xi.3) -1.0231 0.2861 -3.5758 ## log(xi.4) -1.5802 0.3736 -4.2299 ## log(xi.5) -1.4722 0.3500 -4.2069 ## log(xi.6) -1.4383 0.4283 -3.3584 ## log(xi.7) -1.4780 0.5455 -2.7094 ## ## Integration: ## method: Gauss-Hermite ## quadrature points: 15 ## ## Optimization: ## Convergence: 0 Remember that, due to the fact that the jointModel function extracts all the required information from these two objects (e.g., response vectors, design matrices, etc.), in the call to the coxph function we need to specify the argument x = TRUE. With this, the design matrix of the Cox model is included in the returned object. Additionally, the main argument timeVar of jointModel function is used to specify the name of the time variable in the linear mixed effects model, which is required for the computation of \\(m_i(t)\\). Note that in the results of the event process the parameter labeled Assoct is the parameter \\(\\alpha\\) in the equation (4.1) that measures the effect of \\(m_i(t)\\) (i.e., in our case of the true square root CD4 cell count) in the risk for death. The parameters \\(x_i\\) are the parameters for the piecewise constant baseline risk function. As we can see there is a significant effect of longitudinal outcome on the risk. For obtaining the Hazard Ratio for this variable we have to exponenciate the value exposed in the table. In this case the result is 0.33. According to this, one unit increse on the CD4 count cell decreases the risk 67%. If we want to test for a treatment effect, an alternative to the Wald test with a pvalue around 0.03, is the Likelihood Ratio Test (LRT). To perform it we need to fit the joint model under the null hypothesis of no treatment effect in the survival submodel, and then use the anova function fitSURV2 &lt;- coxph(Surv(Time, death) ~ 1, data = aids.id, x = TRUE) fitJM2 &lt;- jointModel(fitLME, fitSURV2, timeVar = &quot;obstime&quot;, method = &quot;piecewise-PH-GH&quot;) anova(fitJM2, fitJM) # the model under the null is the first one ## ## AIC BIC log.Lik LRT df p.value ## fitJM2 4250.53 4312.72 -2110.26 ## fitJM 4247.29 4313.64 -2107.65 5.23 1 0.0222 According to the pvalue (as with the Wald test) we arrive to the same conclusion, there exist an affect of the treatment on the risk. Additionally, if we want to obtain estimates of the Hazard Ratio with confidence intervals for the final model it is possible ti apply the confint function to the created object confint(fitJM, parm = &quot;Event&quot;) ## 2.5 % est. 97.5 % ## drugddI 0.04979688 0.3511323 0.6524677 ## Assoct -1.33281297 -1.1016129 -0.8704128 exp(confint(fitJM, parm = &quot;Event&quot;)) ## 2.5 % est. 97.5 % ## drugddI 1.0510576 1.4206752 1.9202736 ## Assoct 0.2637343 0.3323346 0.4187786 Finally, we will focus on the calculation of expected survival probabilities. For this we have to use the survfitJM function that accepts as main arguments a fitted joint model, and a data frame that contains the longitudinal and covariate information for the subjects for which we wish to calculate the predicted survival probabilities. Here we compute the expected survival probabilies for two patients in the data set who has not died by the time of loss to follow-up. The function assumes that the patient has survived up to the last time point \\(t\\) in newdata for which a CD4 measurement was recorded, and will produce survival probabilities for a set of predefined \\(u &gt; t\\) values set.seed(300716) # it uses Monte Carlo samples preds &lt;- survfitJM(fitJM, newdata = aids[aids$patient %in% c(&quot;7&quot;, &quot;15&quot;), ], idVar = &quot;patient&quot;) # last.time = &quot;Time&quot; survfitJM(fitJM, newdata = aids[aids$patient %in% c(&quot;7&quot;, &quot;15&quot;), ], idVar = &quot;patient&quot;, survTimes = c(20, 30, 40)) # you can specify the times ## ## Prediction of Conditional Probabilities for Event ## based on 200 Monte Carlo samples ## ## $`7` ## times Mean Median Lower Upper ## 1 12 1.0000 1.0000 1.0000 1.0000 ## 1 20 0.6952 0.7093 0.4501 0.8648 ## 2 30 0.3619 0.3734 0.0282 0.7556 ## 3 40 0.1725 0.1171 0.0000 0.6575 ## ## $`15` ## times Mean Median Lower Upper ## 1 12 1.0000 1.0000 1.0000 1.0000 ## 1 20 0.8701 0.8834 0.7532 0.9467 ## 2 30 0.6915 0.7229 0.3519 0.9105 ## 3 40 0.5204 0.5514 0.0697 0.8769 Note that the first time of the output is the last time observed in the longitudinal study. This is because for the time points that are earlier than this time we know that this subject was alive and therefore survival probability is 1. par(mfrow=c(1,2)) plot(preds, which = &quot;7&quot;, conf.int = TRUE) plot(preds, which = &quot;7&quot;, conf.int = TRUE, fun = function (x) -log(x), ylab = &quot;Cumulative Risk&quot;) The CD4 cell counts are known to exhibit right skewed shapes of distribution, and therefore, for the remainder of this analysis we will work with the square root of the CD4 cell values.↩ "],
["condsurv.html", "Chapter 5 Conditinal Survival with condSURV", " Chapter 5 Conditinal Survival with condSURV In this chapter we will see the estimation of the survival function when we have ordered multivariate failure time data. This estimation will be obtained by means of the condSURV package, which provides three different approaches all based on the Kaplan-Meier estimator. "],
["introduction.html", "5.1 Introduction", " 5.1 Introduction As we saw, the most popular method for estimating survival, when there is censoring, is the well-known product-limit estimator also known as Kaplan-Meier estimator (Kaplan and Meier 1958). The popularity of the product-limit estimator is explained by its simplicity and intuitive appeal while requiring very week assumptions. It simply takes into account with the empirical probability of surviving over certain time. The method does not take into account of covariates, so it is mainly descriptive. Discrete covariates can be included by splitting the sample for each level of the covariate and applying the product-limit estimator for each subsample. This approach is not recommended for continuous covariates. To account to this extra difficulty several generalizations to the Kaplan-Meier estimator have been proposed throughout the last decades. Beran (1981) was the first one who proposed an estimator of the conditional distribution (survival) function with censored data in a fully nonparametric way. His estimator was further studied among others by Dabrowska (1987), Akritas (1994), Gonzalez-Manteiga and Cadarso-Suárez (1994) and Van Keilegom, Akritas, and Veraverbeke (2001). All these estimators can be used to estimate the distribution (or survival) function conditional to a continuous covariable in a regression model, when data are subject to censoring. However, none of the above methods can be used to estimate the conditional survival when the covariate is censored. In many longitudinal medical studies, patients may experience several events through a follow-up period. In these studies, the analysis of sequentially ordered events are often of interest. The events of concern can be of the same nature (e.g., recurrent disease episodes in cancer studies) or represent different states in the disease process (e.g., ‘alive and disease-free’, ‘alive with recurrence’ and ‘dead’). If the events are of the same nature, this is usually referred as recurrent events (Cook and Lawless 2007). One example of this scheme can be see at Figure 5.1. Figure 5.1: Illustration of censoring. In the above situation maybe we want to obtain estimates for some conditional survival. Let’s do it now! References "],
["notation.html", "5.2 Notation", " 5.2 Notation Suppose that an individual may experience \\(K\\) consecutive events at times \\(T_1&lt;T_2&lt;\\cdot\\cdot\\cdot&lt;T_K=T\\), which are measured from the start of the follow-up. Here different methods are proposed to estimate conditional survival probabilities such as \\(P(T_2 &gt; y \\mid T_1 &gt; x)\\) or \\(P(T_2 &gt; y \\mid T_1 \\leq x)\\), where \\(T_1\\) and \\(T_2\\) are ordered event times of two successive events. The proposed methods are all based on the Kaplan-Meier estimator and the ideas behind the proposed estimators can also be used to estimate more general functions involving more than two successive event times. However, for ease of presentation and without loss of generality, we take \\(K=2\\) in this section. The extension to \\(K&gt;2\\) is straightforward. Figure 5.2: 3-state progresive model. Let \\((T_{1},T_{2})\\) be a pair of successive event times corresponding to two ordered (possibly consecutive) events measured from the start of the follow-up. Let \\(T=T_{2}\\) denote the total time and assume that both \\(T_1\\) and \\(T\\) are observed subject to a (univariate) random right-censoring variable \\(C\\) assumed to be independent of \\((T_1,T)\\). Due to censoring, rather than \\((T_1,T)\\) we observe \\((\\widetilde T_{1},\\Delta_1,\\widetilde T,\\Delta_2)\\) where \\(\\widetilde T_{1}=\\min (T_{1},C)\\), \\(\\Delta_{1}=I(T_{1}\\leq C)\\), \\(\\widetilde T=\\min (T,C)\\), \\(\\Delta_{2}=I(T\\leq C)\\), where \\(I(\\cdot)\\) is the indicator function. Let \\((\\widetilde T_{1i},\\Delta_{1i},\\widetilde T_i,\\Delta_{2i})\\), \\(1\\leq i\\leq n\\) be independent and identically distributed data with the same distribution as \\((\\widetilde T_{1},\\Delta_1,\\widetilde T,\\Delta_2)\\). "],
["estimation-of-the-conditional-survival.html", "5.3 Estimation of the conditional survival", " 5.3 Estimation of the conditional survival Let \\(S_1\\) and \\(S\\) be the marginal survival functions of \\(T_1\\) and \\(T\\); that is, \\(S_1(y)=P(T_1&gt;y)\\) and \\(S(y)=P(T&gt;y)\\). Introduce also the conditional survival probabilities \\(P(T&gt;y|T_1&gt;x)\\) and \\(P(T&gt;y|T_1\\leq x)\\). without loss of generality, we only consider the estimation of \\(S(y|x)=P(T&gt;y|T_1&gt;x)\\). The Kaplan-Meier estimator, also known as the product-limit estimator, is the most frequently used method to estimate survival for censored data. The most used representation of the Kaplan-Meier estimator of the total time is through a product of the following form \\[\\begin{eqnarray*} \\widehat S(y)=\\prod_{\\widetilde T_i\\leq t}\\left(1-\\frac{\\Delta_{2i}}{R(\\widetilde T_i)}\\right) \\end{eqnarray*}\\] where \\(R(t)=\\sum_{i=1}^{n} I(\\widetilde T_i \\geq t)\\) denote the number of individuals at risk just before time \\(t\\). Below we introduce a weighted average representation of the Kaplan-Meier estimator which will be used later to introduce estimators for the conditional survival function \\[\\begin{equation*} \\widehat S(y)=1-\\sum_{i=1}^{n}W_{i}I(\\widetilde T_{(i)}\\leq y),%\\equiv 1-\\widehat{F}_1(x), \\end{equation*}\\] where \\(\\widetilde T_{\\left( 1\\right) }\\leq ...\\leq \\widetilde T_{\\left( n\\right) }\\) denotes the ordered \\(\\widetilde T\\)-sample and \\[\\begin{equation*} W_{i}=\\frac{\\Delta_{2\\left[ i\\right] }}{n-i+1}\\prod_{j=1}^{i-1}\\left[ 1-\\frac{% \\Delta _{2\\left[ j\\right] }}{n-j+1}\\right] \\end{equation*}\\] is the Kaplan-Meier weight attached to \\(\\widetilde T_{\\left( i\\right) }\\). In the expression of \\(W_{i}\\) notation \\(\\Delta_{2\\left[ i\\right] }\\) is used for the \\(i\\)-th concomitant value of the censoring indicator (that is, \\(\\Delta_{2\\left[ i \\right] }=\\Delta _{2j}\\) if \\(\\widetilde T_{\\left( i\\right) }=\\widetilde T_{j}\\)). Well, we are interested in the estimation of the conditional survival function, \\(S(y\\mid x)=P(T&gt;y\\mid T_1&gt;x)\\). Below we provide estimators for this quantity, all based on the Kaplan-Meier estimator. 5.3.1 Kaplan-Meier Weighted Estimator (KMW) Since \\(S(y\\mid x)\\) can be expressed as \\(S(y\\mid x)=P(T &gt; y|T_1 &gt; x) = 1 - P(T\\leq y\\mid T_1 &gt; x)= 1 - P(T_1 &gt; x, T\\leq y)/\\left(1-P\\left(T_1\\leq x\\right)\\right),\\) the conditional survival function may be estimated as \\[\\begin{equation} \\widehat S^{\\texttt{KMW}}(y\\mid x)=1-\\frac{\\sum_{i=1}^{n}{W_iI(\\widetilde T_{1\\left[i\\right]} &gt;x, \\widetilde T_{\\left(i\\right)} \\leq y)}}{\\widehat S_1(x)}. \\end{equation}\\] 5.3.2 The Landmark approach (LDM) The Landmark approach (Van Houwelingen 2007) states that, given the time point \\(x\\), to estimate \\(S(y\\mid x)=P(T&gt; y\\mid T_1&gt;x)\\) the analysis can be restricted to the individuals with an observed first event time greater than \\(x\\). Let \\(n_x\\) be the cardinal of \\(\\left\\{i:\\widetilde T_{1i}&gt;x\\right\\}\\) and \\(\\left( \\widetilde T_{\\left( i\\right) }^{x},\\Delta_{\\left[ i\\right]}^{x}\\right)\\), \\(i=1,...,n_{x}\\), is the \\(\\left(\\widetilde T,\\Delta\\right)\\)-sample in \\(\\left\\{i:\\widetilde T_{1i}&gt;x\\right\\}\\) ordered with respect to \\(\\widetilde T\\). \\[\\begin{equation*} \\widehat S^{\\texttt{LDM}}(y\\mid x)=1-\\sum_{i=1}^{n_x}{W_i^{x}I(\\widetilde T_{\\left(i\\right)}^x \\leq y)}. \\end{equation*}\\] where \\(W_i^{x}\\) denotes the Kaplan-Meier weight attached to the i-th ordered T-datum, computed from the subsample \\(\\left\\{i:\\widetilde T_{1i}&gt;x\\right\\}\\). 5.3.3 The Presmoothed Landmark approach (PLDM) The standard error of the LDM approach may be large when the censoring is heavy, particularly with a small sample size. Interestingly, the variance of this estimator may be reduced by presmoothing (Dikta 1998). Here, the idea of presmoothing involves replacing the censoring indicators (in the expression of the Kaplan-Meier weights) by some smooth fit before the Kaplan-Meier formula is applied. This preliminary smoothing may be based on a certain parametric family such as the logistic (thus leading to a semiparametric estimator), or on a nonparametric estimator of the binary regression curve. The corresponding presmoothed landmark estimator is then given by \\[\\begin{equation*} \\widehat S^{\\texttt{PDLM}}(y\\mid x)=1-\\sum_{i=1}^{n_x}{W_i^{x\\star}I(\\widetilde T_{\\left(i\\right)}^x \\leq y)} \\end{equation*}\\] where \\(W_{i}^{x\\star}\\) is defined through \\[\\begin{equation*} W_{i}^{x\\star}=\\frac {m(\\widetilde T_{\\left(i\\right)}^{x})}{n_x-i+1}\\prod_{j=1}^{i-1}\\left[1-\\frac {m(\\widetilde T_{\\left(j\\right)}^{x})}{n_x-j+1}\\right], \\quad 1\\leq i\\leq n_{x}, \\end{equation*}\\] where \\(\\left( \\widetilde T_{\\left( i\\right) }^{x},\\Delta_{\\left[ i\\right]}^{x}\\right)\\), \\(i=1,...,n_{x}\\), is the \\(\\left( \\widetilde T,\\Delta\\right)\\)-sample in \\(\\left\\{i:\\widetilde T_{1i}&gt;x\\right\\}\\) ordered with respect to \\(\\widetilde T\\). Here, \\(m(t)= P(\\Delta=1\\mid \\widetilde T^{x}=t)\\). \\(m(\\widetilde T^{x})\\) belongs to a parametric (smooth) family of binary regression curves, e.g., logistic. According to the performance, it has been demonstrated that all of the estimators perform well, approaching their targets as the sample size increases. Besides, simulation results reveal that the landmark estimator (LDM) perform favorably when compared with the first method (KMW). Furthermore, the reported simulation results reveal relative benefits of presmoothing (PLDM) in the heavily censored scenarios or small sample sizes. References "],
["the-condsurv-package.html", "5.4 The condSURV package", " 5.4 The condSURV package To illustrate our methods we will use data from a German Breast cancer study (David W. Hosmer Jr. 2008). This data set is freely available as part of the `condSURV package. In this dataset, a total of 686 woman with primary node positive Breast cancer were recruited in the period between 1984 and 1989. From this total, 299 developed a recurrence and among these 171 died. For each patient, the two event times (time to recurrence and time to death) and the corresponding indicator status is recorded. Other covariates were also recorded. The covariate recurrence is the only time-dependent covariate, while the other covariates included are fixed. Recurrence can be considered as an intermediate transient state and modeled using a three-state progressive model with states Alive and disease-free, Alive with Recurrence and Dead. You can see an example at Figure 5.3. The effect of recurrence is important on the patient outcome and can be studied through the ordered multivariate event time data of time-to-event from enrollment, to recurrence and to death. Results obtained from the estimation of the conditional survival probabilities, \\(S(y\\mid x)=P(T&gt;y|T_1&gt;x)\\), can be used to understand which individuals without recurring cancer after surgery are most likely to survive from their disease and which would benefit from more personal attention, closer follow-up and monitoring. Figure 5.3: Scheme of the model. Bellow is an excerpt of the data.frame with one row per individual head(gbcsCS) ## id diagdateb recdate deathdate age menopause hormone size grade ## 1 1 17-08-1984 15-04-1988 16-11-1990 38 1 1 18 3 ## 2 2 25-04-1985 15-03-1989 22-10-1990 52 1 1 20 1 ## 3 3 11-10-1984 12-04-1988 06-10-1988 47 1 1 30 2 ## 4 4 29-06-1984 24-11-1984 24-11-1984 40 1 1 24 1 ## 5 5 03-07-1984 09-08-1989 09-08-1989 64 2 2 19 2 ## 6 6 24-07-1984 08-11-1989 08-11-1989 49 2 2 56 1 ## nodes prog_recp estrg_recp rectime censrec survtime censdead ## 1 5 141 105 1337 1 2282 0 ## 2 1 78 14 1420 1 2006 0 ## 3 1 422 89 1279 1 1456 1 ## 4 3 25 11 148 0 148 0 ## 5 1 19 9 1863 0 1863 0 ## 6 3 356 64 1933 0 1933 0 kmw1 &lt;- survCOND(survCS(rectime, censrec, survtime, censdead) ~ 1, x = 365, y = 1460, data = gbcsCS, method = &quot;KMW&quot;, conf = TRUE, n.boot = 100) summary(kmw1) ## ## P(T&gt;y|T1&gt;365) ## ## y estimate lower 95% CI upper 95% CI ## 1460 0.8050317 0.77625 0.8395263 With the previous code you can obtain the estimates for the probability that a woman survives more than four years given that she is alive and disease-free at one year after the surgery. Note that the package contains the function survCS which takes the input data as an R formula and creates a survival object among the chosen variables for analysis. This function will verify if the data has been introduced correctly and create a survCS object. Arguments in this function must be introduced in the following order time1, event1, time2, event2,…, Stime and event, where time1, time2, …, Stime are ordered event times and event1, event2,…, event their corresponding indicator statuses. This function plays a similar role as the Surv function in the survival package. # including more times kmw2 &lt;- survCOND(survCS(rectime, censrec, survtime, censdead) ~ 1, x = 365, y = 365 * 1:7, data = gbcsCS, method = &quot;KMW&quot;, conf = TRUE) summary(kmw2) ## ## P(T&gt;y|T1&gt;365) ## ## y estimate lower 95% CI upper 95% CI ## 365 1.0000000 1.0000000 1.0000000 ## 730 0.9429857 0.9215913 0.9604599 ## 1095 0.8805697 0.8542154 0.9050319 ## 1460 0.8050317 0.7717641 0.8406205 ## 1825 0.7506686 0.7052949 0.7905356 ## 2190 0.6627422 0.6040739 0.7232711 ## 2555 0.6205942 0.5157023 0.7088350 # with y omitted kmw3 &lt;- survCOND(survCS(rectime, censrec, survtime, censdead) ~ 1, x = 365, data = gbcsCS, method = &quot;KMW&quot;, conf = TRUE) # note the `times` argument summary(kmw3, times = c(730, 1095)) ## y estimate lower 95% CI upper 95% CI ## 730 0.9429857 0.9216899 0.9620051 ## 1095 0.8805697 0.8532885 0.9092549 In addition, one may also be interested in calculating the conditional survival function, \\(S(y\\mid x)=P(T&gt;y|T_1\\leq x)\\). This is the probability of the individual to be alive at time \\(y\\) conditional that he/she is alive with recurrence at a previous time \\(x\\). # P(T &gt; y | T1 &lt; x) kmw4 &lt;- survCOND(survCS(rectime, censrec, survtime, censdead) ~ 1, x = 365, data = gbcsCS, method = &quot;KMW&quot;, conf = TRUE, lower.tail = TRUE) summary(kmw4, times = c(730, 1095)) ## y estimate lower 95% CI upper 95% CI ## 730 0.3448798 0.2111348 0.4662095 ## 1095 0.2165459 0.1024390 0.3146710 Similarly, one can obtain the results for the landmark methods (LDM and PLDM) using the same function survCOND. The unsmoothed landmark estimator is obtained using argument method = &quot;LDM&quot; whereas for obtaining the presmoothed landmark estimator the argument presmooth = TRUE is also required. plot(kmw3, confcol = &quot;red&quot;, xlab = &quot;Time (days)&quot;, ylab = &quot;S(y|365)&quot;) One important goal is to obtain estimates for the above estimated quantities (conditional survival probabilities) conditionally on current or past covariate measures. The current version of the package allow the inclusion of a single covariate. grade &lt;- survCOND(survCS(rectime, censrec, survtime, censdead) ~ as.factor(grade), x = 365, data = gbcsCS, method = &quot;LDM&quot;, conf = FALSE) plot(grade) Finally, the package also allow the user to estimate the conditional survival given a continuous covariate (i.e., objects of class ‘integer’ or ‘numeric’). For example, estimates and plot for the conditional survival for women aged 60 years, \\(S(y|x,Z=z)=P(T&gt;y|T_1&gt;x, age=60)\\). age &lt;- survCOND(survCS(rectime, censrec, survtime, censdead) ~ age, x = 365, z.value = 60, data = gbcsCS, conf = FALSE) plot(age) The inclusion of continuous covariates can be computationally demanding. In particular, the use of bootstrap resampling techniques are time-consuming processes because it is necessary to estimate the model a great number of times. The use of the condSURV package to more than two consecutive events is illustrated in the Appendix of Meira-Machado and Sestelo (2016b). References "],
["clustcurv.html", "Chapter 6 Spoiler!!", " Chapter 6 Spoiler!! Here we are going to see a new methodology to perform groups of survival curves. This works is still under revision in the Statistics in Medicine journal. "],
["introduction-1.html", "6.1 Introduction", " 6.1 Introduction In many medical studies it could be interesting to ascertain whether groups of survival curves can be carried out, especially when confronted with a considerable number of curves. Though the aforementioned methods like Log-rank, Peto &amp; Peto, etc. can be used to compare multiple survival curves, to the best of our knowledge, there are none available method that can be used to determine groups among a series of survival curves. When the log-rank test (or its analogous) is used to compare three or more survival curves at once, the test reports a single p-value testing the null hypothesis that all the samples come from populations with identical survival. If the null hypothesis of equality of curves is rejected, then, this leads to the clear conclusion that at least one curve is different. However, these methods cannot be used to ascertain whether groups of curves can be performed or if all these curves are different from each other. clustcurv proposes an approach that allows determining survival groups with an automatic selection of their number. "],
["algortihm.html", "6.2 Algortihm", " 6.2 Algortihm \\(k\\)-survival curves algorithm With \\(\\{(\\widetilde{T}_{ij}, \\Delta_{ij})\\), \\(i=1, \\ldots, n_j\\)}, \\(j = 1, \\ldots, J\\), and using the Kaplan-Meier estimator obtain \\(\\hat S_j\\). Initialize with \\(K = 1\\) and test \\(H_0(K)\\): Obtain the ``best&quot; partition \\(G_1, \\ldots, G_K\\) by means of the \\(k\\)-means or \\(k\\)-medians algorithm. For \\(k = 1, \\ldots, K\\), estimate \\(M_k\\) and retrieve the test statistic \\(D\\). Generate \\(B\\) bootstrap samples and calculate \\(D^{\\ast b}\\), for \\(b = 1, \\ldots, B\\). \\(D &gt; D^{\\ast (1-\\alpha)}\\) reject \\(H_0(K)\\) \\(K = K + 1\\) go back to the beginning of current section else accept \\(H_0(K)\\) end The number \\(K\\) of groups of survival curves is determined. "],
["aplication-to-real-data.html", "6.3 Aplication to real data", " 6.3 Aplication to real data head(veteran) ## trt celltype time status karno diagtime age prior ## 1 1 squamous 72 1 60 7 69 0 ## 2 1 squamous 411 1 70 5 64 10 ## 3 1 squamous 228 1 60 3 38 0 ## 4 1 squamous 126 1 60 9 63 10 ## 5 1 squamous 118 1 70 11 65 10 ## 6 1 squamous 10 1 20 5 49 0 fit &lt;- survfit(Surv(time, status) ~ factor(celltype), data = veteran) autoplot(fit) survdiff(Surv(time,status)~factor(celltype), data=veteran) ## Call: ## survdiff(formula = Surv(time, status) ~ factor(celltype), data = veteran) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## factor(celltype)=squamous 35 31 47.7 5.82 10.53 ## factor(celltype)=smallcell 48 45 30.1 7.37 10.20 ## factor(celltype)=adeno 27 26 15.7 6.77 8.19 ## factor(celltype)=large 27 26 34.5 2.12 3.02 ## ## Chisq= 25.4 on 3 degrees of freedom, p= 1.27e-05 survminer::pairwise_survdiff(Surv(time, status) ~ celltype, data = veteran, p.adjust.method = &quot;BH&quot;) ## ## Pairwise comparisons using Log-Rank test ## ## data: veteran and celltype ## ## squamous smallcell adeno ## smallcell 0.00134 - - ## adeno 0.00134 0.75565 - ## large 0.43731 0.00331 0.00016 ## ## P value adjustment method: BH ?clustcurv_surv res &lt;- clustcurv_surv(time = veteran$time, status = veteran$status, fac = veteran$celltype, algorithm = &quot;kmeans&quot;, nboot = 100, cluster = TRUE, seed = 29072016) ## Checking 1 cluster... ## Checking 2 clusters... ## ## Finally, there are 2 clusters. res ## $table ## H0 Tvalue pvalue ## 1 1 3.2108812 0.00 ## 2 2 0.3799373 0.43 ## ## $levels ## [1] &quot;squamous&quot; &quot;smallcell&quot; &quot;adeno&quot; &quot;large&quot; ## ## $cluster ## [1] 2 1 1 2 ## ## $centers ## Call: survfit(formula = Surv(time, status) ~ aux$cluster[fac]) ## ## n events median 0.95LCL 0.95UCL ## aux$cluster[fac]=1 75 71 51 30 73 ## aux$cluster[fac]=2 62 57 143 110 216 ## ## $curves ## Call: survfit(formula = Surv(time, status) ~ fac) ## ## n events median 0.95LCL 0.95UCL ## fac=squamous 35 31 118 82 314 ## fac=smallcell 48 45 51 25 63 ## fac=adeno 27 26 51 35 92 ## fac=large 27 26 156 105 231 ## ## attr(,&quot;class&quot;) ## [1] &quot;clustcurv_surv&quot; autoplot(res, groups_by_colour = TRUE, xlab = &quot;Time (in days)&quot;) Now we are going to see anoyher example with a catgorical variable with more levels. colonCSm &lt;- na.omit(data.frame(time = colonCS$Stime, status = colonCS$event, nodes = colonCS$nodes)) table(colonCSm$nodes) ## ## 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## 2 274 194 125 84 46 43 38 23 20 13 10 11 7 4 6 1 2 ## 19 20 22 24 27 33 ## 2 2 1 1 1 1 # deleting people with zero nodes colonCSm &lt;- colonCSm[-c(which(colonCSm$nodes == 0)), ] table(colonCSm$nodes) ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 19 ## 274 194 125 84 46 43 38 23 20 13 10 11 7 4 6 1 2 2 ## 20 22 24 27 33 ## 2 1 1 1 1 # grouping people with more than 10 nodes colonCSm$nodes[colonCSm$nodes &gt;= 10] &lt;- 10 table(colonCSm$nodes) # 10 levels ## ## 1 2 3 4 5 6 7 8 9 10 ## 274 194 125 84 46 43 38 23 20 62 model &lt;- survfit(Surv(time, status) ~ factor(nodes), data = colonCSm) survdiff(Surv(time,status)~factor(nodes), data = colonCSm) ## Call: ## survdiff(formula = Surv(time, status) ~ factor(nodes), data = colonCSm) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## factor(nodes)=1 274 94 151.93 22.0901 33.9249 ## factor(nodes)=2 194 74 102.87 8.1022 10.5979 ## factor(nodes)=3 125 61 62.56 0.0387 0.0453 ## factor(nodes)=4 84 43 38.26 0.5868 0.6434 ## factor(nodes)=5 46 34 17.06 16.8249 17.5428 ## factor(nodes)=6 43 27 16.43 6.8027 7.0736 ## factor(nodes)=7 38 25 15.41 5.9636 6.1880 ## factor(nodes)=8 23 18 7.22 16.0875 16.3765 ## factor(nodes)=9 20 14 8.05 4.3931 4.4795 ## factor(nodes)=10 62 49 19.21 46.2239 48.6066 ## ## Chisq= 129 on 9 degrees of freedom, p= 0 survminer::pairwise_survdiff(Surv(time, status) ~ nodes, data = colonCSm, p.adjust.method = &quot;BH&quot;) ## ## Pairwise comparisons using Log-Rank test ## ## data: colonCSm and nodes ## ## 1 2 3 4 5 6 7 8 9 ## 2 0.41644 - - - - - - - - ## 3 0.00853 0.10482 - - - - - - - ## 4 0.00221 0.04032 0.51450 - - - - - - ## 5 3.9e-09 1.8e-06 0.00072 0.03427 - - - - - ## 6 1.7e-05 0.00072 0.03750 0.22540 0.60307 - - - - ## 7 2.1e-05 0.00072 0.04219 0.25752 0.49274 0.95088 - - - ## 8 3.0e-08 4.1e-06 0.00047 0.01407 0.51450 0.30796 0.25752 - - ## 9 0.00043 0.00493 0.08152 0.23872 0.76034 0.90717 0.79064 0.51450 - ## 10 &lt; 2e-16 1.2e-11 9.1e-07 0.00047 0.37154 0.16671 0.10386 0.95088 0.37007 ## ## P value adjustment method: BH res &lt;- clustcurv_surv(time = colonCSm$time, status = colonCSm$status, fac = colonCSm$nodes, algorithm = &quot;kmeans&quot;, nboot = 100, cluster = TRUE, seed = 300716) ## Checking 1 cluster... ## Checking 2 clusters... ## ## Finally, there are 2 clusters. autoplot(res, groups_by_colour = FALSE, xlab = &quot;Time (in days)&quot;) autoplot(res, groups_by_colour = TRUE, xlab = &quot;Time (in days)&quot;) res$table ## H0 Tvalue pvalue ## 1 1 15.425589 0.00 ## 2 2 2.364531 0.18 data.frame(res$levels, res$cluster) ## res.levels res.cluster ## 1 1 2 ## 2 2 2 ## 3 3 2 ## 4 4 2 ## 5 5 1 ## 6 6 1 ## 7 7 1 ## 8 8 1 ## 9 9 1 ## 10 10 1 One faster option than applying directly the `clustcurv_surv` function, that is based on boostrap techniques for detecting the number of groups, is to use the `kgroups_surv` for $k = 1, \\ldots, J-1$. Then you can plot the resulted measures for each $k$ and choose the one that with the &quot;less&quot; measure. fun &lt;- function(x){ kgroups_surv(time = colonCSm$time, status = colonCSm$status, fac = colonCSm$nodes, algorithm = &quot;kmeans&quot;, k = x)$measure } ts &lt;- sapply(1:8, fun) qplot(1:8, ts, xlab = &quot;Number of groups&quot;, ylab = &quot;Test estatistic value&quot;) "],
["appendix-install.html", "A Installation of R and RStudio", " A Installation of R and RStudio You can follow these steps to install R and Rstudio, please note that there will be a few new releases of R every year, and you may want to upgrade R occasionally. For Ubuntu users, kindly follow the corresponding instructions here. For Mac OS X users download R from here or from the url below. To this end, click on Download R for Mac OS X. Then click on Download R-3.4.2.pkg (or a newer version) and install it. Leave all default settings in the installation options. Optional for some graphic experiences, download and install XQuartz. Once installed R, you can download RStudio IDE from here or from the url below. You must choose the appropriate version to your operative system and hardware (only certain Ubuntu and Fedora versions are supported), and install it using the package manager. "],
["appendix-rstudio.html", "B Introduction to RStudio", " B Introduction to RStudio RStudio is the premier integrated development environment (IDE) for R. It is available in open source and commercial editions on the desktop (Windows, Mac, and Linux) and from a web browser to a Linux server running RStudio Server or RStudio Server Pro. You can find a global view of the IDE in this Cheat Sheet. An important advice is that for running a line or code selection from the script in the console, you can do it with the keyboard shortcut 'Ctrl+Enter' (Linux) or 'Cmd+Enter' (Mac OS X). "],
["appendix-r.html", "C Introduction to R", " C Introduction to R The manual “An Introduction to R” gives an introduction to the language and how to use R for doing statistical analysis and graphcis in detail. Additionally, in this section you can find a set of Cheat Sheets of this programming language: R Base for first steps and basic functions of the language. R Advanced for environments, data structures, functions, subsetting and more advanced things. The Data Import cheat sheet reminds you how to read in flat files with http://readr.tidyverse.org/, work with the results as tibbles, and reshape messy data with the tidyr package. Use tidyr to reshape your tables into tidy data, the data format that works the most seamlessly with R and the tidyverse. Data Transformations for some functions in dplyr packages very useful and computational efficient to preprocess data. Data Visualization for make beautiful and customizable plots of your data by means of the ggplot2 package. It implements the grammar of graphics, an easy to use system for building plots. Finally, you can find below a list with some well-know web resources related with this statistical language: R-bloggers Quick-R site Revolutions The R Journal Journal of Statistical Software "],
["references.html", "References", " References "]
]
