--- 
title: "A short course on Survival Analysis applied to the Financial Industry"
subtitle: "BBVA Data & Analytics, Madrid"
author: "Marta Sestelo"
date: "27-28 /11/2017, v1.0"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [bib/bibliografia.bib,bib/bib_condsurv.bib]
biblio-style: apalike
link-citations: yes
github-repo: sestelo/sa_financial
description: "This is a short course on survival analysis applied to the financial field."
---

# Preface {-}



This book is designed to provide a guide for a short course on survival analysis. It is mainly focussed on applying the stastical tecnquines developed in the survival field to the financial industry. The emphasis is placed in understanding the methods, building intuition about when aplying each of them and showing their application through the use of statistical software.





# Programing language and software {-}

The **software** used in the course is the statistical language [`R`](https://cran.r-project.org/) and the IDE (Integrated Development Environment) used is [`RStudio`](https://www.rstudio.com/products/rstudio/download/). A basic prior knowledge of both is assumed. Basic introductions to `R` and `RStudio` are presented in the Appendix \@ref(appendix-r) and \@ref(appendix-rstudio) for those students lacking basic expertise on them.

The required packages for the course are:

```{r, echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, eval = FALSE}
# Install packages
install.packages(c("survival", "condSURV", "JM", "dplyr", "survminer", "ggplot2"))
devtools::install_github("noramvillanueva/clustcurv")
```

The codes in the notes may assume that the packages have been loaded, so it is better to do it now:
```{r, echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, eval = TRUE}
# Load packages
library(survival)
library(condSURV)
library(JM)
library(dplyr)
library(survminer)
library(clustcurv)
```

Links: [`survival`](https://cran.r-project.org/web/packages/survival/index.html)[@survival-package],  [`condSURV`](https://cran.r-project.org/web/packages/condSURV/index.html)[@condsurv_package; @meiramachado-sestelo:2016],
[`JM`](https://cran.r-project.org/web/packages/JM/index.html)[@Rizopoulos:2010aa],
[`dplyr`](https://cran.r-project.org/web/packages/dplyr/index.html)[@Wickham:2017aa],
 [`survminer`](https://cran.r-project.org/web/packages/survminer/index.html)[@Kassambara:2017aa],
 [`ggplot2`](https://cran.r-project.org/web/packages/ggplot2/index.html)[@Wickham:2009aa],
 and [`clustcurv`](https://github.com/noramvillanueva/clustcurv).
 



# Main references and credits {-}

Several reference books have been used for preparing these notes. The following list details
the most important ones:

- @kalbfleisch1980statistical
- @book:1205652
- @book:1129209
- @book:1298616
- @kleinbaum2011survival
- @book:1606416

In addition, this material is possible due to the work of persons who contribute greatly to the open source software with incredible pieces of software: @Xie:2015aa, @Xie:2016aa, @R-rmarkdown and @NoRefWorks:1.


The icons used in the notes were designed by [Gregor Cresnar](https://www.flaticon.com/authors/gregor-cresna) from [Flaticon](http://www.flaticon.com/).

All material in these notes is licensed under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).



<!--chapter:end:index.Rmd-->

# About the Author {-}

Marta Sestelo is a PhD in Statistics and Data Scientist at the Centre of Mathematics (CMAT) of the University of Minho. She is focused on the development of new methodologies and algorithms linked with statistics. She is interested in estimation and inference methods of flexible models, in developing practical tools for data analysis and in gaining better understanding of real life issues through statistical knowledge. At the moment, her lines of research are closely related to computational statistics and machine learning, particularly, software development, feature selection, applied predictive modeling, nonparametric curves estimation, survival, clustering, model performance, testing procedures, bootstrap resampling methods and applications to different areas of knowledge.


You can see some topics of her cv at http://sestelo.github.io.

<!--chapter:end:01-about.Rmd-->

# Introduction {#intro}

This introduction to survival analysis tries to give a small overview of the statistical approach called survival analysis. This approach includes the type of problem addressed by survival analysis, the outcome variable considered, the need to take into account *censored data*,  what a survival function and a hazard function represent, the goals of survival analysis, and some examples of survival analysis.



## What is survival analysis? {#intro-what}

In a general way, survival analysis is a collection of statistical procedures for data analysis for which the outcome variable of interest is **time until an event occurs**, often referred to as a failure time, survival time, or event time.


Survival time refers to a variable which measures the time from a particular starting time (e.g., time initiated the treatment) to a particular endpoint of interest: **time-to-event**.


The problem of analyzing time to event data arises in a number of applied fields, such as:

* medicine, biology, public health (time to death)
* social sciences (time for doing some task)
* economics (time looking for employment)
* financial or credit scoring (time to default)
* engineering (time to a failure of some electronic component)




### Time, time origen, time scale, event

In survival analysis three requirements are needed for the precise definition of the failure time of an individual. A **time origin** must be specified, a **time scale** for measuring time must be agreed upon and the meaning of **failure - event** must be clear. 

* By **time**, we mean years, months, weeks, or days from the beginning of follow-up of an individual until the event of study occurs, but we need to specify the scale.

* By **time origin** we understand the time of entry into the study.

* By **event**, we mean --it depends on the field-- death, disease incidence, recovery (e.g., return to work) if we focus on biomedical applications,  default in the credit scoring field, renewals in insurance framework, fault in the engeniering field, etc.

Generally, we will assume that only **one event** is of designated interest. When more than one event is considered (e.g., death from any of several causes), the statistical problem can be characterized as either a **recurrent event** or a **competing risk**. We will see the case of the recurrence event using the [condSURV](https://cran.r-project.org/web/packages/condSURV/index.html) package in the Chapter \@ref(condsurv).




```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, 
                      warning = FALSE, cache = TRUE)
```

<br>
It is time to see now an example in a real dataset. This is the **Prosper Loan data** provided by Udacity Data Analyst Nanodegree (last updated 3/11/14). It is also at [Kaggle](https://www.kaggle.com/jschnessl/prosperloans). 

[Prosper.com](https://www.prosper.com/) is a peer-to-peer lending marketplace. Borrowers make loan requests and investors contribute as little as $25 towards the loans of their choice. Historically, Prosper made their loan data public nightly, however, effective January 2015, information will be made available 45 days after the end of the quarter.

A link to the data is [here](https://s3.amazonaws.com/udacity-hosted-downloads/ud651/prosperLoanData.csv) and a variable dictionary can be found [here](https://docs.google.com/spreadsheets/d/1gDyi_L4UvIrLTEC6Wri5nbaMmkGmLQBk-Yx3z0XDEtI/edit#gid=0).



```{r}
# Prosper Loan data
web <- "https://s3.amazonaws.com/udacity-hosted-downloads/ud651/prosperLoanData.csv"
loan <- read.csv(web)
head(loan)[, c(51, 65, 6, 7, 19, 18, 50)]
```


### Goals of the survival analysis

* Estimate time-to-event for a group of individuals, such as time until default for a group of clients.

* Compare time-to-event between two or more groups, such as residence place for clients.

* Assess the relationship of covariates to time-to-event, such as: occupation, state, income, etc. 






## Censoring {#intro-censor}


The distinguishing feature of survival analysis is that it incorporates a phenomen called **censoring**. Censoring occurs when we have some information about individual survival time, but we don't know the time exactly. 

There are generally several reasons why censoring may occur:

* a person does not experience the event before the study ends
* a person is lost to follow-up during the study period
* a person withdraws from the study because of death (if death is not the event of interest) or some other reason
* a person cancela anticipadamente el credito (in credit scoring)
* el palzo del credito es inferior a la longitud del estudio y por lo tanto el acreditado cumple integramente con el pago de la deuda antes de concluir el estudio (in credit scoring)




There are three types:


* **Right censoring**: Random right censoring arise often in medical, biological and financial applications. In this studies, patients may enter the study at different times and **the real event time is greater than the observed time**. We know that the person’s true survival time becomes incomplete at the right side of the follow-up period, occurring when the study ends or when the person is lost to follow-up or is withdrawn. For these data, the complete survival time interval, which we don’t really know, has been cut off (i.e., censored) at the right side of the observed survival time interval.  **This is the assumed censoring in the case of credit scoring**.


* **Left censoring**: The survival time of some subject is considered to be left censored if it is less than the value observed. That is, **the event of interest has already occurred for the individual before the observed time** (not easy to deal with). For example, if we are following persons until they become HIV positive, we may record a failure when a subject first tests positive for the virus. However, we may not know the exact time of first exposure to the virus, and therefore do not know exactly when the failure occurred.  Thus, the survival time is censored on the left side since the true survival time, which ends at exposure, is shorter than the follow-up time, which ends when the subject’s test is positive.




* **Interval censoring**: When **the survival time is only known to occur within an interval**. Such interval censoring occurs when patients in a clinical trial or longitudinal study have periodic follow-up and the patient’s event time is only known to fall in some interval.  As an example, again considering HIV, a subject may have had two HIV tests, where he/she was HIV negative at the time (say, $t_1$) of the first test and HIV positive at the time ($t_2$) of the second test. In such a case, the subject's true survival time occurred after time $t_1$ and before time $t_2$, i.e., the subject is interval-censored in the time interval ($t_1$, $t_2$).




```{r, "censoring", fig.cap = "Illustration of censoring.", echo = FALSE, fig.align = 'center'}
knitr::include_graphics("images/saBBVA_censoring.pdf")
```


```{block2, type = "rmdhint_sestelo"}

It is important to highlight in this context (**time-to-default**) which situations we are going to considered as censoring. The bank has special characteristics that are not seen in other applications. **Censored cases** are considered to be loans that did **not experience default** by the moment of data gathering. Additionally, early **repayment** and **mature** cases (or complete, those ones who reach their predefined end date before the moment of data gathering) are also marked censored. 
```



Another classification:

* **Random type I censoring**: Also known as *Generalized Type I Censoring*. When individuals enter the study at different times and the terminal point of the study is predetermined by the investigator, so that the censoring times are known when an individual is entered into the study.

* **Type II censoring**: The study continues until the failure of the first $r$ individuals, where $r$ is some predetermined integer ($r<n$). All subjects are put on test at the same time, and the test is terminated when $r$ of the $n$ subjects have "failed".





## Some notation {#intro-notation}

We are now ready to introduce **basic mathematical terminology** and **notation** for survival analysis.

Let $T$ the random variable that denotes the survival time, i.e., the time to an event. Since $T$ denotes time, its possible values include all nonnegative numbers; that is, $T$ can be any number equal to or greater than zero. Furthermore, $t$ will be any specific value of interest for the random variable $T$. 


Additionally, when each subject has a random right censoring time $C_i$ that is independent of their failure time $T_i$, the data is represented by $(Y_i, \Delta_i)$ where $Y_i = \min(T_i, C_i)$ and $\Delta_i = I(T_i \le C_i)$, this $\Delta$ define a $(0,1)$ random variable indicating either failure or censorship. That is, $\Delta = 1$ for failure if the event occurs during the study period, or $\Delta = 0$ if the survival time is censored by the end of the study period.





## Survival/hazard functions {#intro-functions}

Assuming that $T$ is a continuous non-negative random variable which denote the time-to-event. There is a certain probability that an individual will have an event at exactly time $t$. For example, about human longevity, human beings have a certain probability of dying at ages $2$, $20$, $80$, and $140$, that could be: $P(T=2)$, $P(T=20)$, $P(T=80)$ and $P(T=140)$. 

Similarly, human beings have a certain probability of being alive at those same ages: $P(T>2)$, $P(T>20)$, $P(T>80)$, and $P(T>140)$.


Here an example with same real data [^1]: 

```{r, fig.cap = "Relative frequiencies for grouped ages."}
data <- read.table("data/deaths_esp.txt", header = TRUE, sep = "")
data <- data[!data$Age == "110+", ] # to avoid errors
data$Age_cut <- cut(as.numeric(as.character(data$Age)), 
                 breaks =  seq(0,110, 10), right = FALSE)

by_age <- data %>%
  group_by(Age_cut)  %>%
  summarise (sum_deaths = sum(Total, na.rm = TRUE))

barplot(by_age$sum_deaths/sum(data$Total), names.arg = by_age$Age_cut, ylab= "Relative frequency") 

``` 

<!-- ggplot(data = by_age) + -->
<!--   geom_bar(mapping = aes(x = Age_cut, y = sum_deaths/data$Total), stat = "identity") -->



[^1]: Data from *The Human Mortality Database* at http://www.mortality.org.
  
  
  




  
In the case of human longevity, the probability of death is higher at the beginning and end of life (in Spain). Therefore, $T$ is unlikely to follow a normal distribution. We can see a higher chance of dying (the event of interest) in their 70's and 80's and smaller chance of dying in their 100's and 110’s, because few
people make it long enough to die at these age.
  
  
The function that gives the probability of the failure time occurring at exactly time $t$ is the **density function $f(t)$** [^2]

[^2]: The probability mass function  is a function that gives the probability that a discrete random variable is exactly equal to some value.
  

\[
f(t) = \displaystyle{lim_{\Delta_t \to 0}} \frac{P(t \le T < t + \Delta t)}{\Delta t}
\]
and the function that gives the probability of the failure time occur before or exactly at time $t$ is the **cumulative distribution function $F(t)$**
  
\[
F(t) = P(T \le t) = \int_{0}^{t} f(u) du.
\]  
  

  
  
Note that $F(t)$  is more interesting than $f(t)$... And why? Well, as we said, the main goal of survival analysis is to estimate and compare survival experiences of different groups and the survival experience is described by the **survival function $S(t)$**

\[
S(t) = P(T > t) = 1 - F(t)
\]  

The survival function gives the probability that a person survives longer than some specified time $t$: that is, $S(t)$ gives the probability that the random variable $T$ exceeds the specified time $t$. And here, some important characteristics:

- It is nonincreasing; that is, it heads downward as $t$ increases.

- At time $t = 0$, $S(t) = S(0)= 1$; that is, at the start of the study, since no one has gotten the event yet, the probability of surviving past time zero is one.

- At time $t = \inf$, $S(t) = S(\inf) = 0$; that is, theoretically, if the study period increased without limit, eventually nobody would survive, so the survival curve must eventually fall to zero.





```{r}
t <- seq(0, 110, 1)
tdf <- pweibull(t, scale = 80, shape = 5) # weibull dist

d <- reshape2::melt(data.frame(x = t, dist = tdf, surv = 1 - tdf), id = "x")
qplot(x = x, y = value, col = variable, data = d, geom = "line", 
      ylab = "probability", xlab = "t") + 
  scale_colour_discrete(labels= c("F(t)", "S(t)"), name = "") 
``` 




Note that these are theoretical properties of survival curves.
In practice, when using actual data, we usually obtain graphs that are step functions, rather than smooth curves. Moreover, because the study period is never infinite in length and there may be competing risks for failure, it is possible that not everyone studied gets the event. The estimated survival function, $\hat{S}(t)$ thus may not go all the way down to zero at the end of the study.




```{r}
by_age <- data %>%
  group_by(Age)  %>%
  summarise (sum_deaths = sum(Total, na.rm = T))
t <- rep(as.numeric(as.character(by_age$Age)), by_age$sum_deaths) # real times

aux <- ecdf(t)
x <- seq(0, 110, 1)
edf <- aux(x) # evaluating the ecdf in some points
esf <- 1- edf

d <- reshape2::melt(data.frame(x = x, dist = edf, surv = esf), id = "x")
qplot(x = x, y = value, col = variable, data = d, geom = "step", 
      ylab = "Probability", xlab = "t") + scale_colour_discrete(labels = c("F(t)", "S(t)"), name = "")

``` 

<!-- ```{r} -->
<!-- #t <- sort(runif(30, 0, 7)) -->
<!-- #t <- seq(0, 7, length.out = 100) -->
<!-- t <- sort(rweibull(500, scale = 80, shape = 5)) -->
<!-- edf <- ecdf(t) -->
<!-- edf2 <- edf(t) -->
<!-- plot(t, edf2, type = "s", ylab = "Probability", xlab = "t") -->
<!-- lines(t, 1 - edf2, type = "s", ylab = "Probability", xlab = "t", col = 2) -->
<!-- legend("right", c("F(t)", "S(t)"), col = c(1,2), lwd = c(1, 1)) -->
<!-- ```  -->




The **hazard function $h(t)$**, is given by the formula: 

\[
h(t) = \displaystyle{lim_{\Delta_t \to 0}} \frac{P(t \le T < t + \Delta t | T \ge t)}{\Delta t}
\]
This mathematical formula is difficult to explain in practical terms.
We could say that the hazard function is the probability that if you survive to time $t$, you will experience the event in the next instant, or in other words, the hazard function gives the instantaneous potential per unit time for the event to occur, given that the individual has survived up to time $t$. Because of the given sign here, the hazard function is sometimes called a **conditional failure rate**.

Note that, in contrast to the **survival function**, which focuses on **not failing**, the **hazard function** focuses on **failing**, that is, on the event occurring. Thus, in some sense, the hazard function can be considered as giving the opposite side of the information given by the survivor function. 


Additionally, in contrast to a survival function, the graph of $h(t)$ does not have to start at one and go down to zero, but rather can start anywhere and go up and down in any direction over time. In particular, for a specified value of $t$, the hazard function $h(t)$ has the following characteristics:


- It is always nonnegative, that is, equal to or greater than zero.

- It has no upper bound.




Finally note that the hazard function can be expressed as the probability density function divided by the survival function, $h(t) = \frac{f(t)}{S(t)}$:
  

\[
P(t \le T \lt t + dt | T \ge t) = \frac{P(t \le T \lt t + dt, T \ge t)}{P(T \ge t)} = \frac{P(t \le T \lt t + dt)}{P(T \ge t)}
\]




```{r}
h <- hist(t, plot = FALSE)
x <- h$mids
dens <- h$density
surv <- 1 - aux(x)
hazard <- dens/surv
qplot(x = x, y = hazard, geom = "line", ylab = "Conditional probability of death",
      xlab = "Age")
```




<!-- In the shiny application below you can see some examples of hazard functions: -->
<!--  TODO shiny with several hazard: exponential, increasing weibull, decresing weibull, lognormal -->

<!-- ```{r} -->
<!-- x <- seq(0, 5, 0.01) -->
<!-- den <- dexp(x, 1) -->
<!-- surv <- 1 - pexp(x, 1) -->
<!-- plot(x, den/surv, type = "l", ylim = c(0.5,1.5), ylab = "Hazard", xlab = "t") -->

<!-- ``` -->





In some cases it can be more interesting to present the cumulative hazard. It will be $H(t) = \int_{0}^{t} h(u) du$.



```{block2, type = "rmdhint_sestelo"}
**Hazard vs. density function**

According to the human longevity study, note that when you are born, you have a certain probability of dying at any age, that will be $P(T = t)$, i.e. the density function. A woman born today has, say, a 1% chance of dying at 80 years. However, as you survive for a while, your probabilities keep changing, and these new conditional probabilities are given by the hazard function. In such case, we have a woman who is 79 today and has, say, a 7% chance of dying at 80 years.
```




## Relation between functions 

For **parametric** survival models, time is assumed to follow some well-known distribution whose probability density function $f(t)$ can be expressed in terms of unknown parameters. Once a probability density function is specified for survival time, the corresponding survival and hazard functions can be determined. 


For example, the **survival function** can be ascertained from the **probability density function** by integrating over the probability density function from time $t$ to infinity, or by calculating the difference between one and the **cumulative distribution function** $F(t)$. The **hazard** can then be found by dividing the negative derivative of the survival function by the survival function.  Note that the functions $f(t)$, $F(t)$, $h(t)$, and $H(t)$ are all related. 


#.  Assume that $T$ is **non-negative and continuos**:

    * Probability density function: 
  
        + $f(t) = F'(t) = \frac{dF(t)}{dt}$ 
      
    * Cumulative distribution function:
    
        + $F(t) = P(T \le t) =  \int_0^t{f(u)}{du}$ 
    
    
    * Survival function
    
        + $S(t) = 1 - F(t)$ 
        
        + $S(t) = P(T > t) = \int_t^{+\infty}{f(u)}{du}$ 
        
        + $S(t) = exp \left( - \int_0^t h(u) du \right)$ 
        
        + $S(t) =  \exp(-H(t))$
        
        
    * Hazard function
    
        + $h(t) = \frac{ f(t)}{S(t)}= \frac{ -d[S(t)]/dt}{S(t)}$
    
    * Cumulative hazard function
    
        + $H(t) =  \int_0^t h(u) du$
   
<br>

#. Assume that $T$ is **non-negative and discrete**,

    
    * Probability mass function: 
        + $p(t_i) = P(T = t_i)$
        + $p(t_i) = S(t_{i-1}) - S(t_i)$
        + $p(t_i) = F(t_i) - F(t_{i-1})$
        
    * Cumulative distribution function:
        + $F(t) = P(T \le t) =  \sum_{t_i \le t}{p(t_i)}$ 
    
    
    * Survival function
        + $S(t) = \prod_{t_i \le t} \left( 1 - h(t_i) \right)$ 
        
    * Hazard function
        + $h(t) = \frac{ p(t_i)}{S(t_{i-1})}= \frac{ -d[S(t)]/dt}{S(t)}$
        + $h(t) = 1- \frac{ S(t_i)}{S(t_{i-1})}$
    
    * Cumulative hazard function
        + $H(t) =  \sum_{t_i \le t} h(t_i)$


## Some common distributions {#intro-distri}


+--------------------+-----------------------+---------------------------------+
| Definition         | Functions             | Measures                        |
+====================+=======================+=================================+
| **Exponential**    |* $f(t)=\lambda        |  $E(T)=\int_0^{+\infty}uf(u)    |
|                    | exp(-\lambda t)$      | du= \frac{1}{\lambda}$          |                            
|   $T\sim Exp(      | where $t \ge 0 and    |                                 |
|  \lambda)$         |  \lambda > 0$         |                                 |
|                    |                       |                                 |
|                    |* $F(t)=1-exp(-\lambda |                                 |
|                    |  t)$                  |  $Var(T)=E(T^2)-E(T)^2 =        |
|                    |                       |   \ldots = \frac{1}{\lambda^2}$ |
|                    |* $S(t)=exp(-\lambda   |                                 |
|                    |   t)$                 |                                 |
|                    |                       |                                 |
|                    |* $h(t) =  \lambda$    |                                 |
|                    |                       |                                 |
|                    |* $H(t) =  \lambda t$  |                                 |
+--------------------+-----------------------+---------------------------------+
| **Weibull**        |* $f(t)=\frac{a}{b}    | $E(T)=b\Gamma \left(1+          |
|                    | (\frac{t}{b})^{a-1}   |   \frac{1}{a}\right)$           |
|   $T\sim Weib(a,b)$| exp^{-\left(\frac{t}  |                                 |
|   with $a$ shape   | {b} \right)^a}$       | $Var(T) = b^2 \Gamma \left(1+   |
|  and $b$ scale     |  where                |  \frac{2}{a}\right) - b^2       |
|                    |  $t\ge 0$ and $a,b> 0$|  \left [ \Gamma \left(1+        |
|                    |                       |   \frac{1}{a}\right)\right]^2$  |
|                    |* $F(t)= 1-exp^{-      |                                 |
|                    |   \left(\frac{t}{b}   | where, $\Gamma(k)$ is the gamma |
|                    |   \right)^a}$         | function.                       |
|                    |                       |                                 |
|                    |* $S(t)=exp^{-\left(   | $\Gamma (k) = \int_0^{+\infty}  |
|                    |    \frac{t}{b}        | u^{k-1} exp^{-u}du$             |
|                    | \right)^a}$           |                                 |
|                    |                       |                                 |
|                    |* $h(t)=ab^{-a}t^{a-1}$|                                 |
|                    |                       |                                 |
|                    |* $H(t)=(\frac{t}      |                                 |
|                    | {b})^a$               |                                 |
+--------------------+-----------------------+---------------------------------+




There are other distributions such as Log-Normal, Log-Logistic, Pareto, Rayleigh, Gomptertz, or even more. For more details see http://data.princeton.edu/pop509/ParametricSurvival.pdf. 












<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->

<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->


<!--chapter:end:02_intro.Rmd-->

# Kaplan Meier estimator {#km}


Once we have explained what is the survival curve and other introductory questions, we move on the estimation. Note that we can estimate the survival (or hazard) function in two ways:

- by specifying a **parametric** model for $\lambda(t)$ based on a particular density function $f(t)$ (parametric estimation)

- by developing an **empirical** estimate of the survival function (i.e., **nonparametric** estimation)



This Chapter describes how to plot and interpret survival data using the Kaplan-Meier (KM) estimator (nonparametric) and how to test whether or not two or more KM curves are equivalent using the log–rank test. Alternative tests to the log–rank test are also described. Furthermore, methods for computing $(1-\alpha)$% confidence intervals for a KM curve are afforded.



## Estimating survival by means of the Kaplan Meier estimator


If there are no censored observations in a sample of dimension $n$, the most natural estimator for survival is the **empirical estimator**, given by

\[
\hat S(t) = P(T \gt t) = \frac{1}{n} \sum_{i=1}^{n} I(t_i \gt t)
\]
that is, the proportion of observations with failure times greater than $t$. 

```{r}
x <- c(1, 1, 2, 2, 3, 4, 4, 5, 5, 8, 8, 8, 8, 11, 11, 12, 12, 15, 17, 22, 23)
sum(x > 8/length(n)) # hat S(8) 
sum(x > 12/length(n)) # hat S(12) 

```

Another option for estimating survival could be to use the hazard:

\[
\hat S(t) = \prod_{k = 1}^{t-1} \bigg [ 1- \hat \lambda(k)\bigg] \quad {\text{where}} \quad  \hat \lambda(t) = \frac{\sum_{i=1}^{n} I(Y_i = t)}{\sum_{i=1}^{n} I (Y_i \ge t)} 
\]


Note that $\hat \lambda(t)$ is obtained as the number of individuals that die at time $t$ divided by the number of individuals that survive to $t$, the number of individuals at risk at time $t$ (using the death as event).



However, alternative methods are necessary to incorporate censoring (censored times are different than event times).

```{r}
#  preprocesing data

head(loan)[, c(51, 65, 6, 7, 19, 18, 50)]
table(loan$LoanStatus)

# removing duplicates
loan_nd <- loan[unique(loan$LoanKey), ] 

# removing LoanStatus no needed 
sel_status  <- loan_nd$LoanStatus %in% c("Completed", "Current", 
                                          "ChargedOff", "Defaulted", 
                                          "Cancelled")
loan_filtered <- loan_nd[sel_status, ]

# creating status variable for censoring
loan_filtered$status <- ifelse(
  loan_filtered$LoanStatus == "Defaulted" |
    loan_filtered$LoanStatus == "Chargedoff",  1, 0)

# adding the final date to "current" status
head(levels(loan_filtered$ClosedDate))
levels(loan_filtered$ClosedDate)[1] <- "2014-11-03 00:00:00"

# creating the time-to-event variable
loan_filtered$start <- as.Date(loan_filtered$LoanOriginationDate)
loan_filtered$end <- as.Date(loan_filtered$ClosedDate)
loan_filtered$time <- as.numeric(difftime(loan_filtered$end, loan_filtered$start, units = "days"))

# there is an error in the data (time to event less than 0)
loan_filtered <- loan_filtered[-loan_filtered$time < 0, ]

# just considering a year of loans creation
ii <- format(as.Date(loan_filtered$LoanOriginationDate),'%Y') %in% 
  c("2006")
loan_filtered <- loan_filtered[ii, ] 


dim(loan_filtered)
head(loan_filtered)[, c(51, 65, 6, 7, 19, 18, 50, 83, 84, 85)]

#------



# censoring status 0 = censored, 1 = no censored (default)
table(loan_filtered$status)
prop.table(table(loan_filtered$status))


# median time until default (taking into account just no cendored data)
median(loan_filtered$time[loan_filtered$status==1])  # I'm underestimating


# median time until default (with all data)
mean(loan_filtered$time)  # I'm underestimating the median survival too 
# (in censored times, the real time is bigger)

```





@KM58 obtained a nonparametric estimate of the survival function, called product-limit, which is the generalization of the empirical estimator for censored data

\[
\hat S(t) = P(T \gt t) = \prod_{i:t_i \le t} \bigg[1-\frac{d_i}{n_i} \bigg]
\]
where $t_1, t_2, \ldots,t_n$ are the observed event times, $d_i$ is the number of events at time $t_i$, and $n_i$ is the number of individuals at risk at time $t_i$ (i.e, the original sample minus all those who had the event before $t_j$.)

Note that $d_i/n_i$ is the proportion that failed at the event time $t_i$ and $1 - d_i/n_i$ is the proportion surviving the event time $t_j$.

The Kaplan-Meier estimate is a step function with jumps at event times. The size of the steps depend on the number of events and the number of individuals at risk at the corresponding time. Note that if the last data is censored, the estimator will not reach the zero value.

Without censoring, the estimator is equivalent to the empirical survival function 
$\hat S(t) = \frac{1}{n} \sum_{i=1}^{n} I(t_i \gt t)$ or  to the one using the risk estimates $\hat S(t) = \prod_{k = 1}^{t-1} \bigg [ 1- \hat \lambda(k)\bigg]$.

```{r}
km <- survfit(Surv(time, status) ~ 1, data = loan_filtered)
km  # we can see the correct estimated median
print(km, print.rmean = TRUE)
```




```{block2, type = "rmdexercise_sestelo"}
Take a look at `?Surv` of the survival package.
```



### Other representation 

Assume that $\widetilde T_i = min (T_i, C_i)$ and $\Delta_i = I (T_i \le C_i)$, we introduce a weighted average representation of the Kaplan-Meier estimator which will be used later to introduce estimators for the conditional survival function

\begin{equation*}
\widehat S(y)=1-\sum_{i=1}^{n}W_{i}I(\widetilde T_{(i)}\leq y),
\end{equation*}
where $\widetilde T_{\left( 1\right) }\leq ...\leq \widetilde T_{\left( n\right) }$ denotes the ordered $\widetilde T$-sample and 


\begin{equation*}
W_{i}=\frac{\Delta_{\left[ i\right] }}{n-i+1}\prod_{j=1}^{i-1}\left[ 1-\frac{%
\Delta _{\left[ j\right] }}{n-j+1}\right]
\end{equation*}

\noindent is the Kaplan-Meier weight attached to $\widetilde T_{\left( i\right) }$. In the expression of $W_{i}$ notation $\Delta_{\left[ i\right] }$ is used for the $i$-th concomitant value of the censoring indicator (that is, $\Delta_{\left[ i \right] }=\Delta _{j}$ if $\widetilde T_{\left( i\right) }=\widetilde T_{j}$).









## Pointwise confidence interval for $S(t)$

For the contruction of the confidence interval for the estimated survival we can use a well-know estimator of the variance, the **Greenwood estimator**[@greenwood]. The Greenwood variance estimate for a Kaplan-Meier curve is defined as


\[
\hat \sigma^2[\hat S(t)] = \widehat var[\hat S(t)] = \hat S(t)^2 \sum_{i:t_i \le t} \frac{d_i}{n_i(n_i-d_i)}
\]

In case of no censoring, this estimator reduces to

$\hat \sigma^2[\hat S(t)] = \frac{\hat S(t) [1- \hat S(t)]}{n}$.



It is possible to use this estimator to derive a confidence interval for all time points $t$. Assuming asintotic normality ($\hat S(t) \simeq N(\hat S(t), \sigma(t)/\sqrt(n))$) and let $\sigma$ denotes the Greenwood’s standard deviation. Then confidence intervals for the survival function are then computed as follows (plain)


\[
\bigg(\hat S(t) \pm z_{1-\alpha/2}  \cdot \hat \sigma/\sqrt(n) \bigg), 
\]
where $\hat \sigma = se(\hat S(t))$ is calculated using Greenwood's formula.

It is important to hightlight here that this confidence interval may be out of the (0,1) interval! For solve this, the approximation to the normal distribution is improved by using the **log-minus-log** transformation

\[
\bigg(\hat S(t) \pm e^{z_{1-\alpha/2}  \cdot  \frac{\hat\sigma}{\hat S(t) ln \hat S(t)}} \bigg). 
\]

Other options include the **log** transformation
\[
 \exp \bigg( \ln(\hat S(t)) \pm z_{1-\alpha/2}  \cdot \hat\sigma/ \hat S(t)  \bigg). 
\]

In `R` we can select these options as: `log`(default), `log-log` and `plain`.


```{r}
km1 <- survfit(Surv(time, status) ~ 1, data = loan_filtered) # conf.type = "log" (default) 
summary(km1, times = c(200, 1100))

km2 <- survfit(Surv(time, status) ~ 1, data = loan_filtered, conf.type = "plain") 
summary(km2, times = c(200, 1100))

km3 <- survfit(Surv(time, status) ~ 1, data = loan_filtered, conf.type = "log-log") 
summary(km3, times = c(200, 1100))
```




```{block2, type = "rmdexercise_sestelo"}
See arguments `times` and `censored` of the function `summary.survfit`.
```

<br><br>

And now... what about the **empirical distribution** (without taking into account the censored data)? We can compare both!




```{block2, type = "rmdexercise_sestelo"}
With the Prosper dataset, try to compare in a graphical manner the survival function based on empirical distribution function of the time to default and based on the Kaplan-Meier estimator.
```



<!-- ```{r, eval = FALSE} -->
<!-- ed <- survfit(Surv(time, rep(1, length(time))) ~ 1, data = loan_filtered)  # with survfit taking the censoring into account -->

<!-- aux <- ecdf(loan_filtered$time) # with ecdf -->
<!-- t <- sort(loan_filtered$time) -->
<!-- res <- aux(t) -->

<!-- plot(km, conf.int = FALSE) -->
<!-- lines(ed$time, ed$surv, col = 2, type = "s") -->
<!-- lines(t, 1 - res, col = 3, type = "s") -->

<!-- ``` -->









## Comparing survival curves 

As we have seen before, we can use the `survfit` function to estimate the survival using the Kaplan-Meier estimator taking into account the censored data. Additionally, it is possible to include a **factor** in the model and to obtain the estimated survival for each of the levels of the factor. 


```{r}
model <- survfit(Surv(time, status) ~ IsBorrowerHomeowner, data = loan_filtered)
plot(model, ylab = "Survival", xlab = "Time (in days)", col = 1:2, mark.time = TRUE)
legend("topright", col = 1:2, legend =
         levels(factor(lung$sex)), 
       bty = "n", pch = 19)
```



Now, the questions that arises is if **these two curves are statistivally equivalent**. For answering it, we can use the **log-rank test** [@mantel;@CIS-11103]. This is the most well-known and widely used method to test the null hypothesis of no difference in survival between two or more independent groups. It is a large-sample chi-square test that is obtained by constructing a two by two contingency table at each distinct event time, and comparing the failure rates between the two groups, conditional on the number at risk in each group. The test compares the entire survival experience between groups and can be thought of as a test of whether the survival curves are identical or not. 




```{block2, type = "rmdhint_sestelo"}
When we state that two KM curves are *statistically equivalent*, we mean that, based on a testing procedure that compares the two curves in some *overall sense*, we do not have evidence to indicate that the true (population) survival curves are different.
```




The null hypothesis ($H_0$) of the testing procedure is that **there is no overall difference between the two (or $k$) survival curves**. Under this $H_0$, the log–rank statistic is approximately a chi-square with $k-1$ degree of freedom. Thus, tables of the chi-square distribution are used to determine the pvalue. 


This test is the one with most power to test differences that fit the proportional hazards model - so works well as a set-up for subsequent Cox regression. It gives equal weight to early and late failures. 

An alternative test that is often used is the **Peto & Peto**  [@CIS-11103] modification of the Gehan-Wilcoxon test [@10.2307/2333825]. This last one is a variation of the log-rank test statistic and is derived by applying different weights at the $f-$th failure time. This approach  is most sensitive to early differences (or earlier time points) between survival.


 
 This type of weighting may be used to assess whether the effect of a treatment/marketing campaing on survival is strongest in the earlier phases of administration/contacto and tends to be less effective over time.
 
 

 <!-- The log-rank or Mantel-Haenszel test is the most powerful under proportional hazards whereas the Peto \& Peto modification of the log-rank test is more sensitive to early differences between survival -->




In the **absence of censoring**, these methods reduce to the Wilcoxon-Mann-Whitney rank-sum test  [@mann1947] for two samples and to the Kruskal-Wallis test [@doi:10.1080/01621459.1952.10483441] for more than two groups of survival times.



Of course, several other variations of the log-rank test statistic using weights on each event time have been proposed in the literature [@CIS-23788; doi:10.1093/biomet/69.3.553; 10.2307/2289169].




The log-rank test and the Peto & Peto  modification of the log-rank test are both implemented in the `survdiff` function in library `survival`.

```{r}
survdiff(Surv(time, status) ~ IsBorrowerHomeowner, data = loan_filtered, rho = 0) # log-rank

survdiff(Surv(time, status) ~ IsBorrowerHomeowner, data = loan_filtered, rho = 1)# peto & peto

# with more than 2 groups
survdiff(Surv(time, status) ~ CreditGrade, data = loan_filtered)
```

If the null hyphotesis is rejected, we can apply a post-hoc analysis. One approach would be to perform pairwise comparisons. This can be achieved with the `pairwise_survdiff` function of the package `survminer` which calculates pairwise comparisons between group levels with corrections for multiple testing.  



```{block2, type = "rmdexercise_sestelo"}
Use the function `pairwise_survdiff` of the library `survminer` in order to perform pairwise comparisons.
```



More beaitiful plots... 

```{r}
autoplot(model) #using ggplot2
survminer::ggsurvplot(model)
survminer::ggsurvplot(model, conf.int = TRUE)





```






## Pros and Cons of the Kaplan-Meirs estimator


Pros:

- It is commonly used to describe survivor.
- It is commonly used to compare two study populations.
- It is intuitive graphical presentation.

Cons:

- It is mainly descriptive.
- It does not control for covariates.
- It can not accommodate time-dependent variables.






```{block2, type = "rmdexercise_sestelo"}
With your dataset, obtain the estimated survival curve with the Kaplan-Meier estimator for the time-to-event "bring the payroll to the BBVA". Try to find some differences between type of client.
```








<!-- Nelson-Aalen estimator -->

<!-- ```{r} -->
<!-- km <- survfit(Surv(time, status) ~ 1, data = lung) -->
<!-- h <- km$n.event/km$n.risk -->
<!-- na <- cumsum(h) # Nelson-Aalen (NA) -->

<!-- plot(km$time, na, type = "s", main = "Nelson-Aalen estimator", -->
<!--      xlab = "Time", ylab = "Cumulative hazard")  -->
<!-- #points(km$time,-log(km$surv),col=2) #NA aprox.  -->
<!-- lines(km$time, na + qnorm(0.975, 0, 1) * km$std.err, lty = 2) -->
<!-- lines(km$time, na - qnorm(0.975, 0, 1) * km$std.err, lty = 2)  -->

<!-- z=survreg(Surv(time,status)~1,dist="weibull",data=lung) -->

<!-- sur <- pweibull(lung$time,shape = 1/exp(z$icoef[1]), scale = exp(z$icoef[1])) -->
<!-- plot(lung$time, 1 - sur) -->
<!-- par(add = T) -->
<!-- plot(km) -->



<!-- ``` -->



<!-- See examples of `?survreg` for parametrization weibull. -->





<!--chapter:end:03_kaplan_meier.Rmd-->

# The Cox Proportional Hazards Model {#cox}

This Chapter describes the Cox Proportional Hazards model, a very popular statistical model used for analyzing survival data. 

As we seen before, the survival function is the probability that the time-to-event will be greater than some specified time and this probability depends on:

- the **underlying hazard function** (how the risk of occurs the event per unit time changes over time at baseline covariates)

- the **effect parameters** (how the hazard varies in response to the covariates)

We are going to use the Cox proportional hazards model to determine the effect of the covariates on survival. 



## The semiparametric model


A parametric survival model is one in which survival time (the outcome) is assumed to follow a known distribution. Examples of distributions that are commonly used for survival time are: the **Weibull**, the **exponential** (a special case of the Weibull), the **log-logistic**, the **log-normal**, etc.


The Cox proportional hazards model, by contrast, is not a fully parametric model. Rather it is a **semi-parametric model** because even if the regression parameters (the betas) are known, the distribution of the outcome remains unknown. The baseline survival (or hazard) function is not specified in a Cox model (we do not assume any shape or form).




As before, let $T$ denote the time to some event. Our data, based on a sample of size $n$, consists of the triple $(\widetilde{T}_i, \Delta_i, \textbf{X}_i$, $i  = 1,...,n$ where $\widetilde{T}_i$ is the time on study for the $i$-th patient, $\Delta_i$ is the event indicator for the $i$-th patient ($\Delta_i=1$ if the event has occurred and  $\Delta_i=0$ if the lifetime is right-censored) and $\textbf{X}_i=   (X_{i1},\ldots, X_{ip})^t$ is the vector of covariates or risk factors for the $i$-th individual which may affect the survival distribution of $T$.  


```{block2, type = "rmdhint_sestelo"}
Note that the covariates $X_{ij}$, with $j = 1, \ldots, p$,  may be time-dependent as $\textbf X_i(t)=(X_{i1},\ldots,X_{ip})^t$ whose value changes over time. This situation must be analyzed using the **Extended Cox PH model**. However, for ease of presentation, we shall consider the fixed-covariate case.
```


The Cox PH regression model [@CIS-11133] is usually written in terms of the hazard model formula as follows

\[
h(t, \textbf X) = h_0(t)  e^{\sum_{j=1}^p \beta_j X_j}.
\]

This model gives an expression for the hazard at time $t$ for an individual with a given specification of a set of explanatory variables denoted by the bold $\textbf X$. 

Based on this model we can say that the hazard at time $t$ is the product of two quantities:

- The first of these, $h_0(t)$, is called the **baseline hazard function** or the hazard for a reference individual with covariate values 0.

- The second quantity is a **parametric component** which is a linear function of a set of $p$ explanatory $X$ variables that is exponentiated (it will be the *relative risk* associated with covariate values $X$).

<!-- The second quantity is the exponential expression $e$ to the **linear sum of $\beta_j X_j$**, where the sum is over the $p$ explanatory $X$ variables. -->


Note that an important feature of this model, which concerns the  **proportional hazards (PH) assumption**, is that  the baseline hazard is a function of $t$, but does not involve the covariates. By contrast, the exponential expresion involves the $X$'s but not the time. The covariates here have a multiplicative effect and are called **time-independent**.[^3]



[^3]: It is possible, nevertheless, to consider covariates which do involve time. Such covariates are called **time-dependent** variables. When we consider these time-dependent covariates, the model is called the **extended Cox model** and in this case it no longer satisfies the proportional hazards assumption.


Note that **the model is assuming proportional hazards** (the hazard for any individual $i$ is a fixed proportion of the hazard for any other individual $j$), that is:

\[
\frac{h_i(t|\textbf X_i)}{h_j(t|\textbf X_j)} = exp(\boldsymbol \beta(\textbf X_i - \textbf X_j))
\]

or

\[
h_i(t|\textbf X_i) = \exp( \boldsymbol \beta(\textbf X_i - \textbf X_j)) h_j(t|\textbf X_j)
\]
so hazard functions for each individual should be strictly parallel and the hazard ratio is constant over time.





## Estimation


The estimation of the model is obtained by **Maximun Likelihood**, particularly maximazing the **"partial" likelihood function** rather than a (complete) likelihood function. The term "partial" likelihood is used because the likelihood formula considers probabilities only for those subjects who fail, and does not explicitly consider probabilities for those subjects who are censored. The "partial" likekihood is given by:

\[
L(\boldsymbol \beta) = \prod_{i:\Delta_i = 1} \frac{\exp\bigg[ \sum_{j=1}^{p}\beta_j X_{(i)j} \bigg]}{\sum_{k \in R(t_i)} \exp \bigg[ \sum_{j=1}^{p}\beta_j X_{(k)j} \bigg]}
\]
being $t_1 < t_2 < \ldots < t_D$  the ordered event times, $Z_{(i)j}$ the $j$-th covariate associated with the individual whose failure time is $t_i$ and $R(t_i)$ the risk set at time $t_i$, that is, the the set of all individuals who are still under study at a time just prior to $t_i$.

Note that the numerator of the likelihood depends only on information from the individual who experiences the event, whereas the denominator uses information about all individuals who have not yet experienced the event (including some individuals who will be censored later).


The **(partial) maximum likelihood** estimates are found by maximizing the $ln (L(\boldsymbol \beta))$ particularly, by taking partial derivatives of $ln (L(\boldsymbol \beta))$ with respect to each parameter in the model, and then solving a system of equations. For this algorithm such as Newton–Raphson [@doi:10.1137/1037125] or Expectation-Maximitazion [@10.2307/2984875] are used.[^4]

[^4]: In the presence of ties, the @10.2307/1402659 or @doi:10.1080/01621459.1977.10480613 approximations to the log-likelihood can be used.



In `R`, we can estimate this model using the `coxph` function of the `survival` package.



```{r}
loan_filtered$LoanOriginalAmount2 <-  loan_filtered$LoanOriginalAmount/10000

model <- coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner +
                 IncomeVerifiable, data = loan_filtered) 
```




For taking the **ties** into account we can use the `method` argument


```{r, eval = FALSE}
coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner +
        IncomeVerifiable, data = loan_filtered, method = "efron") 

coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner +
        IncomeVerifiable, data = loan_filtered, method = "breslow") 

coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner +
        IncomeVerifiable, data = loan_filtered, method = "exact") 
```




## Computing the Hazard Ratio

One of the main goals of the Cox PH model is to **compare the hazard rates** of individuals who have different values for the covariates. The idea is that we care more about comparing groups than about estimating absolute survival. To this end, we are going to use the **Hazard Ratio** (HR).

A hazard ratio is defined as the hazard for one individual divided by the hazard for a different individual. The two individuals being compared can be distinguished by their values for the set of predictors, that is, the $X$'s. We can write the hazard ratio as the estimate of



\[
\widehat{HR} = \frac{\hat h_i(t|\textbf X_i)}{h_j(t|\textbf X_j)} = \frac{\hat h_0(t) \exp (\boldsymbol{\hat \beta} \textbf X_i)}{\hat h_0(t) \exp (\boldsymbol{\hat \beta}\textbf X_j)}=exp(\boldsymbol{\hat \beta}(\textbf X_i - \textbf X_j)).
\]






Additionally, we can construct a $(1-\alpha)$% confidence interval for the hazard ratio as
\[
\exp( \boldsymbol{\hat \beta}(\textbf X_i - \textbf X_j) \pm z_{1-\alpha/2} \hspace{0.2cm} \widehat{se}(\boldsymbol{\hat \beta}(\textbf X_i - \textbf X_j)), 
\]
where $\widehat{se}(\boldsymbol{\hat \beta}(\textbf X_i - \textbf X_j))$ is equal to $\sqrt{ \widehat{Var}(\boldsymbol{\hat \beta}(\textbf X_i - \textbf X_j))}$.


In order to understand what this hazard ratio means, we are going to see same examples. 

In the first one we are using **a discrete predictor** (`smoking`) and  we will see the hazard ratio for smoking versus not smoking adjusted by the age. So, let $\textbf X_i:(smoking=1, age = 60)$ and $\textbf X_j:(smoking=0, age = 60)$, the hazard ratio is

\[
HR= \frac{h_i(t|\textbf X_i)}{h_j(t|\textbf X_j)} = \frac{h_0(t) e^{\beta_{smoking} \cdot 1 + \beta_{age} \cdot 60}}{h_0(t) e^{\beta_{age} \cdot 60}} = e^ {\beta_{smoking}}
\]

For example, if $\beta_{smoking}= 0.5$,  the hazard ratio for smoking adjusted for age will be $exp(0.5)= 1.65$. That is, the hazard of death increases 65% for smokers.



In the second example we use a **continuous predictor**, `age` of the individuals.  Let $\textbf X_i:(smoking=0, age = 70)$ and $\textbf X_j:(smoking=0, age = 60)$, the hazard ratio for a ten years increase in age adjusted by smoking is

\[
HR= \frac{h_i(t|\textbf X_i)}{h_j(t|\textbf X_j)} = \frac{h_0(t) e^{\beta_{smoking} \cdot 0 + \beta_{age} \cdot 70}}{h_0(t) e^{\beta_{smoking} \cdot 0+ \beta_{age} \cdot 60}} = e^{\beta_{age}(70-60)} = e^{\beta_{age}\cdot 10 = (e^{\beta_{age}})^{10} }
\]

Note that $e^{\beta_{age}}$ is the hazard ratio for a 1-unit increase in the predictor.


```{block2, type = "rmdhint_sestelo"}
**Interpretation of the hazard ratio (like Odds Ratio in Logistic Models)**
  
  - HR = 1: no effect 
  - HR > 1: increase in the hazard      
  - HR < 1/10: reduction in the hazard    
```



Moving again on the `R` code, we can see (by means of the `summary` function) the hazard ratios for the covariates included in the model

```{r}
m1 <- coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner +
              IncomeVerifiable, data = loan_filtered) 
summary(m1)
termplot(m1, terms = "IsBorrowerHomeowner")
```


The estimated hazard ratio for `IsBorrowerHomeowner == True` vs `IsBorrowerHomeowner == False` is 0.78 with a 95% CI of (0.69, 0.88), that is, `IsBorrowerHomeowner == True` has 0.78 times the hazard of `IsBorrowerHomeowner == False`, a 22% lower hazard rate.  The estimated hazard ratio for `IsBorrowerHomeowner == False` vs `IsBorrowerHomeowner == True` is  1.28. Note that the procedure is the same for the other covariates. 




## Hypothesis testing

In order to test the significance of a variable or a interaction term in the model we can use two procedures:

- the **Wald test** (typically used with Maximun Likelihood estimates)

- the **Likelihood Ratio test (LRT)** (it uses the log likelihood to compare two nested models)

The null hypothesis of the **Wald test** states that the coeficient $\beta_j$ is equal to 0. The test statistics is

\[
Z = \frac{\hat \beta_j - 0}{Std. Err (\hat \beta_j)} \sim N(0,1)
\]

```{r}
summary(m1)$coef

# by hand... for IncomeVerifiable
z <- summary(m1)$coef[3, 1]/summary(m1)$coef[3, 3]  
pvalue <- 2 * pnorm(z, lower.tail = FALSE)
pvalue
```
According to the pvalue of the test, the null hypothesis is accepted (for the `IncomeVerifiable` variable). Thus, the model must not include this variable.


The other approach is to use the  **Likelihood Ratio test**. In this case, we need to compute the difference between the log likelihood statistic of the *reduced model* which does not contain the variable that we want to test and the log likelihood statistic of the *full model* containing the variable. In general, the LRT statistic can be written in the form of 

\[
LRT = -2 ln \frac{L_R}{L_F}= 2 ln(L_F) - 2 ln(L_R) \sim \chi^2_p
\]
where $L_R$ denotes the log likelihood of the reduced model with $k$ parameter  and $L_F$ is the log likelihood of the full model with $k + p$ parameters. $\chi^2_p$ is a Chi-square with $p$ degrees of freedom, where $p$ denotes the number of predictors being assessed.



```{block2, type = "rmdhint_sestelo"}
In general, the Likelihood Ratio test and Wald statistics may not give exactly the same answer. It has been shown that of the two test procedures, the LR statistic has better statistical properties, so when in doubt, you should use the **LRT**.
```




```{r}

m_red <- coxph(Surv(time, status) ~ LoanOriginalAmount2 + IsBorrowerHomeowner,
               data = loan_filtered) 
anova(m_red, m1) #fist the reduced, second the full

# by hand... for IncomeVerifiable variable
m1$loglik  # the first is the log likelihood of a model that contains
           #      none of the predictors, so we need the second one

chi <- 2 * m1$loglik[2] - 2 * m_red$loglik[2]
pvalue <- 1 - pchisq(chi, df = 1) # df = 3 - 2
pvalue
```

In this case, using an $\alpha = 0.05$ and testing the significance of the `IncomeVerifiable` variable, we must remove it from the model.



## Adjusting Survival Curves

From a survival analysis point of view, we want to obtain also estimates for the survival curve. Remember that if we do not use a model, we can apply the Kaplan-Meier estimator.  However, when a Cox model is used to fit survival data, survival curves can be obtained adjusted for the explanatory variables used as predictors. These are called **adjusted survival curves** and, like Kaplan-Meier curves, these are also plotted as step functions.


The hazard formula seen before can be converted to a survival function as

\[
S(t|\textbf X) = \bigg[ S_0(t) \bigg]^{e^{\sum_{j=1}^p \beta_j X_j}}.
\]

This survival function formula is the basis for determining adjusted survival curves. The estimates of $\hat S_0(t)$ and $\hat b_j$ are provided by the computer program that fits the Cox model. The $X$'s, however, must first be specified by the investigator before the computer program can compute the estimated survival curve.


```{block2, type = "rmdhint_sestelo"}
Typically, when computing adjusted survival curves, the value chosen for a covariate being adjusted is an average value like an arithmetic mean or a median.
```


The `survfit` function estimates $S(t)$, by default at the mean values of the covariates:

```{r}
m2 <- m_red
newdf <- data.frame(IsBorrowerHomeowner = levels(loan_filtered$IsBorrowerHomeowner), 
                    LoanOriginalAmount2 = rep(mean(loan_filtered$LoanOriginalAmount2), 2))
fit <- survfit(m2, newdata = newdf)
#summary(fit) # to see the estimated values
plot(fit, conf.int = TRUE, col = c(1,2))
legend("bottomleft", levels(newdf[,1]), col = c(1, 2), lty = c(1,1))
```

```{r}
# another option using the survminer package
survminer::ggsurvplot(fit)
```


```{r}
# easier... without refitting
ggcoxadjustedcurves(m2, data = loan_filtered, 
                    variable = loan_filtered$IsBorrowerHomeowner)
```




```{block2, type = "rmdhint_sestelo"}
For some help with the `survminer` package... Download the cheatsheet [here](
http://www.sthda.com/english/rpkgs/survminer/survminer_cheatsheet.pdf).
```


```{block2, type = "rmdexercise_sestelo"}
Try to estimate a Cox PH model using your dataset.
```





## How to evaluate the PH assumption?

Now we are going to illustrate two methods to evaluate the proportional hazards assumptions: one **graphical approach** and one **goodness-of-fit test**. Recall that the Hazard Ratio that compares two specifications of the covariates (defined as $\textbf{X}^*$ and $\textbf{X}$) can be expressed as

\[
HR = \exp(\sum_{j=1}^p \beta_j (X_j^* - X_j)) 
\]
where $\textbf{X}^*=(X_1^*, X_2^*, \ldots, X_j^*)$ and $\textbf{X}=(X_1, X_2, \ldots, X_j)$, and proportionally of hazards assumption indicates that this quantity is constant over time. Equivalently, this means that the hazard for one individual is proportional to the hazard for any other individual, where the proportionality constant is independent of time.


```{block2, type = "rmdhint_sestelo"}
**Think about this...**

It is important to note that if the graph of the **hazards cross** for two or more categories of a predictor of interest, the **PH assumption is not met**. However, althought the hazard functions do not cross, it is possible that the PH assumption is not met. Thus, rather than checking for crossing hazards, we need to use other apporaches.
```





### Graphical approach


The most popular graphical techniques for evaluating the PH assumption involves comparing estimated **–ln(–ln) survival curves** over different (combinations of) categories of variables being investigated. 

A log–log survival curve is simply a transformation of an estimated survival curve that results from taking the natural log of an estimated survival probability twice.[^5] 

[^5]: Note that the scale of the y-axis of an estimated survival curve ranges between 0 and 1, whereas the corresponding scale for a  -ln(-ln) curve ranges between  $-\infty$ and $+\infty$.


As we said, the hazard function can be rewritten as 
\[
S(t|\textbf X) = \bigg[ S_0(t) \bigg]^{e^{\sum_{j=1}^p \beta_j X_j}}
\]
and once we applied the -ln(-ln),  the expression can be rewritten as 
\[
-\ln \bigg[-\ln S(t|\textbf X) \bigg] =  - \sum_{j=1}^p \beta_j X_j - \ln  \bigg[-\ln S_0(t|\textbf X) \bigg].  
\]

Now, considering two different specifications of the covariates, corresponding to two different individuals, $\textbf X_1$ and $\textbf X_2$, and subtracting the second log–log curve from the first yields the expression

\[
-\ln \bigg[-\ln S(t|\textbf X_1) \bigg] = -\ln \bigg[-\ln S(t|\textbf X_2) \bigg] + \sum_{j=1}^p \beta_j (X_{1j} - X_{2j}) 
\]

This expression indicates that if we use a Cox model (well-used) and plot the estimated log-log survival curves for individuals on the same graph, the two plots would be approximately parallel. The distance between the two curves is the linear expression involving the differences in predictor values, which does not involve time.


Note that there is an **important problem** associated with this approach, that is, **how to decide** "how parallel is parallel?". This fact can be subjective, thus the proposal is to be conservative for this decision by assuming the PH assumption is satisfied unless there is strong evidence of nonparallelism of the log–log curves.


Now we are going to check the proportinal hazards assumption for the variable `IsBorrowerHomeowner`. This can be done by plotting **log-log Kaplan Meier survival estimates** against time (or against the log of time) and evaluating whether the curves are reasonably parallel. 

```{r}
km_home <- survfit(Surv(time, status) ~ IsBorrowerHomeowner, data = loan_filtered)
#autoplot(km_home) # just to see the km curves

plot(km_home, fun = "cloglog", xlab = "Time (in days) using log",
     ylab = "log-log survival", main = "log-log curves by clinic") 
```


```{r}
# another option
ggsurvplot(km_home, fun = "cloglog")
```

It seems that **the proportional hazards assumption is violated** as the log-log survival curves are not parallel.

<br>

Another graphical option could be to use the **Schoenfeld residuals** to examine model fit and detect outlying covariate values.  Shoenfeld residuals represent the difference between the observed covariate and the expected given the risk set at that time. They should be flat, centered about zero.


```{r}
ggcoxdiagnostics(m2, type = "schoenfeld")
```



```{r}
# another option
zph <- cox.zph(m2)
par(mfrow = c(1, 2))
plot(zph, var = 1)
plot(zph, var = 2)
```



### Goodness-of-fit test

A second approach for assessing the PH assumption involves **goodness-of-fit (GOF) tests**. To this end,  different test have been proposed in the literature [@1568a5d7e9974c31be97c0ff34c233a7]. We focuss in the @Harrell86, a variation of a test originally proposed by  @doi:10.1093/biomet/69.1.239. This is a test of **correlation between the Schoenfeld residuals and survival time**. A correlation of zero indicates that the model met the proportional hazards assumption (the null hypothesis). 


This can be applied by means of the `cox.zph` function of the  `survival` package.


```{r}
cox.zph(m2)
```


It seems again that the proportional hazards assumption is not satisfied (as we saw with the log-log survival curves).






## Non-Proportional Hazards... and now what?

A insignificant nonproportionality may make no difference to the interpretation of a dataset, particularly for large sample sizes. What if the nonproportionality is large and real? Possible approaches are possible in the context of the Cox model itself:

- **Stratify**. Covariates with nonproportional effects may be incorporated into the model as stratification factors rather than predictors (but... be careful, stratification works naturally for categorical variables, however for quantitative variables you would have to discretize).

- **Partition of the time axis**, if the proportional hazards assumption holds for short time periods but not for the entire study.

- **Nonlinear effect**. Continuous covariates with nonlinear effect may lead to nonproportional effects.





### An example... Stratified Proportional Hazards Models

Sometimes the proportional hazard assumption is violated for some covariate. In such cases, it is possible to stratify taking this variable into account and use the proportional hazards model in each stratum for the other covariates. We include in the model  predictors that satify the proportional hazard assumption and remove from it the predictor that is stratified.

Now, the subjects in the $z$-th stratum have an arbitrary baseline hazard function $h_{0z}(t)$ and the effect of other explanatory variables on the hazard function can be represented by a proportional hazards model in that stratum.



In the Stratified Proportional Hazards Model the regression coefficients are assumed to be the same for each stratum although the baseline hazard functions may be different and completely unrelated.





```{r}
m3 <- coxph(Surv(time, status) ~ LoanOriginalAmount2  +
              strata(IsBorrowerHomeowner), data = loan_filtered) 
summary(m3)
```
You can see that the output is similar to previous model without stratification however, in this case, we do not have information about the hazard ratio of the stratification variable, `IsBorrowerHomeowner`. This variable is not really in the model.  In any case, you can plot it...


```{r}
ggsurvplot(survfit(m3), data = loan_filtered, conf.int = TRUE)
```




```{block2, type = "rmdexercise_sestelo"}
Check  the proportional hazard assumption of the Cox PH model estimated in your dataset.
```



## Why Cox PH model is so popular? (pros of the model)



- It is a "robust" model, so that the results from using the Cox model will closely approximate the results for the correct parametric model (even though the baseline hazard is not specified).

- The specific form of the model - which gives the hazard function as a product of a baseline hazard involving $t$ and an exponential expression involving the $X$'s without $t$ - is very interesting. The exponential part of this product is appealing because it ensures that the fitted model will always give estimated hazards that are non-negative (this is perfect, because by definition, the value of the hazard function must range between zero and plus infinity).

- Although the baseline hazard part of the model is unspecified, we can estimate the betas in the exponential part of the model (as we have seen). Then, the hazard function $h(t,\textbf X)$ and its corresponding survival curves $S(t, \textbf X$) can also be estimated. 

- Finally, it is preferred over the logistic model when survival time information is available and there is censoring. Because you can obtain more information!







## Bonus track 1: Additive Cox model

The Cox PH model assumes a linear effect of the predictors. If the true effect is highly nonlinear this can lead to a nonproportinal hazards or misleading statistical conclusions.

One alternative approach is to use an Additive Cox model [@NoRefworks:5] of the form

\[
h(t, \textbf X) = h_0(t) e^{\sum_{j=1}^p f_j(\textbf X_j)}
\]
with $f_j$ being an unknown and smooth function.

In order to estimate this model one could use  the [`mgcv`](https://cran.r-project.org/web/packages/mgcv/index.html) package as follows 


```{r}
m4 <- mgcv::gam(time ~ s(LoanOriginalAmount2) + IsBorrowerHomeowner, 
                data = loan_filtered, family = "cox.ph", weights = status)
summary(m4)
plot(m4, pages = 1, all.terms = TRUE)
```


```{block2, type = "rmdhint_sestelo"}
Note the change in the sintaxis compared with the previous examples. The status indicator in used in the `weights` argument.
```




## Bonus track 2: Machine Learning for estimating the Cox PM model

The [`rpart`](https://cran.r-project.org/web/packages/rpart/index.html) package  builds R's basic tree models of survival data. For an overview you can consult the section 8.4 of the rpart [vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf).


Additionally, the new package [`ranger`](https://cran.r-project.org/web/packages/ranger/index.html) [@Wright:2017aa] is a fast implementation of the Random Forests algorithm for building ensembles of classification and regression trees, working also with survival data. Since `ranger()` uses standard `Surv survival objects, it’s an ideal tool for getting acquainted with survival analysis in this machine-learning age.


Under construction!







<!--chapter:end:04_cox_model.Rmd-->

# Joint Models for Longitudinal and Time-to-Event Data

In this Chapter we will see a joint modelling approach in order to analyze **two types of outcomes** produced usually in longitudinal studies, particularly, a set of **longitudinal response measurements** and the **time to an event of interest**, such as default, death, etc.

These two outcomes are usually analyzed separately, using a **mixed effects model** ((CITAS)) for the longitudinal response and a **survival model** for the time-to-event. Here, we are going to see how we can analyze them jointly.


#### Why should I use these type of models? {-}

As we mentioned in Chapter \@ref(cox), the Cox PH hazard model can be extended in order to incorporate time-dependent variables. However, when we focus our interest in the time-to-event and we wish to take into account the effect of the longitudinal variable as a time-dependent covariate, **traditional approaches** for analyzing time-to-event data (such as the partial likelihood for the Cox proportional hazards models) **are not applicable in all situations**. In particular, **standard time-to-event models require that time-dependent covariates are external**; that is, the value of this covariate at time point $t$ is not affected by the occurrence of an event at time point $u$, with $t > u$ [@kalbfleisch1980statistical, Section 6.3]. However, the type of time-dependent covariates that we have in longitudinal studies do not met this condition, this is due to the fact that they are the output of a stochastic process generated by the subject, which is directly related to the failure mechanism. Based on this, in order to produce correct inferences, we need to apply a joint model that takes into account the joint distribution of the longitudinal and survival outcomes.

**Another advantage** of these models is that they allow  to deal with the **error measurements** in the time dependent variables (longitudinal variable in this case). In a Cox model with time dependent covariates we assume that the variables are measured without error.



```{block2, type = "rmdhint_sestelo"}
When we think in time-dependent covariates, we should first distinguish between two different categories, namely, **internal or endogenous** covariates or **external or exogenous** covariates. Internal covariates are generated from the patient herself and therefore require the existence of the patient, for example *CD4 cell count* and the hazard for death by HIV are stochastic processes generated by the patient herself. On the other hand, *air pollution* is an external covariate to asthma attacks, since the patient has no influence on air pollution.
```






## Linear Mixed Models 

As we mentioned, **Joint Models** take two outcomes into account, the **longitudinal response** and the **survival time**. In order to estimate these type of models, we need first to fit  a model for the longitudinal response (usually a **linear mixed model**) and then for the survival time.   I am assuming here that you have understood entirely the Chapter \@ref(cox) and you do not have any problem with the estimation of the Cox model by means of the `coxph` function. Regarding the linear mixed model you can see an brief introduction with  examples below using the [`nlme`](https://cran.r-project.org/web/packages/nlme/index.html) package. For a good overview you can consult the Chapter 2 of @book:1606416.




<br><br>



So, our focus in this part is on longitudinal data. This data can be defined as the data resulting from the **observations of subjects** (e.g., human beings, animals, etc.) that are **measured repeatedly over time**.  From this descriptions, it is evident that in a longitudinal setting we expect repeated measurements taken on the same subject to exhibit positive correlation. This feature implies that standard statistical tools, such as the t-test and simple linear regression that assume independent observations, are not appropriate for longitudinal data analysis (they  may produce invalid standard errors). In order to solve this situation and obtain valid inference, one possible approach is to use a **mixed model**, a regression method for continuous outcomes that models longitudinal data by assuming, for example, **random errors within a subject** and **random variation in the trajectory among subjects**.




We are going to explain briefly this approach. Figure \@ref(fig:mixed) shows an example with hypothetical longitudinal data for two subjects. In this figure, monthly observations are recorded for up to one year. Note that each subject appears to have their own linear trajectory but with small fluctuations about the line. This fluctuations are referred to as the **within-subject variation** in the outcomes. Note that if we only have data from one person these will be the typical error term in regression.  The dashed line in the center of the figure shows the average of individual linear-time trajectories. This line characterizes the **average for the population** as a function of time.  For example, the value of the dashed line at month 2 is the mean response if the observation (at two months) for all subjects was averaged. Thus, this line represents both the typical trajectory and the population average as a function of time.



The main idea of **Linear Mixed Model** is that they make specific assumptions about the variation in observations attributable
to **variation within a subject** and to **variation among subjects**. To formally introduce this representation of longitudinal data, we let $Y_{ij}$ denote the response of subject $i, i = 1, \ldots, n$ at time $X_{ij}, j = 1,...,n_i$ and $\beta_{i0} + \beta_{i1} X_{ij}$ denote the line that characterizes the observation path for $i$.  Note that each subject has an individual-specific intercept and slope. Note that

+ The **within-subject variation** is seen as the deviation between individual observations, $Y_{ij}$, and the individual linear trajectory, that is $Y_{ij} - (\beta_{i0} + \beta_{i1} X_{ij})$.

+ The **between-subject variation** is represented by the variation among the intercepts, $var(\beta_{i0})$ and the variation among subject in the slopes $var(\beta_{i1})$.


```{r, "mixed", fig.cap = "Hypothetical longitudinal data for two subjects.", echo = FALSE, fig.align = 'center'}
knitr::include_graphics("images//mixed_image.png")
```

Figure \@ref(fig:mixed) taken from @bio.





If parametric assumptions are made regarding the within- and between-subject components of variation, it is possibel to use  *maximum likelihood methods* for estimating the regression parameters (which characterize the population average), and the variance components (which characterize the magnitude of within- and between-subject heterogeneity). For continuous outcomes it is a good idea to assume that within-subject errors are normally distributed and to assume that intercepts and slopes are normally distributed among subjects. This will be


+ within-subjects:
\[
E(Y_{ij}|\beta_i) = \beta_{i,0} + \beta_{i, 1} X_{ij}
\]


\[
Y_{ij} = \beta_{i,0} + \beta_{i, 1} X_{ij} + \varepsilon_{ij}
\]

\[
 \varepsilon_{ij} \sim N(0, \sigma^2)
\]


+ between-subjects:


\[
 \bigg(\begin{array}{c} \beta_{i,0}\\ \beta_{i,1}\\ \end{array} \bigg) \sim
 
 N \bigg[ \bigg(\begin{array}{c} \beta_{0}\\ \beta_{1}\\ \end{array} \bigg),  \bigg(\begin{array}{c} D_{00} & D_{01}\\ D_{10} & D_{11}\\ \end{array} \bigg)   \bigg]
\]
where $D$ is the variance-covariance matrix of the random effects, with $D_00= var(b_{i,0})$  and $D_11= var(b_{i,1})$.




If we think in $b_{i,0}= (\beta_{i,0} - \beta_0)$ and $b_{i,1}= (\beta_{i,1} - \beta_1)$, the model can be written as

\[
Y_{ij} = \beta_0 + \beta_1 X_{ij} + b_{i,0} + b_{i,1} X_{ij} + \varepsilon_{ij}
\]
where $ b_{i,0}$ and  $b_{i,1}$ represent deviations from the population average intercept and slope respectively. In this equation there is *systematic* variation (given by the two first betas) and a **random** variation (the rest). 

The random component is partitioned into the observation level and subject level fluctuations: that is, the between-subject ($b_{i,0} + b_{i,1} X_{ij}$) and within-subject ($\varepsilon_{ij}$) variations.

A more general form including $p$ predictors is

\[
Y_{ij} = \beta_0 + \beta_1 X_{ij,1} +\ldots +  + \beta_p X_{ij,p} + b_{i,0} + b_{i,1} X_{ij,1} + \ldots + b_{i,p} X_{ij,p}+ \varepsilon_{ij}
\]

\[
Y_{ij} = X_{ij}'\beta + Z_{ij}' b_i + \varepsilon_{ij}
\]
where $X_{ij}'=[X_{ij,1}, X_{ij,2}, \ldots, X_{ij,p}]$ and $Z_{ij}'=[X_{ij,1}, X_{ij,2}, \ldots, X_{ij,q}]$. In general way, we assume that the covariates in $Z_{ij}$ are a subset of the variables in $X_{ij}$ and thus $q < p$.


```{block2, type = "rmdhint_sestelo"}
It is important to highlighted that based on this model the coefficient of covariate $k$ for subject $i$ is given as $(\beta_k + b_{i,k})$ if $k \le q$, and is simply $\beta_k$ if $q < k \le p$. Therefore,inalinearmixed model there may be some regression parameters that vary among subjects while some regression parameters are common to all subjects. 
```

Moving again onto the example, it seems that each subject has their own intercept, but the subjects may have a common slope. So, a **random intercept** model assumes parallel trajectories for any two subjects and is given as a special case of the general mixed model:

\[
Y_{ij} = \beta_0 + \beta_1 X_{ij,1} + b_{i,0} + \varepsilon_{ij}.
\]

Using the above model, the intercept for subject $i$ is given by $\beta_0 + b_{i,0}$ while the slope for subject $i$ is simply $\beta_1$ since there is no additional random slope, $b_{i,1}$ in the random intercept model.


If we assume that the slope for each individual $i$ can also be different, we have to use a **random intercept and slope** model of the type

\[
Y_{ij} = \beta_0 + \beta_1 X_{ij,1} + b_{i,0} + b_{i,1} X_{ij,1}+ \varepsilon_{ij}.
\]
and now the intercept for subject $i$ is given by $\beta_0 + b_{i,0}$ while the slope for subject $i$ is  $\beta_1 + b_{i, 1}$.



In order to fit these models, we can use the `lme` function of the `nlme` package. 


```{r}
head(aids) 
# CD4: square root CD4 cell count measurements
# obstime: time points at which the corresponding longitudinal response was recorded
```

```{r}
# random-intercepts model (single random effect term for each patient)

fit1 <- lme(fixed = CD4 ~ obstime, random = ~ 1 | patient, data = aids)
summary(fit1)
```

Note that the estimation for the variability or the variance components, that is, the variance of the errors ($\varepsilon_{ij}$, within personal errors) and the variance between subject (the variance of the $b_{i, 0}$) are given under *Random effects* heading. Under `(Intercept)` we can see the estimated standard desviation for the $b_{i,0}$ coefficients and under `Residual`, the estimated desviation for $\varepsilon_{ij}$.



```{r}
# variance of the beta_i0
getVarCov(fit1)

# standard desviation of e_ij
fit1$sigma

# total variance of the model
getVarCov(fit1)[1] + fit1$sigma**2 

# % variance within person
(fit1$sigma**2/(getVarCov(fit1)[1] + fit1$sigma**2)) * 100 

# % variance between person
(getVarCov(fit1)[1]/(getVarCov(fit1)[1] + fit1$sigma**2)) * 100 
```



The total variation in CD4 is estimated as `r round(nlme::getVarCov(fit1)[1] + fit1$sigma**2, 2)`. So, the proportion of total variation that is attributed to within-person variability is `r round((fit1$sigma**2/(nlme::getVarCov(fit1)[1] + fit1$sigma**2)) * 100, 2)`% with `r round((nlme::getVarCov(fit1)[1]/(nlme::getVarCov(fit1)[1] + fit1$sigma**2)) * 100, 2)`% of total variation attributable to individual variation in their general level of CD4 (attributable to random intercepts).





The estimated regression coefficients $\beta$ are provided under the *Fixed effects* heading. As expected, the coefficient for the time effect has a negative sign indicating that on average the square root CD4 cell counts declines in time. 

Well, this random-intercepts model poses the unrealistic restriction that the correlation between the repeated measurements remains constant over time (we are not includiying the random slope yet). So, a natural extension  is a more flexible specification of the covariance structure with the random-intercepts and random-slopes model.  This model introduces an additional random effects term, and assumes that the rate of change in the CD4 cell count is different from patient to patient.



```{r}
# random-intercepts and random-slopes model
fit2 <- lme(CD4 ~ obstime, random = ~ obstime | patient, data = aids) # the intercept is  included by default
summary(fit2)
```
 
We observe very minor differences in the estimated fixed-effect parameters compared with the previous model.

For the random effects, we can observe that there is greater variability between patients in the baseline levels of CD4 (given by `(Intercept)` variance) than in the evolutions of the marker in time (`obstime` variance). 









## Estimation of the Joint Model


In this section we are going to present the joint modelling framework motivated by the **time-to-event point of view**, that is, we want to add a time-dependent covariate measured with error in a survival model. 

Let $T_i$ denote the observed failure time for the $i$-th subject $(i = 1,...,n)$, which is taken as the minimum of the true event time $T_i$ and the censoring time $C_i$, i.e., $\widetilde T_i = \min(T_i,C_i)$. Furthermore, we define the event indicator as $\Delta_i = I(T_i \le C_i)$, where $I$ is the indicator function that takes the value 1 if the condition $T_i \le C_i$ is satisfied, and 0 otherwise.  So, the observed data for the time-to-event outcome consist of the pairs $\{(\widetilde T_i, \Delta_i), i = 1, . . . , n\}$. For the longitudinal responses, let $y_i(t)$ denote the value of the longitudinal outcome at time point $t$ for the $i$-th subject. Note that we do not actually observe $y_i(t)$ at all time points, but only at the very specific occasions $t_{ij}$ at which measurements were taken. Thus, the observed longitudinal data consist of the measurements $y_{ij} = \{yi(t_{ij}),j = 1,...,n_i\}$.


The objective is to associate the **true** and **unobserved** value of the longitudinal outcome at time $t$, denoted by $m_i(t)$, with the event outcome $\widetilde T_i$. Note that  $m_i(t)$ is different from $y_i(t)$ because this last is contaminated with measurement error value of the longitudinal outcome at time $t$.

In order to quantify the effect of  $m_i(t)$  on the risk of an event, we can use a relative risk model of the form:

\begin{equation}
h_i(t|\mathcal{M}_i(t),w_i) = h_0(t) \exp \{ \gamma^t w_i + \alpha m_i(t) \}
(\#eq:joint)
\end{equation}
where $\mathcal{M}_i(t)=\{ m_i(s), 0 \le s < t\}$ denotes the denotes the history of the true unobserved longitudinal process up to time point $t$, $h_0(\cdot)$ denotes the baseline risk function, and $w_i$ is a vector of baseline covariates (such as a treatment indicator, history of diseases, etc.) with a corresponding vector of regression coefficients $\gamma$. Similarly, parameter $\alpha$ quantifies the effect of the underlying longitudinal outcome to the risk for an event.  



```{block2, type = "rmdhint_sestelo"}
The interpretation of $\gamma$ and $\alpha$ is exactly the same as we have seen in Chapter \@ref(cox). In particular, $exp(\gamma_j)$ denotes the ratio of hazards for one unit change in $w_{ij}$ at any time $t$, whereas $exp(\alpha)$ denotes the relative increase in the risk for an event at time $t$ that results from one unit increase in $m_i(t)$ at the same time point. 
```

To complete the specification of teh above model, we need to think about the choice for the baseline risk function $h_0(\cdot)$. In standard survival analysis it is customary to leave $h_0(\cdot)$ completely unspecified in order to avoid the impact of misspecifying the distribution of survival times. However, within the joint modeling framework, it turns out that following such a route may lead to an underestimation of the standard errors of the parameter estimates [@BIOM:BIOM570]. To avoid such problems we will need to explicitly define $h_0(\cdot)$, for example with a known parametric distribution or  alternatively, and even more preferably, we can opt for a parametric but flexible specification of the baseline risk function. Several approaches are implemented in the `JM` package under the argument `method`.



#### The longitudinal submodel {-}

In the model above we use $m_i(t)$ to denote the true value of the underlying longitudinal covariate at time point $t$. However, and as mentioned earlier, longitudinal information is actually collected intermittently and with error at a set of a few time points $t_{ij}$ for each subject. So, to messure the effect of the longitudinal variable on the risk dor an event, we need to estimate $m_i(t)$. To do this, we are going to use the linear mixed models of the form

\[
y_i(t) = m_i(t) + \varepsilon_i(t),
\]

\[
m_i(t) = x_i^T(t)\beta + z_i^T(t)b_i + \varepsilon_i(t),
\]

\[
b_i \sim N(0, D), \quad \varepsilon_i(t) \sim N(0, \sigma^2),
\]
where $\beta$ denotes the vector of the unknown fixed effects parameters, $b_i$ denotes a vector of random effects, $x_i(t)$ and $z_i(t)$ denote row vectors of the design matrices for the fixed and random effects, respectively, and $\varepsilon_i(t)$ is the measument error term, which is assumed independent of $b_i$.




The main estimation methods for joint models are based on **(semiparametric) maximum likelihood** and **Bayes using MCMM techniques**. The `JM` package that we are going to use is based on maximum likelihood. The idea is the maximization of the log-likelihood corresponding to the joint distribution of the time-to-event and longitudinal out-comes $\{\widetilde T_i,\Delta_i,y_i\}$. Standard numerical integration techniques such as Gaussian quadrature and Monte Carlo have been successfully applied in the joint modelling framework. See Section 4.3 of @book:1606416 for details.








## The `JM` package



Now it is time to fit these models in `R`. To this end, we need first to **fit separately** the linear mixed effect model and the Cox model, and then take the returned objects and use them as main arguments in the `jointModel` function.  The dataset used is the same that the one seen with the mixed model, `aids`.  The survival information can be found in `aids.id`. 
```{r}
head(aids)

head(aids.id)
```





The idea here is to test for a **treatment effect on survival** after adjusting for the CD4 cell count.[^6]

[^6]: The CD4 cell counts are known to exhibit right skewed shapes of distribution, and therefore, for the remainder of this analysis we will work with the square root of the CD4 cell values.

```{r}
lattice::xyplot(sqrt(CD4) ~ obstime | drug, group = patient, data = aids, 
    xlab = "Months", ylab = expression(sqrt("CD4")), col = 1, type = "l")


lattice::xyplot(sqrt(CD4) ~ obstime | patient, group = patient, 
       data = aids[aids$patient %in% c(1:10),], 
       xlab = "Months", ylab = expression(sqrt("CD4")), col = 1, type = "b")

```


Now we are going to specify and fit a joint model. The linear mixed effects model for the CD4 cell counts include:

* Fixed-effects part:  main effect of time and the interaction with the treatment. 
* random-effects design matrix: an intercept and a time term.  

The survival submodel include: treatment effect (as a time-independent covariate) and the true underlying effect of CD4 cell count as estimated from the longitudinal model (as time-dependent). The baseline risk function is assumed piecewise constant. 


```{r}
fitLME <- lme(sqrt(CD4) ~ obstime : drug, random = ~ obstime | patient, data = aids)
fitSURV <- coxph(Surv(Time, death) ~ drug, data = aids.id, x = TRUE)
fitJM <- jointModel(fitLME, fitSURV, timeVar = "obstime", method = "piecewise-PH-GH")
summary(fitJM)
```





```{block2, type = "rmdhint_sestelo"}
Remember that, due to the fact that the `jointModel` function extracts all the required information from these two objects (e.g., response vectors, design matrices, etc.), in the call to the `coxph` function we need to specify the argument `x = TRUE`. With this, the design matrix of the Cox model is included in the returned object.

Additionally, the main argument `timeVar` of `jointModel` function  is used to specify the name of the time variable in the linear mixed effects model, which is required for the computation of $m_i(t)$.
```




Note that in the results of the event process the parameter labeled `Assoct` is the parameter $\alpha$ in the equation \@ref(eq:joint) that measures the effect of $m_i(t)$ (i.e., in our case of the true square root CD4 cell count) in the risk for death. The parameters $x_i$ are the  parameters for the piecewise constant baseline risk function. As we can see there is a significant effect of longitudinal outcome on the risk. For obtaining the Hazard Ratio for this variable we have to exponenciate the value exposed in the table. In this case the result is  `r round(exp(fitJM$coeff$alpha), 2)`. According to this, one unit increse on the CD4 count cell decreases the risk  `r  (1 - round(exp(fitJM$coeff$alpha), 2)) * 100`%.


If we want to test for a treatment effect, an alternative to the Wald test with a pvalue around 0.03, is the Likelihood Ratio Test (LRT). To perform it we need to fit the joint model under the null hypothesis of no treatment effect in the survival submodel, and then use the `anova`  function


```{r}
fitSURV2 <- coxph(Surv(Time, death) ~ 1, data = aids.id, x = TRUE)
fitJM2 <- jointModel(fitLME, fitSURV2, timeVar = "obstime", method = "piecewise-PH-GH")
anova(fitJM2, fitJM) # the model under the null is the first one
```

According to the pvalue (as with the Wald test) we arrive to the same conclusion, there exist an affect of the treatment on the risk.





Additionally, if we want to obtain **estimates of the Hazard Ratio with confidence intervals** for the final model it is possible ti apply the `confint` function to the created object

```{r}
confint(fitJM, parm = "Event")
exp(confint(fitJM, parm = "Event"))
```






<!-- As usually, we can check the fit of the model using residuals plots using the `plot` function. This include the plots of the subject-specific residuals versus the corresponding fitted values, the Q-Q plot of the subject-specific residuals, and the marginal survival and cumulative risk functions for the event process. -->

<!-- ```{r} -->
<!-- par(mfrow = c(2, 2)) -->
<!-- plot(fitJM) -->
<!-- ``` -->


<!-- One problem associated with these models is the **nonrandom dropout** in the longitudinal outcome caused by the occurrence of events. This can be seen on the following plot -->




<!-- ```{r} -->
<!-- # a useful function used in the residual plots below -->
<!-- plotResid <- function (x, y, ...) { -->
<!--     plot(x, y, ...) -->
<!--     lines(lowess(x, y), col = "red", lwd = 2) -->
<!--     abline(h = 0, lty = 3, col = "grey", lwd = 2) -->
<!-- } -->

<!-- # Marginal Residuals vs Fitted Values -->
<!-- resMargY <- residuals(fit.JM, process = "Longitudinal", type = "stand-Marginal") -->
<!-- fitMargY <- fitted(fit.JM, process = "Longitudinal", type = "Marginal") -->
<!-- plotResid(fitMargY, resMargY, xlab = "Fitted Values", ylab = "Residuals", -->
<!--     main = "Marginal Residuals vs Fitted Values") -->


<!-- ``` -->

<!-- We can observe a systematic trend with more positive residuals for small fitted values. However, due to the **nonrandom dropout** in the longitudinal outcome caused by the occurrence of events, conclusions from residuals based on the observed data alone should be extracted with caution. Note that the problem occurs in low values of CD4 that are related with higher times (thus we have less individuals).  -->

<!-- Based on the above, it is important to highlight that to **take dropout into account** we will use the multiply-imputed residuals.   -->







Finally, we will focus on the calculation of **expected survival probabilities**. For this we have to use the `survfitJM` function that accepts as main arguments a fitted joint model, and a data frame that contains the longitudinal and covariate information for the subjects for which we wish to calculate the predicted survival probabilities.


Here we compute the expected survival probabilies for two patients in the data set who **has not died** by the time of loss to follow-up. The function assumes that the patient has survived up to the last time point $t$ in newdata for which a CD4 measurement was recorded, and will produce survival probabilities for a set of predefined $u > t$ values


```{r}
set.seed(300716) # it uses Monte Carlo samples
preds <- survfitJM(fitJM, newdata = aids[aids$patient %in% c("7", "15"), ],
          idVar = "patient")  # last.time = "Time"

survfitJM(fitJM, newdata = aids[aids$patient %in% c("7", "15"), ], idVar = "patient", 
          survTimes = c(20, 30, 40))  # you can specify the times
```


Note that the first time of the output is the last time observed in the longitudinal study. This is because for the time points that are earlier than this time we know that this subject was alive and therefore survival probability is 1. 



```{r}
par(mfrow=c(1,2))
plot(preds, which = "7", conf.int = TRUE)

plot(preds, which = "7", conf.int = TRUE, 
     fun = function (x) -log(x), ylab = "Cumulative Risk")
```












<!--chapter:end:05-joint_model.Rmd-->

# Conditinal Survival with `condSURV` {#condsurv}


In this chapter we will see  the estimation of the survival function when we have ordered multivariate failure time data. This estimation will be obtained by means of the `condSURV` package, which provides  three different approaches all based on the Kaplan-Meier estimator.



## Introduction


As we saw, the **most popular** method for estimating survival, when there is censoring, is the well-known product-limit estimator also known as **Kaplan-Meier estimator** [@KM58]. The popularity of the product-limit estimator is explained by its simplicity and intuitive appeal while requiring very week assumptions. It simply takes into account with the empirical probability of surviving over certain time. 

**The method does not take into account of covariates**, so it is mainly descriptive. Discrete covariates can be included by splitting the sample for each level of the covariate and applying the product-limit estimator for each subsample. This approach is not recommended for continuous covariates. 

To account to this extra difficulty several generalizations to the Kaplan-Meier estimator have been proposed throughout the last decades. @Beran81 was the first one who proposed an estimator of the conditional distribution (survival) function with censored data in a fully nonparametric way. His estimator was further studied among others by @Dabrowska87, @Akritas94, @Manteiga94 and @VanKeilegom2001. All these estimators can be used to estimate the distribution (or survival) function conditional to a continuous covariable in a regression model, when data are subject to censoring. However, **none of the above** methods can be used to estimate the conditional survival when **the covariate is censored**.

In many longitudinal medical studies, patients may experience **several events** through a follow-up period. In these studies, the analysis of sequentially ordered events are often of interest. The events of concern can be of the same nature (e.g., recurrent disease episodes in cancer studies) or represent different states in the disease process (e.g., 'alive and disease-free', 'alive with recurrence' and 'dead'). If the events are of the same nature, this is usually referred as **recurrent events** [@Cook]. One example of this scheme can be see at Figure \@ref(fig:image).


```{r, "image", fig.cap = "Illustration of censoring.", echo = FALSE, fig.align = 'center'}
knitr::include_graphics("images/saBBVA_recurrence.pdf")
```


In the above situation maybe we want to obtain estimates for some  conditional survival. Let's do it now!




## Notation



Suppose that an individual may experience $K$ consecutive events at times $T_1<T_2<\cdot\cdot\cdot<T_K=T$, which are measured from the start of the follow-up.


Here different methods are proposed to estimate **conditional survival probabilities** such as $P(T_2 > y \mid T_1 > x)$ or $P(T_2 > y \mid T_1 \leq x)$, where $T_1$ and $T_2$ are ordered event times of two successive events. 

The proposed methods are all **based on the Kaplan-Meier** estimator and the ideas behind the proposed estimators can also be used to estimate more general functions involving **more than two successive event times**. However, for ease of presentation and without loss of generality, we take $K=2$ in this section. The extension to $K>2$ is straightforward.


```{r, "progres", fig.cap = "3-state progresive model.", echo = FALSE, fig.align = 'center'}
knitr::include_graphics("images/stage3box.jpg")
```



Let $(T_{1},T_{2})$ be a pair of successive event times corresponding to two ordered (possibly consecutive) events measured from the start of the follow-up.

Let $T=T_{2}$ denote the total time and assume that both $T_1$ and $T$ are observed subject to a (univariate) random right-censoring variable $C$ assumed to be independent of $(T_1,T)$. Due to censoring, rather than $(T_1,T)$ we observe $(\widetilde T_{1},\Delta_1,\widetilde T,\Delta_2)$ where $\widetilde T_{1}=\min (T_{1},C)$, $\Delta_{1}=I(T_{1}\leq C)$, $\widetilde T=\min (T,C)$, $\Delta_{2}=I(T\leq C)$, where $I(\cdot)$ is the indicator function. Let $(\widetilde T_{1i},\Delta_{1i},\widetilde T_i,\Delta_{2i})$, $1\leq i\leq n$ be independent and identically distributed data with the same distribution as $(\widetilde T_{1},\Delta_1,\widetilde T,\Delta_2)$.





## Estimation of the conditional survival

Let $S_1$ and $S$ be the marginal survival functions of $T_1$ and $T$; that is, $S_1(y)=P(T_1>y)$ and $S(y)=P(T>y)$. Introduce also the conditional survival probabilities $P(T>y|T_1>x)$ and $P(T>y|T_1\leq x)$. without loss of generality, we only consider the estimation of $S(y|x)=P(T>y|T_1>x)$.


The Kaplan-Meier estimator, also known as the product-limit estimator, is the most frequently used method to estimate survival for censored data. The most used representation of the Kaplan-Meier estimator of the total time is through a product of the following form

\begin{eqnarray*}
\widehat S(y)=\prod_{\widetilde T_i\leq t}\left(1-\frac{\Delta_{2i}}{R(\widetilde T_i)}\right)
\end{eqnarray*}

\noindent where $R(t)=\sum_{i=1}^{n} I(\widetilde T_i \geq t)$ denote the number of individuals at risk just before time $t$.


Below we introduce a weighted average representation of the Kaplan-Meier estimator which will be used later to introduce estimators for the conditional survival function

\begin{equation*}
\widehat S(y)=1-\sum_{i=1}^{n}W_{i}I(\widetilde T_{(i)}\leq y),%\equiv 1-\widehat{F}_1(x),
\end{equation*}

\noindent where $\widetilde T_{\left( 1\right) }\leq ...\leq \widetilde T_{\left( n\right) }$ denotes the ordered $\widetilde T$-sample and

\begin{equation*}
W_{i}=\frac{\Delta_{2\left[ i\right] }}{n-i+1}\prod_{j=1}^{i-1}\left[ 1-\frac{%
\Delta _{2\left[ j\right] }}{n-j+1}\right]
\end{equation*}

\noindent is the Kaplan-Meier weight attached to $\widetilde T_{\left( i\right) }$. In the expression of $W_{i}$ notation $\Delta_{2\left[ i\right] }$ is used for the $i$-th concomitant value of the censoring indicator (that is, $\Delta_{2\left[ i \right] }=\Delta _{2j}$ if $\widetilde T_{\left( i\right) }=\widetilde T_{j}$).






<br>

Well, we are interested in the estimation of the **conditional survival function, $S(y\mid x)=P(T>y\mid T_1>x)$**. Below we provide estimators for this quantity, all based on the Kaplan-Meier estimator.




### Kaplan-Meier Weighted Estimator (`KMW`)

Since $S(y\mid x)$ can be expressed as $S(y\mid x)=P(T > y|T_1 > x) = 1 - P(T\leq y\mid T_1 > x)= 1 - P(T_1 > x, T\leq y)/\left(1-P\left(T_1\leq x\right)\right),$ the conditional survival function may be estimated as


\begin{equation}
\widehat S^{\texttt{KMW}}(y\mid x)=1-\frac{\sum_{i=1}^{n}{W_iI(\widetilde T_{1\left[i\right]} >x, \widetilde T_{\left(i\right)} \leq y)}}{\widehat S_1(x)}.
\end{equation}






###  The Landmark approach (`LDM`)

The Landmark approach [@vanHouwelingen] states that, given the time point $x$, to estimate $S(y\mid x)=P(T> y\mid T_1>x)$ the analysis can be restricted to the individuals with an observed first event time greater than $x$. 

Let $n_x$ be the cardinal of $\left\{i:\widetilde T_{1i}>x\right\}$ and $\left( \widetilde T_{\left( i\right) }^{x},\Delta_{\left[ i\right]}^{x}\right)$, $i=1,...,n_{x}$, is the $\left(\widetilde T,\Delta\right)$-sample in $\left\{i:\widetilde T_{1i}>x\right\}$\ ordered with respect to $\widetilde T$. 




\begin{equation*}
\widehat S^{\texttt{LDM}}(y\mid x)=1-\sum_{i=1}^{n_x}{W_i^{x}I(\widetilde T_{\left(i\right)}^x \leq y)}.
\end{equation*}

\noindent where $W_i^{x}$ denotes the Kaplan-Meier weight attached to the i-th ordered T-datum, computed from the subsample $\left\{i:\widetilde T_{1i}>x\right\}$.




###   The Presmoothed Landmark approach (`PLDM`)

The standard error of the LDM approach may be large when the censoring is heavy, particularly with a small sample size. Interestingly, the variance of this estimator may be reduced by presmoothing [@Dikta1998]. Here, the idea of presmoothing involves replacing
the censoring indicators (in the expression of the Kaplan-Meier weights) by some smooth fit before the Kaplan-Meier formula is applied. This preliminary smoothing may be based on a certain parametric family such as the logistic (thus leading to a semiparametric estimator), or on a nonparametric estimator of the binary regression curve. The corresponding presmoothed landmark estimator is then given by



\begin{equation*}
\widehat S^{\texttt{PDLM}}(y\mid x)=1-\sum_{i=1}^{n_x}{W_i^{x\star}I(\widetilde T_{\left(i\right)}^x \leq y)}
\end{equation*}

\noindent where $W_{i}^{x\star}$ is defined through


\begin{equation*}
	W_{i}^{x\star}=\frac {m(\widetilde T_{\left(i\right)}^{x})}{n_x-i+1}\prod_{j=1}^{i-1}\left[1-\frac {m(\widetilde T_{\left(j\right)}^{x})}{n_x-j+1}\right], \quad 1\leq i\leq n_{x},
\end{equation*}

\noindent where $\left( \widetilde T_{\left( i\right) }^{x},\Delta_{\left[ i\right]}^{x}\right)$, $i=1,...,n_{x}$,
is the $\left( \widetilde T,\Delta\right)$-sample in $\left\{i:\widetilde T_{1i}>x\right\}$\ ordered with respect to $\widetilde T$.


Here, $m(t)= P(\Delta=1\mid \widetilde T^{x}=t)$. $m(\widetilde T^{x})$ belongs to a
parametric (smooth) family of binary regression curves, e.g., logistic. 




According to the performance, it has been demonstrated that **all of the  estimators perform well**, approaching their targets as the sample size increases. Besides, simulation results reveal that the landmark estimator (`LDM`) perform favorably when compared with the first method (`KMW`). Furthermore, the reported simulation results reveal relative benefits of presmoothing (`PLDM`) in the heavily censored scenarios or small sample sizes.






## The `condSURV` package

To illustrate our methods we will use data from a German Breast cancer study [@book:1506027]. This data set is freely available as part of the `condSURV  package.

In this dataset, a total of 686 woman with primary node positive Breast cancer were
recruited in the period between 1984 and 1989. From this total, 299 developed a recurrence and among these 171 died.

For each patient, the two event times (time to recurrence and time to death) and the
corresponding indicator status is recorded. Other covariates were also recorded. The covariate `recurrence` is the only time-dependent
covariate, while the other covariates included are fixed. Recurrence can be considered
as an intermediate transient state and modeled using a three-state progressive model
with states **Alive and disease-free**, **Alive with Recurrence** and **Dead**. You can see an example at Figure \@ref(fig:breast).


The effect of `recurrence` is important on the patient outcome and can be studied through the ordered multivariate event time data of time-to-event from enrollment, to recurrence and to death. Results obtained from the estimation of the conditional survival probabilities, $S(y\mid x)=P(T>y|T_1>x)$, can be used to understand which individuals without recurring cancer after surgery are most likely to survive from their disease and which would benefit from more personal attention, closer follow-up and monitoring.


```{r, "breast", fig.cap = "Scheme of the model.", echo = FALSE, fig.align = 'center'}
knitr::include_graphics("images/alive3box.jpg")
```

Bellow is an excerpt of the data.frame with one row per individual

```{r}
head(gbcsCS)

kmw1 <- survCOND(survCS(rectime, censrec, survtime, censdead) ~ 1,
x = 365, y = 1460, data = gbcsCS, method = "KMW", conf = TRUE, n.boot = 100)

summary(kmw1)
```

With the previous code you can obtain the estimates for the probability that a woman survives more than four years given that she is alive and disease-free at one year after the surgery. Note that the package contains the function `survCS` which takes the input data as an `R` formula and creates a survival object among the chosen variables for analysis. This function will verify if the data has been introduced correctly and create a `survCS` object. Arguments in this function must be introduced in the following order `time1`, `event1`, `time2`, `event2`,..., `Stime` and `event`, where `time1`, `time2`, ..., `Stime` are ordered event times and `event1`, `event2`,..., `event` their corresponding indicator statuses. This function plays a similar role as the `Surv` function in the `survival` package.




```{r}
# including more times
kmw2 <- survCOND(survCS(rectime, censrec, survtime, censdead) ~ 1,
x = 365, y = 365 * 1:7, data = gbcsCS, method = "KMW", conf = TRUE) 

summary(kmw2)


# with y omitted
kmw3 <- survCOND(survCS(rectime, censrec, survtime, censdead) ~ 1,
x = 365, data = gbcsCS, method = "KMW", conf = TRUE)

# note the `times` argument
summary(kmw3, times = c(730, 1095))  
```


In addition, one may also be interested in calculating the conditional survival function, $S(y\mid x)=P(T>y|T_1\leq x)$. This is the probability of the individual to be alive at time $y$ conditional that he/she is alive with recurrence at a previous time $x$.

```{r}
# P(T > y | T1 < x)
kmw4 <- survCOND(survCS(rectime, censrec, survtime, censdead) ~ 1,
x = 365, data = gbcsCS, method = "KMW", conf = TRUE, lower.tail = TRUE) 

summary(kmw4, times = c(730, 1095))
```


Similarly, one can obtain the results for the landmark methods (`LDM` and `PLDM`) using the same function `survCOND`. The unsmoothed landmark estimator is obtained using argument `method = "LDM"` whereas for obtaining the presmoothed landmark estimator the argument `presmooth = TRUE` is also required.


```{r}
plot(kmw3, confcol = "red", xlab = "Time (days)", ylab = "S(y|365)")
```




One important goal is to obtain estimates for the above estimated quantities (conditional survival probabilities) conditionally on current or past covariate measures. The current version of the package allow the inclusion of a single covariate.


```{r}
grade <- survCOND(survCS(rectime, censrec, survtime, censdead) ~ as.factor(grade),
                  x = 365, data = gbcsCS, method = "LDM", conf = FALSE)
plot(grade)
```


Finally, the package also allow the user to estimate the conditional survival given a continuous covariate (i.e., objects of class 'integer' or 'numeric'). For example, estimates and plot for the conditional survival for women aged 60 years, $S(y|x,Z=z)=P(T>y|T_1>x, age=60)$. 




```{r}
age <- survCOND(survCS(rectime, censrec, survtime, censdead) ~ age, x = 365, 
                z.value = 60, data = gbcsCS, conf = FALSE)
plot(age)
```



```{block2, type = "rmdhint_sestelo"}
The **inclusion of continuous covariates** can be computationally demanding. In particular, the use of bootstrap resampling techniques are time-consuming processes because it is necessary to estimate the model a great number of times. 
```



The use of the `condSURV` package to more than two consecutive events is illustrated in the Appendix of @meiramachado-sestelo:2016.







<!--chapter:end:06-condsurv.Rmd-->


# (APPENDIX) Appendix {-}

# Installation of `R` and `RStudio` {#appendix-install}

You can follow these steps to install `R` and `Rstudio`, please note that there will be a few new releases of `R` every year, and you may want to upgrade `R` occasionally.



* For **Ubuntu** users, kindly follow the corresponding instructions [here](https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2).

* For **Mac OS X** users download `R` from [here](https://cran.r-project.org) or from the url below. To this end, click on *Download R for Mac OS X*. Then click on *Download R-3.4.2.pkg* (or a newer version) and install it. Leave all default settings in the installation options. Optional for some graphic experiences,  download and install [`XQuartz`](http://xquartz.macosforge.org/).


```{r, screenshot.opts = list(delay = 5), echo = FALSE, fig.width = 8}
 knitr::include_url("https://cran.r-project.org")
```





<br><br>

    
Once installed `R`, you can download `RStudio IDE` from [here](http://www.rstudio.com/ide/download/desktop) or from the url below. You must choose the appropriate version to your operative system and hardware (only certain Ubuntu and Fedora versions are supported), and install it using the package manager.

```{r, screenshot.opts = list(delay = 5), echo = FALSE, fig.width = 8}
 knitr::include_url("http://www.rstudio.com/ide/download/desktop")
```









# Introduction to `RStudio` {#appendix-rstudio}

[`RStudio`](https://www.rstudio.com) is the premier integrated development environment (IDE) for `R`. It is available in open source and commercial editions on the desktop (Windows, Mac, and Linux) and from a web browser to a Linux server running RStudio Server or RStudio Server Pro.

You can find a global view of the IDE in this [Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf).

An important advice is that for running a line or code selection from the script in the console, you can do it with the keyboard shortcut `'Ctrl+Enter'` (Linux) or `'Cmd+Enter'` (Mac OS X).




# Introduction to `R` {#appendix-r}

The  manual ["An Introduction to R"](https://cran.r-project.org/doc/manuals/r-release/R-intro.html) gives an introduction to the language and how to use `R` for doing statistical analysis and graphcis in detail. 



Additionally, in this section you can find a set of Cheat Sheets of this programming language:

* [R Base](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf) for first steps and basic functions of the language.

* [R Advanced](https://www.rstudio.com/wp-content/uploads/2016/02/advancedR.pdf) for environments, data structures, functions, subsetting and more advanced things.

* The [Data Import](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf) cheat sheet reminds you how to read in flat files with http://readr.tidyverse.org/, work with the results as tibbles, and reshape messy data with the [`tidyr`](https://cran.r-project.org/web/packages/tidyr/index.html) package. Use `tidyr` to reshape your tables into tidy data, the data format that works the most seamlessly with `R` and the [`tidyverse`](https://cran.r-project.org/web/packages/tidyverse/index.html).

* [Data Transformations](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) for some functions in [`dplyr`](https://cran.r-project.org/web/packages/dplyr/index.html) packages very useful and computational efficient to preprocess data.

* [Data Visualization](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf) for make beautiful and customizable plots of your data by means of the [`ggplot2`](https://cran.r-project.org/web/packages/ggplot2/index.html) package. It implements the grammar of graphics, an easy to use system for building plots.






Finally, you can find below a list with some well-know **web resources** related with this statistical language:

- [R-bloggers](https://www.r-bloggers.com)
- [Quick-R site](http://www.statmethods.net/index.html)
- [Revolutions](http://revolution-computing.typepad.com)
- [The R Journal](http://journal.r-project.org/current.html)
- [Journal of Statistical Software](https://www.jstatsoft.org/index)





<!-- ## Why Use `R`? {-} -->

<!-- Why learn R if I currently use another statistical package? -->

<!-- #. R is free. If you are a teacher or a student, the benefits are obvious.  -->

<!-- #. R is very popular. It is becoming the Standard and is well manteined by an active and highly talented community.  -->

<!-- #. R is powerful.  -->
<!--     * It can handle complex and large data.  -->
<!--     * R can easily program complex simulations -->
<!--     * R can be used on High Performance Computer Clusters -->
<!--     * R supports multicore task distribution -->

<!-- #. R is flexible: from complex or standard statistical practices, to bayesian modelling, to GIS map building, to building interactive web applications, and to building interactive tests. -->
<!--     * Statistical Models -->
<!--     * Build R powered Web Applications using Shiny -->
<!--     * Build R powered Adaptive Tests using Concerto -->
<!--     * Build R powered Web book using Bookdown -->
<!--     * Render beautiful HTML Slideshows using Slidify -->


<!-- #. R is well supported. There is extensive support for R in the form of documentation, FAQs, StackOverflow, blogs (e.g. R-Bloggers), webinars, workshops, and many books (and many are also free).  -->



<!-- ### Web Resources {-} -->

<!--chapter:end:07-appendix.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:08-references.Rmd-->

